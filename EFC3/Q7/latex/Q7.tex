\documentclass[10pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Q7}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{ia353---redes-neurais}{%
\section*{IA353 - Redes Neurais}\label{ia353---redes-neurais}}

\hypertarget{efc3---questuxe3o-7}{%
\section*{EFC3 - Questão 7}\label{efc3---questuxe3o-7}}

    \hypertarget{interpretability}{%
\section*{Interpretability}\label{interpretability}}

dataset: MNIST

    \textbf{Professor:} Fernando J. Von Zuben \textbf{Aluno(a):} Rafael Ito

    \hypertarget{libraries-and-packages}{%
\section*{7.1 Libraries and packages}\label{libraries-and-packages}}

    \hypertarget{install-packages}{%
\subsection*{7.1.1 Install packages}\label{install-packages}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} install Python libs}
\PY{o}{!}pip install \PYZhy{}q         \PY{err}{\PYZbs{}}
    \PY{n}{numpy}               \PYZbs{}
    \PY{n}{innvestigate}        \PYZbs{}
    \PY{n}{keras}\PY{o}{==}\PY{l+m+mf}{2.2}\PY{o}{.}\PY{l+m+mi}{4}        \PYZbs{}
    \PY{n}{tensorflow}\PY{o}{==}\PY{l+m+mf}{1.15}\PY{o}{.}\PY{l+m+mi}{0}   \PYZbs{}
    \PY{n}{git}\PY{o}{+}\PY{n}{https}\PY{p}{:}\PY{o}{/}\PY{o}{/}\PY{n}{github}\PY{o}{.}\PY{n}{com}\PY{o}{/}\PY{n}{raghakot}\PY{o}{/}\PY{n}{keras}\PY{o}{\PYZhy{}}\PY{n}{vis}\PY{o}{.}\PY{n}{git} \PY{o}{\PYZhy{}}\PY{n}{U}
    \PY{c+c1}{\PYZsh{}keras\PYZhy{}vis}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
  Building wheel for keras-vis (setup.py) {\ldots} done
    \end{Verbatim}

    \hypertarget{import-libraries}{%
\subsection*{7.1.2 Import libraries}\label{import-libraries}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{c+c1}{\PYZsh{} general}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{sys}
\PY{k+kn}{import} \PY{n+nn}{os}
\PY{k+kn}{import} \PY{n+nn}{pdb}
\PY{k+kn}{import} \PY{n+nn}{subprocess}
\PY{k+kn}{from} \PY{n+nn}{collections} \PY{k+kn}{import} \PY{n}{Counter}
\PY{k+kn}{from} \PY{n+nn}{multiprocessing} \PY{k+kn}{import} \PY{n}{cpu\PYZus{}count}
\PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{c+c1}{\PYZsh{} interpretability}
\PY{k+kn}{import} \PY{n+nn}{keras}
\PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
\PY{n}{tf}\PY{o}{.}\PY{n}{logging}\PY{o}{.}\PY{n}{set\PYZus{}verbosity}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{logging}\PY{o}{.}\PY{n}{ERROR}\PY{p}{)}
\PY{k+kn}{import} \PY{n+nn}{innvestigate}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{vis}\PY{n+nn}{.}\PY{n+nn}{visualization} \PY{k+kn}{import} \PY{n}{visualize\PYZus{}activation}
\PY{k+kn}{from} \PY{n+nn}{vis}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k+kn}{import} \PY{n}{utils}
\PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{c+c1}{\PYZsh{} random seed generator}
\PY{n}{os}\PY{o}{.}\PY{n}{environ}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PYTHONHASHSEED}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{=}\PY{n+nb}{str}\PY{p}{(}\PY{l+m+mi}{42}\PY{p}{)}
\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{42}\PY{p}{)}
\PY{c+c1}{\PYZsh{}tf.random.set\PYZus{}seed(42)}
\PY{n}{tf}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{set\PYZus{}random\PYZus{}seed}\PY{p}{(}\PY{l+m+mi}{42}\PY{p}{)}
\PY{n}{os}\PY{o}{.}\PY{n}{environ}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TF\PYZus{}DETERMINISTIC\PYZus{}OPS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{device-info}{%
\subsection*{7.1.3 Device info}\label{device-info}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} choose between CPU and GPU}
\PY{n}{device} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/cpu:0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{}if tf.config.list\PYZus{}physical\PYZus{}devices(\PYZsq{}GPU\PYZsq{}):}
\PY{k}{if} \PY{n}{tf}\PY{o}{.}\PY{n}{config}\PY{o}{.}\PY{n}{experimental}\PY{o}{.}\PY{n}{list\PYZus{}physical\PYZus{}devices}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GPU}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
    \PY{n}{device} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/device:GPU:0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{}    device\PYZus{}model = torch.cuda.get\PYZus{}device\PYZus{}name(0)}
\PY{c+c1}{\PYZsh{}    device\PYZus{}memory = torch.cuda.get\PYZus{}device\PYZus{}properties(device).total\PYZus{}memory / 1e9}
    \PY{n}{device\PYZus{}number} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{config}\PY{o}{.}\PY{n}{experimental}\PY{o}{.}\PY{n}{list\PYZus{}physical\PYZus{}devices}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GPU}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
    \PY{c+c1}{\PYZsh{}from tensorflow.python.client import device\PYZus{}lib}
    \PY{c+c1}{\PYZsh{}print(device\PYZus{}lib.list\PYZus{}local\PYZus{}devices())}
    \PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Device: gpu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}print(\PYZsq{}GPU model:\PYZsq{}, device\PYZus{}model)}
    \PY{c+c1}{\PYZsh{}print(\PYZsq{}GPU memory: \PYZob{}0:.2f\PYZcb{} GB\PYZsq{}.format(device\PYZus{}memory))}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{GPUs available: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{device\PYZus{}number}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CPU cores:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{cpu\PYZus{}count}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Device: gpu
GPUs available:  1
\#-------------------
CPU cores: 2
    \end{Verbatim}

    \hypertarget{training}{%
\section*{7.2 Training}\label{training}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Questao 7:}
\PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{n}{mnist} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{datasets}\PY{o}{.}\PY{n}{mnist}
\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)} \PY{o}{=} \PY{n}{mnist}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}
\PY{n}{x\PYZus{}train} \PY{o}{=} \PY{n}{x\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{x\PYZus{}test} \PY{o}{=} \PY{n}{x\PYZus{}test}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{x\PYZus{}test} \PY{o}{=} \PY{n}{x\PYZus{}train} \PY{o}{/} \PY{l+m+mf}{255.0}\PY{p}{,} \PY{n}{x\PYZus{}test} \PY{o}{/} \PY{l+m+mf}{255.0}

\PY{n}{model} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.25}\PY{p}{)}\PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}config}\PY{p}{(}\PY{p}{)}

\PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sparse\PYZus{}categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{evaluation} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}

\PY{n}{model}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}model.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/5
60000/60000 [==============================] - 8s 141us/step - loss: 0.1859 -
acc: 0.9431
Epoch 2/5
60000/60000 [==============================] - 8s 135us/step - loss: 0.0807 -
acc: 0.9762
Epoch 3/5
60000/60000 [==============================] - 8s 135us/step - loss: 0.0589 -
acc: 0.9821
Epoch 4/5
60000/60000 [==============================] - 8s 135us/step - loss: 0.0497 -
acc: 0.9844
Epoch 5/5
60000/60000 [==============================] - 8s 136us/step - loss: 0.0427 -
acc: 0.9869
10000/10000 [==============================] - 1s 57us/step
    \end{Verbatim}

    \hypertarget{innvestigate-item-7a}{%
\section*{7.3 iNNvestigate (item 7a)}\label{innvestigate-item-7a}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} counting samples per class}
\PY{n}{unique}\PY{p}{,} \PY{n}{counts} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{return\PYZus{}counts}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{c+c1}{\PYZsh{} cumulative sum}
\PY{n}{sum\PYZus{}count} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{cumsum}\PY{p}{(}\PY{n}{counts}\PY{p}{)}
\PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{counts}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{sum\PYZus{}count}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[ 980 1135 1032 1010  982  892  958 1028  974 1009]
[  980  2115  3147  4157  5139  6031  6989  8017  8991 10000]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} sort X and y test set}
\PY{n}{sort} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{argsort}\PY{p}{(}\PY{p}{)}
\PY{n}{x\PYZus{}test\PYZus{}sort} \PY{o}{=} \PY{n}{x\PYZus{}test}\PY{p}{[}\PY{n}{sort}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}
\PY{n}{y\PYZus{}test\PYZus{}sort} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{p}{[}\PY{n}{sort}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} function that randomly sample two different images from the same class}
\PY{k}{def} \PY{n+nf}{get\PYZus{}index}\PY{p}{(}\PY{n}{digit\PYZus{}class}\PY{p}{,} \PY{n}{counter}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{p}{(}\PY{n}{digit\PYZus{}class} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
        \PY{n}{rand1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{low}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{high}\PY{o}{=}\PY{n}{counter}\PY{p}{[}\PY{n}{digit\PYZus{}class}\PY{o}{\PYZpc{}}\PY{k}{10}])
        \PY{n}{rand2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{low}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{high}\PY{o}{=}\PY{n}{counter}\PY{p}{[}\PY{n}{digit\PYZus{}class}\PY{o}{\PYZpc{}}\PY{k}{10}])
        \PY{k}{while} \PY{p}{(}\PY{n}{rand2} \PY{o}{==} \PY{n}{rand1}\PY{p}{)}\PY{p}{:}
            \PY{n}{rand2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{low}\PY{o}{=}\PY{n}{counter}\PY{p}{[}\PY{p}{(}\PY{n}{digit\PYZus{}class}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{\PYZpc{}}\PY{k}{10}], high=counter[digit\PYZus{}class\PYZpc{}10])
    \PY{k}{else}\PY{p}{:}
        \PY{n}{rand1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{low}\PY{o}{=}\PY{n}{counter}\PY{p}{[}\PY{p}{(}\PY{n}{digit\PYZus{}class}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{\PYZpc{}}\PY{k}{10}], high=counter[digit\PYZus{}class\PYZpc{}10])
        \PY{n}{rand2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{low}\PY{o}{=}\PY{n}{counter}\PY{p}{[}\PY{p}{(}\PY{n}{digit\PYZus{}class}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{\PYZpc{}}\PY{k}{10}], high=counter[digit\PYZus{}class\PYZpc{}10])
        \PY{k}{while} \PY{p}{(}\PY{n}{rand2} \PY{o}{==} \PY{n}{rand1}\PY{p}{)}\PY{p}{:}
            \PY{n}{rand2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{low}\PY{o}{=}\PY{n}{counter}\PY{p}{[}\PY{p}{(}\PY{n}{digit\PYZus{}class}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{\PYZpc{}}\PY{k}{10}], high=counter[digit\PYZus{}class\PYZpc{}10])
    \PY{k}{return} \PY{n}{rand1}\PY{p}{,} \PY{n}{rand2}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sample} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{c+c1}{\PYZsh{} digit 3}
\PY{n}{rand1}\PY{p}{,} \PY{n}{rand2} \PY{o}{=} \PY{n}{get\PYZus{}index}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{sum\PYZus{}count}\PY{p}{)}
\PY{n}{sample3\PYZus{}1} \PY{o}{=} \PY{n}{x\PYZus{}test\PYZus{}sort}\PY{p}{[}\PY{n}{rand1}\PY{p}{]}
\PY{n}{sample3\PYZus{}2} \PY{o}{=} \PY{n}{x\PYZus{}test\PYZus{}sort}\PY{p}{[}\PY{n}{rand2}\PY{p}{]}
\PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{sample}\PY{p}{,} \PY{p}{(}\PY{n}{sample3\PYZus{}1}\PY{p}{,} \PY{n}{sample3\PYZus{}2}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([0., 0., 0., {\ldots}, 0., 0., 0.])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} getting samples}
\PY{n}{sample} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{c+c1}{\PYZsh{} digit 3}
\PY{n}{rand1}\PY{p}{,} \PY{n}{rand2} \PY{o}{=} \PY{n}{get\PYZus{}index}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{sum\PYZus{}count}\PY{p}{)}
\PY{n}{sample}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{x\PYZus{}test\PYZus{}sort}\PY{p}{[}\PY{n}{rand1}\PY{p}{]}\PY{p}{)}
\PY{n}{sample}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{x\PYZus{}test\PYZus{}sort}\PY{p}{[}\PY{n}{rand2}\PY{p}{]}\PY{p}{)}
\PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{c+c1}{\PYZsh{} digit 4}
\PY{n}{rand1}\PY{p}{,} \PY{n}{rand2} \PY{o}{=} \PY{n}{get\PYZus{}index}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{sum\PYZus{}count}\PY{p}{)}
\PY{n}{sample}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{x\PYZus{}test\PYZus{}sort}\PY{p}{[}\PY{n}{rand1}\PY{p}{]}\PY{p}{)}
\PY{n}{sample}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{x\PYZus{}test\PYZus{}sort}\PY{p}{[}\PY{n}{rand2}\PY{p}{]}\PY{p}{)}
\PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{c+c1}{\PYZsh{} digit 7}
\PY{n}{rand1}\PY{p}{,} \PY{n}{rand2} \PY{o}{=} \PY{n}{get\PYZus{}index}\PY{p}{(}\PY{l+m+mi}{7}\PY{p}{,} \PY{n}{sum\PYZus{}count}\PY{p}{)}
\PY{n}{sample}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{x\PYZus{}test\PYZus{}sort}\PY{p}{[}\PY{n}{rand1}\PY{p}{]}\PY{p}{)}
\PY{n}{sample}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{x\PYZus{}test\PYZus{}sort}\PY{p}{[}\PY{n}{rand2}\PY{p}{]}\PY{p}{)}
\PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{n}{sample}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(6, 28, 28)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Definition of a function to visualize some digits}
\PY{k}{def} \PY{n+nf}{show}\PY{p}{(}\PY{n}{img}\PY{p}{)}\PY{p}{:}
    \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cmap} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{gray}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{interpolation} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{none}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Visualization of the sampled images}
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{7}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}
\PY{n}{ind} \PY{o}{=} \PY{l+m+mi}{1}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
        \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{ind}\PY{p}{)}
        \PY{n}{show}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{sample}\PY{p}{[}\PY{n}{ind}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{yticks}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}
        \PY{n}{ind}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Q7_files/Q7_19_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Questao 7a:}
\PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{c+c1}{\PYZsh{}mnist = keras.datasets.mnist}
\PY{c+c1}{\PYZsh{}(x\PYZus{}train, y\PYZus{}train),(x\PYZus{}test, y\PYZus{}test) = mnist.load\PYZus{}data()}
\PY{c+c1}{\PYZsh{}x\PYZus{}train = x\PYZus{}train.reshape(x\PYZus{}train.shape[0], 28, 28, 1)}
\PY{c+c1}{\PYZsh{}x\PYZus{}test = x\PYZus{}test.reshape(x\PYZus{}test.shape[0], 28, 28, 1)}
\PY{c+c1}{\PYZsh{}x\PYZus{}train, x\PYZus{}test = x\PYZus{}train / 255.0, x\PYZus{}test / 255.0}

\PY{n}{model} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{load\PYZus{}model}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}model.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{model\PYZus{}wo\PYZus{}sm} \PY{o}{=} \PY{n}{innvestigate}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{keras}\PY{o}{.}\PY{n}{graph}\PY{o}{.}\PY{n}{model\PYZus{}wo\PYZus{}softmax}\PY{p}{(}\PY{n}{model}\PY{p}{)}

\PY{c+c1}{\PYZsh{}imagem = x\PYZus{}test[0:1]}
\PY{c+c1}{\PYZsh{}plot.imshow(imagem.squeeze(), cmap=\PYZsq{}gray\PYZsq{}, interpolation=\PYZsq{}nearest\PYZsq{})}

\PY{c+c1}{\PYZsh{}analyzer = innvestigate.analyzer.LRPEpsilon(model=model\PYZus{}wo\PYZus{}sm, epsilon=1)}
\PY{c+c1}{\PYZsh{}analysis = analyzer.analyze(imagem)}
\PY{c+c1}{\PYZsh{}plt.imshow(analysis.squeeze(), cmap=\PYZsq{}seismic\PYZsq{}, interpolation=\PYZsq{}nearest\PYZsq{})}
\end{Verbatim}
\end{tcolorbox}

    Importing classes and instantiating them

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{innvestigate}\PY{n+nn}{.}\PY{n+nn}{analyzer}\PY{n+nn}{.}\PY{n+nn}{gradient\PYZus{}based} \PY{k+kn}{import} \PY{n}{Gradient}\PY{p}{,} \PY{n}{SmoothGrad}
\PY{k+kn}{from} \PY{n+nn}{innvestigate}\PY{n+nn}{.}\PY{n+nn}{analyzer}\PY{n+nn}{.}\PY{n+nn}{deeptaylor} \PY{k+kn}{import} \PY{n}{DeepTaylor}
\PY{k+kn}{from} \PY{n+nn}{innvestigate}\PY{n+nn}{.}\PY{n+nn}{analyzer}\PY{n+nn}{.}\PY{n+nn}{relevance\PYZus{}based}\PY{n+nn}{.}\PY{n+nn}{relevance\PYZus{}analyzer} \PY{k+kn}{import} \PY{n}{LRPAlphaBeta}\PY{p}{,} \PY{n}{LRPEpsilon}\PY{p}{,} \PY{n}{BaselineLRPZ}
\PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{n}{gradient}    \PY{o}{=} \PY{n}{Gradient}\PY{p}{(}\PY{n}{model}\PY{o}{=}\PY{n}{model\PYZus{}wo\PYZus{}sm}\PY{p}{)}
\PY{n}{smoothgrad}  \PY{o}{=} \PY{n}{SmoothGrad}\PY{p}{(}\PY{n}{model}\PY{o}{=}\PY{n}{model\PYZus{}wo\PYZus{}sm}\PY{p}{)}
\PY{n}{deeptaylor}  \PY{o}{=} \PY{n}{DeepTaylor}\PY{p}{(}\PY{n}{model}\PY{o}{=}\PY{n}{model\PYZus{}wo\PYZus{}sm}\PY{p}{)}
\PY{n}{lrpa}        \PY{o}{=} \PY{n}{LRPAlphaBeta}\PY{p}{(}\PY{n}{model}\PY{o}{=}\PY{n}{model\PYZus{}wo\PYZus{}sm}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{beta}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{lrpe}        \PY{o}{=} \PY{n}{LRPEpsilon}\PY{p}{(}\PY{n}{model}\PY{o}{=}\PY{n}{model\PYZus{}wo\PYZus{}sm}\PY{p}{,} \PY{n}{epsilon}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{lrpz}        \PY{o}{=} \PY{n}{BaselineLRPZ}\PY{p}{(}\PY{n}{model}\PY{o}{=}\PY{n}{model\PYZus{}wo\PYZus{}sm}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Function that plots analyzer outputs

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{plot\PYZus{}analyzer}\PY{p}{(}\PY{n}{raw}\PY{p}{,} \PY{n}{analyzed}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Visualization of tha sampled images}
    \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
    \PY{n}{ind} \PY{o}{=} \PY{l+m+mi}{1}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{:}
        \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{n}{j}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{:}
                \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{p}{(}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{2}\PY{o}{+}\PY{n}{j}\PY{p}{)}
                \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{raw}\PY{p}{[}\PY{n}{ind}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{cmap} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{gray}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{interpolation} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{none}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{if} \PY{n}{j}\PY{o}{==}\PY{l+m+mi}{2}\PY{p}{:}
                \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{p}{(}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{2}\PY{o}{+}\PY{n}{j}\PY{p}{)}
                \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{analyzed}\PY{p}{[}\PY{n}{ind}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seismic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{yticks}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}
        \PY{n}{ind}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{gradient}{%
\subsection*{7.3.1 Gradient}\label{gradient}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{analysis\PYZus{}gradient} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{tensor} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{sample}\PY{p}{)}\PY{p}{:}
    \PY{n}{analysis\PYZus{}gradient}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{gradient}\PY{o}{.}\PY{n}{analyze}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{tensor}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{plot\PYZus{}analyzer}\PY{p}{(}\PY{n}{sample}\PY{p}{,} \PY{n}{analysis\PYZus{}gradient}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Q7_files/Q7_26_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{smoothgrad}{%
\subsection*{7.3.2 SmoothGrad}\label{smoothgrad}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{analysis\PYZus{}smoothgrad} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{tensor} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{sample}\PY{p}{)}\PY{p}{:}
    \PY{n}{analysis\PYZus{}smoothgrad}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{smoothgrad}\PY{o}{.}\PY{n}{analyze}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{tensor}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{plot\PYZus{}analyzer}\PY{p}{(}\PY{n}{sample}\PY{p}{,} \PY{n}{analysis\PYZus{}smoothgrad}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Q7_files/Q7_28_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{deeptaylor}{%
\subsection*{7.3.3 DeepTaylor}\label{deeptaylor}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{analysis\PYZus{}deeptaylor} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{tensor} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{sample}\PY{p}{)}\PY{p}{:}
    \PY{n}{analysis\PYZus{}deeptaylor}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{deeptaylor}\PY{o}{.}\PY{n}{analyze}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{tensor}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{plot\PYZus{}analyzer}\PY{p}{(}\PY{n}{sample}\PY{p}{,} \PY{n}{analysis\PYZus{}deeptaylor}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Q7_files/Q7_30_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{lrpalphabeta}{%
\subsection*{7.3.4 LRPAlphaBeta}\label{lrpalphabeta}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{analysis\PYZus{}lrpa} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{tensor} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{sample}\PY{p}{)}\PY{p}{:}
    \PY{n}{analysis\PYZus{}lrpa}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{lrpa}\PY{o}{.}\PY{n}{analyze}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{tensor}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{plot\PYZus{}analyzer}\PY{p}{(}\PY{n}{sample}\PY{p}{,} \PY{n}{analysis\PYZus{}lrpa}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Q7_files/Q7_32_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{lrpepsilon}{%
\subsection*{7.3.5 LRPEpsilon}\label{lrpepsilon}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{analysis\PYZus{}lrpe} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{tensor} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{sample}\PY{p}{)}\PY{p}{:}
    \PY{n}{analysis\PYZus{}lrpe}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{lrpe}\PY{o}{.}\PY{n}{analyze}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{tensor}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{plot\PYZus{}analyzer}\PY{p}{(}\PY{n}{sample}\PY{p}{,} \PY{n}{analysis\PYZus{}lrpe}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Q7_files/Q7_34_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{lrpz}{%
\subsection*{7.3.6 LRPZ}\label{lrpz}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{analysis\PYZus{}lrpz} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{28}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{tensor} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{sample}\PY{p}{)}\PY{p}{:}
    \PY{n}{analysis\PYZus{}lrpz}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{lrpz}\PY{o}{.}\PY{n}{analyze}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{tensor}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{plot\PYZus{}analyzer}\PY{p}{(}\PY{n}{sample}\PY{p}{,} \PY{n}{analysis\PYZus{}lrpz}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Q7_files/Q7_36_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{summary}{%
\subsection*{7.3.7 Summary}\label{summary}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Visualization of tha sampled images}
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{ind} \PY{o}{=} \PY{l+m+mi}{1}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{:}
    \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{n}{j}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{:}
            \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{p}{(}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{7}\PY{o}{+}\PY{n}{j}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{sample}\PY{p}{[}\PY{n}{ind}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{cmap} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{gray}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{interpolation} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{none}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{k}{if} \PY{n}{j}\PY{o}{==}\PY{l+m+mi}{2}\PY{p}{:}
            \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{p}{(}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{7}\PY{o}{+}\PY{n}{j}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{analysis\PYZus{}gradient}\PY{p}{[}\PY{n}{ind}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seismic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{if} \PY{n}{j}\PY{o}{==}\PY{l+m+mi}{3}\PY{p}{:}
            \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{p}{(}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{7}\PY{o}{+}\PY{n}{j}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{analysis\PYZus{}smoothgrad}\PY{p}{[}\PY{n}{ind}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seismic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{if} \PY{n}{j}\PY{o}{==}\PY{l+m+mi}{4}\PY{p}{:}
            \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{p}{(}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{7}\PY{o}{+}\PY{n}{j}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{analysis\PYZus{}deeptaylor}\PY{p}{[}\PY{n}{ind}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seismic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{if} \PY{n}{j}\PY{o}{==}\PY{l+m+mi}{5}\PY{p}{:}
            \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{p}{(}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{7}\PY{o}{+}\PY{n}{j}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{analysis\PYZus{}lrpa}\PY{p}{[}\PY{n}{ind}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seismic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{if} \PY{n}{j}\PY{o}{==}\PY{l+m+mi}{6}\PY{p}{:}
            \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{p}{(}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{7}\PY{o}{+}\PY{n}{j}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{analysis\PYZus{}lrpe}\PY{p}{[}\PY{n}{ind}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seismic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{if} \PY{n}{j}\PY{o}{==}\PY{l+m+mi}{7}\PY{p}{:}
            \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{p}{(}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{7}\PY{o}{+}\PY{n}{j}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{analysis\PYZus{}lrpz}\PY{p}{[}\PY{n}{ind}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seismic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{yticks}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}
    \PY{n}{ind}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Q7_files/Q7_38_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{keras-vis-item-7b}{%
\section*{7.4 Keras-vis (item 7b)}\label{keras-vis-item-7b}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Questao 7b:}
\PY{c+c1}{\PYZsh{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{n}{model} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{load\PYZus{}model}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnist\PYZus{}model.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} change last layer (softmax) with a linear layer}
\PY{c+c1}{\PYZsh{}layer\PYZus{}idx = utils.find\PYZus{}layer\PYZus{}idx(model, \PYZsq{}dense\PYZus{}2\PYZsq{})}
\PY{n}{layer\PYZus{}idx} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
\PY{n}{model}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{n}{layer\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{activation} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{activations}\PY{o}{.}\PY{n}{linear}
\PY{n}{model} \PY{o}{=} \PY{n}{utils}\PY{o}{.}\PY{n}{apply\PYZus{}modifications}\PY{p}{(}\PY{n}{model}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{visualizing-input-that-maximizes-the-output-of-class-0}{%
\subsection*{7.4.1 Visualizing input that maximizes the output of class
0}\label{visualizing-input-that-maximizes-the-output-of-class-0}}

(total variation = L-p norm = 0)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{filter\PYZus{}idx} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{img} \PY{o}{=} \PY{n}{visualize\PYZus{}activation}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{layer\PYZus{}idx}\PY{p}{,}
    \PY{n}{filter\PYZus{}indices}\PY{o}{=}\PY{n}{filter\PYZus{}idx}\PY{p}{,} \PY{n}{input\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.}\PY{p}{,} \PY{l+m+mf}{1.}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
    \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{tv\PYZus{}weight}\PY{o}{=}\PY{l+m+mf}{10.}\PY{p}{,} \PY{n}{lp\PYZus{}norm\PYZus{}weight}\PY{o}{=}\PY{l+m+mf}{0.}\PY{p}{)}
\PY{c+c1}{\PYZsh{}plot.imshow(img.squeeze(), cmap=\PYZsq{}seismic\PYZsq{}, interpolation=\PYZsq{}nearest\PYZsq{})}
\PY{n}{plot}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{img}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration: 1, named\_losses: [('ActivationMax Loss', -0.029343983), ('TV(2.0)
Loss', 0.09444798)], overall loss: 0.06510399281978607
Iteration: 2, named\_losses: [('ActivationMax Loss', 27.618795), ('TV(2.0) Loss',
551.72473)], overall loss: 579.343505859375
Iteration: 3, named\_losses: [('ActivationMax Loss', -53.254124), ('TV(2.0)
Loss', 214.1011)], overall loss: 160.84698486328125
Iteration: 4, named\_losses: [('ActivationMax Loss', -162.3963), ('TV(2.0) Loss',
178.28627)], overall loss: 15.889968872070312
Iteration: 5, named\_losses: [('ActivationMax Loss', -329.24582), ('TV(2.0)
Loss', 190.78125)], overall loss: -138.46456909179688
Iteration: 6, named\_losses: [('ActivationMax Loss', -446.4407), ('TV(2.0) Loss',
236.95157)], overall loss: -209.4891357421875
Iteration: 7, named\_losses: [('ActivationMax Loss', -553.3071), ('TV(2.0) Loss',
259.10437)], overall loss: -294.2027587890625
Iteration: 8, named\_losses: [('ActivationMax Loss', -643.4229), ('TV(2.0) Loss',
297.29596)], overall loss: -346.126953125
Iteration: 9, named\_losses: [('ActivationMax Loss', -713.5302), ('TV(2.0) Loss',
315.65384)], overall loss: -397.8763732910156
Iteration: 10, named\_losses: [('ActivationMax Loss', -793.4411), ('TV(2.0)
Loss', 354.47522)], overall loss: -438.96588134765625
Iteration: 11, named\_losses: [('ActivationMax Loss', -842.2541), ('TV(2.0)
Loss', 380.7285)], overall loss: -461.5256042480469
Iteration: 12, named\_losses: [('ActivationMax Loss', -902.3113), ('TV(2.0)
Loss', 407.1828)], overall loss: -495.12847900390625
Iteration: 13, named\_losses: [('ActivationMax Loss', -941.30975), ('TV(2.0)
Loss', 431.47275)], overall loss: -509.8370056152344
Iteration: 14, named\_losses: [('ActivationMax Loss', -983.52325), ('TV(2.0)
Loss', 452.41174)], overall loss: -531.1115112304688
Iteration: 15, named\_losses: [('ActivationMax Loss', -1030.4989), ('TV(2.0)
Loss', 470.71213)], overall loss: -559.7867431640625
Iteration: 16, named\_losses: [('ActivationMax Loss', -1051.6495), ('TV(2.0)
Loss', 471.84283)], overall loss: -579.8067016601562
Iteration: 17, named\_losses: [('ActivationMax Loss', -1093.2156), ('TV(2.0)
Loss', 494.59067)], overall loss: -598.6248779296875
Iteration: 18, named\_losses: [('ActivationMax Loss', -1121.3766), ('TV(2.0)
Loss', 509.76007)], overall loss: -611.6165161132812
Iteration: 19, named\_losses: [('ActivationMax Loss', -1139.5532), ('TV(2.0)
Loss', 529.16254)], overall loss: -610.3906860351562
Iteration: 20, named\_losses: [('ActivationMax Loss', -1178.4144), ('TV(2.0)
Loss', 536.6476)], overall loss: -641.766845703125
Iteration: 21, named\_losses: [('ActivationMax Loss', -1185.6025), ('TV(2.0)
Loss', 554.91327)], overall loss: -630.6892700195312
Iteration: 22, named\_losses: [('ActivationMax Loss', -1229.4319), ('TV(2.0)
Loss', 570.0529)], overall loss: -659.3789672851562
Iteration: 23, named\_losses: [('ActivationMax Loss', -1236.3905), ('TV(2.0)
Loss', 582.1664)], overall loss: -654.22412109375
Iteration: 24, named\_losses: [('ActivationMax Loss', -1269.4296), ('TV(2.0)
Loss', 589.34705)], overall loss: -680.08251953125
Iteration: 25, named\_losses: [('ActivationMax Loss', -1283.7299), ('TV(2.0)
Loss', 602.90906)], overall loss: -680.82080078125
Iteration: 26, named\_losses: [('ActivationMax Loss', -1313.6786), ('TV(2.0)
Loss', 615.6161)], overall loss: -698.0625
Iteration: 27, named\_losses: [('ActivationMax Loss', -1321.4624), ('TV(2.0)
Loss', 620.11615)], overall loss: -701.3462524414062
Iteration: 28, named\_losses: [('ActivationMax Loss', -1345.4363), ('TV(2.0)
Loss', 625.9033)], overall loss: -719.532958984375
Iteration: 29, named\_losses: [('ActivationMax Loss', -1351.3058), ('TV(2.0)
Loss', 639.3844)], overall loss: -711.92138671875
Iteration: 30, named\_losses: [('ActivationMax Loss', -1383.2675), ('TV(2.0)
Loss', 646.25433)], overall loss: -737.0131225585938
Iteration: 31, named\_losses: [('ActivationMax Loss', -1391.793), ('TV(2.0)
Loss', 661.8012)], overall loss: -729.9917602539062
Iteration: 32, named\_losses: [('ActivationMax Loss', -1414.9486), ('TV(2.0)
Loss', 670.5463)], overall loss: -744.4022827148438
Iteration: 33, named\_losses: [('ActivationMax Loss', -1414.6484), ('TV(2.0)
Loss', 670.17834)], overall loss: -744.4700927734375
Iteration: 34, named\_losses: [('ActivationMax Loss', -1444.4396), ('TV(2.0)
Loss', 689.67773)], overall loss: -754.7618408203125
Iteration: 35, named\_losses: [('ActivationMax Loss', -1445.1906), ('TV(2.0)
Loss', 691.7637)], overall loss: -753.4268798828125
Iteration: 36, named\_losses: [('ActivationMax Loss', -1471.892), ('TV(2.0)
Loss', 707.17413)], overall loss: -764.7178344726562
Iteration: 37, named\_losses: [('ActivationMax Loss', -1485.3118), ('TV(2.0)
Loss', 710.02185)], overall loss: -775.2899169921875
Iteration: 38, named\_losses: [('ActivationMax Loss', -1501.7599), ('TV(2.0)
Loss', 724.8281)], overall loss: -776.9317626953125
Iteration: 39, named\_losses: [('ActivationMax Loss', -1515.7671), ('TV(2.0)
Loss', 731.45013)], overall loss: -784.3169555664062
Iteration: 40, named\_losses: [('ActivationMax Loss', -1527.5938), ('TV(2.0)
Loss', 741.71643)], overall loss: -785.8773193359375
Iteration: 41, named\_losses: [('ActivationMax Loss', -1536.5825), ('TV(2.0)
Loss', 746.02155)], overall loss: -790.5609741210938
Iteration: 42, named\_losses: [('ActivationMax Loss', -1547.0154), ('TV(2.0)
Loss', 755.57715)], overall loss: -791.438232421875
Iteration: 43, named\_losses: [('ActivationMax Loss', -1550.7847), ('TV(2.0)
Loss', 752.96466)], overall loss: -797.8200073242188
Iteration: 44, named\_losses: [('ActivationMax Loss', -1565.4526), ('TV(2.0)
Loss', 763.1931)], overall loss: -802.259521484375
Iteration: 45, named\_losses: [('ActivationMax Loss', -1569.629), ('TV(2.0)
Loss', 763.9116)], overall loss: -805.7174072265625
Iteration: 46, named\_losses: [('ActivationMax Loss', -1582.2272), ('TV(2.0)
Loss', 774.11426)], overall loss: -808.1129150390625
Iteration: 47, named\_losses: [('ActivationMax Loss', -1578.568), ('TV(2.0)
Loss', 772.10876)], overall loss: -806.459228515625
Iteration: 48, named\_losses: [('ActivationMax Loss', -1601.7421), ('TV(2.0)
Loss', 786.25586)], overall loss: -815.4862060546875
Iteration: 49, named\_losses: [('ActivationMax Loss', -1605.0199), ('TV(2.0)
Loss', 790.3953)], overall loss: -814.6245727539062
Iteration: 50, named\_losses: [('ActivationMax Loss', -1612.6204), ('TV(2.0)
Loss', 796.4207)], overall loss: -816.1996459960938
Iteration: 51, named\_losses: [('ActivationMax Loss', -1617.4395), ('TV(2.0)
Loss', 797.8587)], overall loss: -819.5807495117188
Iteration: 52, named\_losses: [('ActivationMax Loss', -1623.712), ('TV(2.0)
Loss', 805.22064)], overall loss: -818.4913940429688
Iteration: 53, named\_losses: [('ActivationMax Loss', -1625.087), ('TV(2.0)
Loss', 807.80853)], overall loss: -817.2785034179688
Iteration: 54, named\_losses: [('ActivationMax Loss', -1638.0977), ('TV(2.0)
Loss', 813.8759)], overall loss: -824.2217407226562
Iteration: 55, named\_losses: [('ActivationMax Loss', -1639.7666), ('TV(2.0)
Loss', 814.1425)], overall loss: -825.6240844726562
Iteration: 56, named\_losses: [('ActivationMax Loss', -1648.1733), ('TV(2.0)
Loss', 825.08673)], overall loss: -823.0866088867188
Iteration: 57, named\_losses: [('ActivationMax Loss', -1647.9382), ('TV(2.0)
Loss', 820.1757)], overall loss: -827.7625122070312
Iteration: 58, named\_losses: [('ActivationMax Loss', -1660.1608), ('TV(2.0)
Loss', 834.38715)], overall loss: -825.7736206054688
Iteration: 59, named\_losses: [('ActivationMax Loss', -1656.8456), ('TV(2.0)
Loss', 826.6818)], overall loss: -830.1637573242188
Iteration: 60, named\_losses: [('ActivationMax Loss', -1662.6694), ('TV(2.0)
Loss', 837.18933)], overall loss: -825.4801025390625
Iteration: 61, named\_losses: [('ActivationMax Loss', -1662.572), ('TV(2.0)
Loss', 835.6509)], overall loss: -826.921142578125
Iteration: 62, named\_losses: [('ActivationMax Loss', -1667.7911), ('TV(2.0)
Loss', 841.293)], overall loss: -826.4981079101562
Iteration: 63, named\_losses: [('ActivationMax Loss', -1669.7709), ('TV(2.0)
Loss', 842.8576)], overall loss: -826.9132690429688
Iteration: 64, named\_losses: [('ActivationMax Loss', -1677.842), ('TV(2.0)
Loss', 847.86926)], overall loss: -829.9727783203125
Iteration: 65, named\_losses: [('ActivationMax Loss', -1678.9229), ('TV(2.0)
Loss', 851.1859)], overall loss: -827.7369384765625
Iteration: 66, named\_losses: [('ActivationMax Loss', -1687.3771), ('TV(2.0)
Loss', 855.9513)], overall loss: -831.42578125
Iteration: 67, named\_losses: [('ActivationMax Loss', -1683.6859), ('TV(2.0)
Loss', 855.9437)], overall loss: -827.7421875
Iteration: 68, named\_losses: [('ActivationMax Loss', -1690.2726), ('TV(2.0)
Loss', 855.78937)], overall loss: -834.4832153320312
Iteration: 69, named\_losses: [('ActivationMax Loss', -1684.843), ('TV(2.0)
Loss', 857.7921)], overall loss: -827.0509033203125
Iteration: 70, named\_losses: [('ActivationMax Loss', -1690.8168), ('TV(2.0)
Loss', 860.8235)], overall loss: -829.9932861328125
Iteration: 71, named\_losses: [('ActivationMax Loss', -1692.1891), ('TV(2.0)
Loss', 855.25476)], overall loss: -836.934326171875
Iteration: 72, named\_losses: [('ActivationMax Loss', -1697.8872), ('TV(2.0)
Loss', 866.84705)], overall loss: -831.0401611328125
Iteration: 73, named\_losses: [('ActivationMax Loss', -1702.8535), ('TV(2.0)
Loss', 868.3273)], overall loss: -834.5262451171875
Iteration: 74, named\_losses: [('ActivationMax Loss', -1701.4608), ('TV(2.0)
Loss', 868.78735)], overall loss: -832.6734619140625
Iteration: 75, named\_losses: [('ActivationMax Loss', -1703.0087), ('TV(2.0)
Loss', 869.66705)], overall loss: -833.3416137695312
Iteration: 76, named\_losses: [('ActivationMax Loss', -1704.1666), ('TV(2.0)
Loss', 875.2205)], overall loss: -828.9461059570312
Iteration: 77, named\_losses: [('ActivationMax Loss', -1716.4152), ('TV(2.0)
Loss', 879.5051)], overall loss: -836.9100341796875
Iteration: 78, named\_losses: [('ActivationMax Loss', -1706.346), ('TV(2.0)
Loss', 876.9546)], overall loss: -829.391357421875
Iteration: 79, named\_losses: [('ActivationMax Loss', -1709.241), ('TV(2.0)
Loss', 872.4084)], overall loss: -836.8325805664062
Iteration: 80, named\_losses: [('ActivationMax Loss', -1709.0377), ('TV(2.0)
Loss', 875.4469)], overall loss: -833.5908203125
Iteration: 81, named\_losses: [('ActivationMax Loss', -1717.1409), ('TV(2.0)
Loss', 878.2087)], overall loss: -838.9321899414062
Iteration: 82, named\_losses: [('ActivationMax Loss', -1715.5032), ('TV(2.0)
Loss', 882.2635)], overall loss: -833.2396850585938
Iteration: 83, named\_losses: [('ActivationMax Loss', -1709.4656), ('TV(2.0)
Loss', 874.32184)], overall loss: -835.1437377929688
Iteration: 84, named\_losses: [('ActivationMax Loss', -1722.7501), ('TV(2.0)
Loss', 883.26227)], overall loss: -839.4878540039062
Iteration: 85, named\_losses: [('ActivationMax Loss', -1714.9198), ('TV(2.0)
Loss', 878.4141)], overall loss: -836.5056762695312
Iteration: 86, named\_losses: [('ActivationMax Loss', -1719.8612), ('TV(2.0)
Loss', 879.5622)], overall loss: -840.2990112304688
Iteration: 87, named\_losses: [('ActivationMax Loss', -1714.2672), ('TV(2.0)
Loss', 881.02905)], overall loss: -833.2381591796875
Iteration: 88, named\_losses: [('ActivationMax Loss', -1722.5328), ('TV(2.0)
Loss', 881.8342)], overall loss: -840.6986083984375
Iteration: 89, named\_losses: [('ActivationMax Loss', -1715.5873), ('TV(2.0)
Loss', 879.1525)], overall loss: -836.4347534179688
Iteration: 90, named\_losses: [('ActivationMax Loss', -1723.9076), ('TV(2.0)
Loss', 884.9597)], overall loss: -838.9478759765625
Iteration: 91, named\_losses: [('ActivationMax Loss', -1724.7551), ('TV(2.0)
Loss', 881.91473)], overall loss: -842.8403930664062
Iteration: 92, named\_losses: [('ActivationMax Loss', -1722.7678), ('TV(2.0)
Loss', 886.67)], overall loss: -836.0978393554688
Iteration: 93, named\_losses: [('ActivationMax Loss', -1726.3252), ('TV(2.0)
Loss', 883.20526)], overall loss: -843.1199340820312
Iteration: 94, named\_losses: [('ActivationMax Loss', -1726.3694), ('TV(2.0)
Loss', 887.4328)], overall loss: -838.9365844726562
Iteration: 95, named\_losses: [('ActivationMax Loss', -1729.9131), ('TV(2.0)
Loss', 888.6173)], overall loss: -841.2957763671875
Iteration: 96, named\_losses: [('ActivationMax Loss', -1726.5479), ('TV(2.0)
Loss', 885.955)], overall loss: -840.5928344726562
Iteration: 97, named\_losses: [('ActivationMax Loss', -1733.1432), ('TV(2.0)
Loss', 891.12103)], overall loss: -842.0221557617188
Iteration: 98, named\_losses: [('ActivationMax Loss', -1732.2146), ('TV(2.0)
Loss', 889.3462)], overall loss: -842.868408203125
Iteration: 99, named\_losses: [('ActivationMax Loss', -1734.9366), ('TV(2.0)
Loss', 896.6492)], overall loss: -838.2874755859375
Iteration: 100, named\_losses: [('ActivationMax Loss', -1736.1516), ('TV(2.0)
Loss', 899.7575)], overall loss: -836.3941040039062
Iteration: 101, named\_losses: [('ActivationMax Loss', -1737.8312), ('TV(2.0)
Loss', 902.15405)], overall loss: -835.6771240234375
Iteration: 102, named\_losses: [('ActivationMax Loss', -1734.101), ('TV(2.0)
Loss', 893.29346)], overall loss: -840.8074951171875
Iteration: 103, named\_losses: [('ActivationMax Loss', -1737.0436), ('TV(2.0)
Loss', 894.9081)], overall loss: -842.135498046875
Iteration: 104, named\_losses: [('ActivationMax Loss', -1733.4213), ('TV(2.0)
Loss', 894.3747)], overall loss: -839.0465698242188
Iteration: 105, named\_losses: [('ActivationMax Loss', -1735.4603), ('TV(2.0)
Loss', 894.7455)], overall loss: -840.71484375
Iteration: 106, named\_losses: [('ActivationMax Loss', -1730.2012), ('TV(2.0)
Loss', 889.83936)], overall loss: -840.36181640625
Iteration: 107, named\_losses: [('ActivationMax Loss', -1734.965), ('TV(2.0)
Loss', 896.2839)], overall loss: -838.6810913085938
Iteration: 108, named\_losses: [('ActivationMax Loss', -1734.3077), ('TV(2.0)
Loss', 896.5454)], overall loss: -837.7623291015625
Iteration: 109, named\_losses: [('ActivationMax Loss', -1741.4989), ('TV(2.0)
Loss', 894.26917)], overall loss: -847.229736328125
Iteration: 110, named\_losses: [('ActivationMax Loss', -1729.8446), ('TV(2.0)
Loss', 893.6425)], overall loss: -836.2020874023438
Iteration: 111, named\_losses: [('ActivationMax Loss', -1735.3884), ('TV(2.0)
Loss', 893.4215)], overall loss: -841.9669189453125
Iteration: 112, named\_losses: [('ActivationMax Loss', -1732.0072), ('TV(2.0)
Loss', 897.4369)], overall loss: -834.5703125
Iteration: 113, named\_losses: [('ActivationMax Loss', -1741.1107), ('TV(2.0)
Loss', 898.5395)], overall loss: -842.5712280273438
Iteration: 114, named\_losses: [('ActivationMax Loss', -1741.7645), ('TV(2.0)
Loss', 904.548)], overall loss: -837.216552734375
Iteration: 115, named\_losses: [('ActivationMax Loss', -1743.8809), ('TV(2.0)
Loss', 900.63184)], overall loss: -843.2490234375
Iteration: 116, named\_losses: [('ActivationMax Loss', -1732.8018), ('TV(2.0)
Loss', 892.78503)], overall loss: -840.0167236328125
Iteration: 117, named\_losses: [('ActivationMax Loss', -1742.012), ('TV(2.0)
Loss', 898.77954)], overall loss: -843.232421875
Iteration: 118, named\_losses: [('ActivationMax Loss', -1732.1237), ('TV(2.0)
Loss', 892.6599)], overall loss: -839.4637451171875
Iteration: 119, named\_losses: [('ActivationMax Loss', -1743.552), ('TV(2.0)
Loss', 902.23706)], overall loss: -841.31494140625
Iteration: 120, named\_losses: [('ActivationMax Loss', -1741.6824), ('TV(2.0)
Loss', 899.8635)], overall loss: -841.81884765625
Iteration: 121, named\_losses: [('ActivationMax Loss', -1747.4529), ('TV(2.0)
Loss', 906.9834)], overall loss: -840.469482421875
Iteration: 122, named\_losses: [('ActivationMax Loss', -1740.0771), ('TV(2.0)
Loss', 892.05524)], overall loss: -848.0219116210938
Iteration: 123, named\_losses: [('ActivationMax Loss', -1739.1179), ('TV(2.0)
Loss', 902.6399)], overall loss: -836.47802734375
Iteration: 124, named\_losses: [('ActivationMax Loss', -1740.5756), ('TV(2.0)
Loss', 899.34753)], overall loss: -841.22802734375
Iteration: 125, named\_losses: [('ActivationMax Loss', -1738.3301), ('TV(2.0)
Loss', 901.9262)], overall loss: -836.4038696289062
Iteration: 126, named\_losses: [('ActivationMax Loss', -1744.8046), ('TV(2.0)
Loss', 899.31836)], overall loss: -845.4862060546875
Iteration: 127, named\_losses: [('ActivationMax Loss', -1739.9725), ('TV(2.0)
Loss', 902.9614)], overall loss: -837.0111083984375
Iteration: 128, named\_losses: [('ActivationMax Loss', -1741.5216), ('TV(2.0)
Loss', 899.66455)], overall loss: -841.8570556640625
Iteration: 129, named\_losses: [('ActivationMax Loss', -1742.8098), ('TV(2.0)
Loss', 901.58936)], overall loss: -841.220458984375
Iteration: 130, named\_losses: [('ActivationMax Loss', -1737.675), ('TV(2.0)
Loss', 897.56696)], overall loss: -840.1080932617188
Iteration: 131, named\_losses: [('ActivationMax Loss', -1748.944), ('TV(2.0)
Loss', 908.5812)], overall loss: -840.36279296875
Iteration: 132, named\_losses: [('ActivationMax Loss', -1739.2427), ('TV(2.0)
Loss', 899.0547)], overall loss: -840.18798828125
Iteration: 133, named\_losses: [('ActivationMax Loss', -1743.2185), ('TV(2.0)
Loss', 901.4914)], overall loss: -841.7271118164062
Iteration: 134, named\_losses: [('ActivationMax Loss', -1742.2633), ('TV(2.0)
Loss', 903.0682)], overall loss: -839.1951293945312
Iteration: 135, named\_losses: [('ActivationMax Loss', -1750.587), ('TV(2.0)
Loss', 906.68774)], overall loss: -843.8992919921875
Iteration: 136, named\_losses: [('ActivationMax Loss', -1742.4855), ('TV(2.0)
Loss', 902.98206)], overall loss: -839.50341796875
Iteration: 137, named\_losses: [('ActivationMax Loss', -1748.2422), ('TV(2.0)
Loss', 905.645)], overall loss: -842.59716796875
Iteration: 138, named\_losses: [('ActivationMax Loss', -1742.6428), ('TV(2.0)
Loss', 900.36694)], overall loss: -842.27587890625
Iteration: 139, named\_losses: [('ActivationMax Loss', -1749.3274), ('TV(2.0)
Loss', 905.2664)], overall loss: -844.0609741210938
Iteration: 140, named\_losses: [('ActivationMax Loss', -1743.9142), ('TV(2.0)
Loss', 904.94806)], overall loss: -838.9661254882812
Iteration: 141, named\_losses: [('ActivationMax Loss', -1755.9244), ('TV(2.0)
Loss', 907.1235)], overall loss: -848.8009643554688
Iteration: 142, named\_losses: [('ActivationMax Loss', -1746.1758), ('TV(2.0)
Loss', 904.04395)], overall loss: -842.1318359375
Iteration: 143, named\_losses: [('ActivationMax Loss', -1754.9893), ('TV(2.0)
Loss', 903.7701)], overall loss: -851.2191772460938
Iteration: 144, named\_losses: [('ActivationMax Loss', -1743.9016), ('TV(2.0)
Loss', 897.6136)], overall loss: -846.2880249023438
Iteration: 145, named\_losses: [('ActivationMax Loss', -1752.8274), ('TV(2.0)
Loss', 901.5882)], overall loss: -851.2391967773438
Iteration: 146, named\_losses: [('ActivationMax Loss', -1753.1854), ('TV(2.0)
Loss', 903.7091)], overall loss: -849.476318359375
Iteration: 147, named\_losses: [('ActivationMax Loss', -1757.8027), ('TV(2.0)
Loss', 903.95074)], overall loss: -853.8519897460938
Iteration: 148, named\_losses: [('ActivationMax Loss', -1760.4886), ('TV(2.0)
Loss', 909.67413)], overall loss: -850.8145141601562
Iteration: 149, named\_losses: [('ActivationMax Loss', -1764.2368), ('TV(2.0)
Loss', 906.775)], overall loss: -857.4617919921875
Iteration: 150, named\_losses: [('ActivationMax Loss', -1767.4458), ('TV(2.0)
Loss', 915.0569)], overall loss: -852.388916015625
Iteration: 151, named\_losses: [('ActivationMax Loss', -1772.9386), ('TV(2.0)
Loss', 912.11285)], overall loss: -860.8257446289062
Iteration: 152, named\_losses: [('ActivationMax Loss', -1770.5524), ('TV(2.0)
Loss', 916.8508)], overall loss: -853.7015380859375
Iteration: 153, named\_losses: [('ActivationMax Loss', -1774.8292), ('TV(2.0)
Loss', 914.6961)], overall loss: -860.1331176757812
Iteration: 154, named\_losses: [('ActivationMax Loss', -1769.0146), ('TV(2.0)
Loss', 915.852)], overall loss: -853.1626586914062
Iteration: 155, named\_losses: [('ActivationMax Loss', -1772.2351), ('TV(2.0)
Loss', 913.3444)], overall loss: -858.8906860351562
Iteration: 156, named\_losses: [('ActivationMax Loss', -1767.8215), ('TV(2.0)
Loss', 914.35535)], overall loss: -853.4661865234375
Iteration: 157, named\_losses: [('ActivationMax Loss', -1775.3704), ('TV(2.0)
Loss', 916.26135)], overall loss: -859.1090087890625
Iteration: 158, named\_losses: [('ActivationMax Loss', -1766.705), ('TV(2.0)
Loss', 912.5623)], overall loss: -854.1426391601562
Iteration: 159, named\_losses: [('ActivationMax Loss', -1777.3323), ('TV(2.0)
Loss', 921.9451)], overall loss: -855.3871459960938
Iteration: 160, named\_losses: [('ActivationMax Loss', -1772.6292), ('TV(2.0)
Loss', 917.96173)], overall loss: -854.6674194335938
Iteration: 161, named\_losses: [('ActivationMax Loss', -1780.1969), ('TV(2.0)
Loss', 926.11066)], overall loss: -854.0862426757812
Iteration: 162, named\_losses: [('ActivationMax Loss', -1778.1902), ('TV(2.0)
Loss', 920.30786)], overall loss: -857.88232421875
Iteration: 163, named\_losses: [('ActivationMax Loss', -1778.6886), ('TV(2.0)
Loss', 923.11383)], overall loss: -855.5747680664062
Iteration: 164, named\_losses: [('ActivationMax Loss', -1774.9828), ('TV(2.0)
Loss', 922.35754)], overall loss: -852.625244140625
Iteration: 165, named\_losses: [('ActivationMax Loss', -1791.7542), ('TV(2.0)
Loss', 932.83905)], overall loss: -858.9151000976562
Iteration: 166, named\_losses: [('ActivationMax Loss', -1779.2656), ('TV(2.0)
Loss', 921.77454)], overall loss: -857.4910888671875
Iteration: 167, named\_losses: [('ActivationMax Loss', -1787.3125), ('TV(2.0)
Loss', 931.33496)], overall loss: -855.9775390625
Iteration: 168, named\_losses: [('ActivationMax Loss', -1779.4229), ('TV(2.0)
Loss', 923.1934)], overall loss: -856.2294311523438
Iteration: 169, named\_losses: [('ActivationMax Loss', -1779.9714), ('TV(2.0)
Loss', 923.2109)], overall loss: -856.7605590820312
Iteration: 170, named\_losses: [('ActivationMax Loss', -1779.9297), ('TV(2.0)
Loss', 922.16174)], overall loss: -857.7679443359375
Iteration: 171, named\_losses: [('ActivationMax Loss', -1782.6515), ('TV(2.0)
Loss', 923.0252)], overall loss: -859.6262817382812
Iteration: 172, named\_losses: [('ActivationMax Loss', -1779.4375), ('TV(2.0)
Loss', 925.8848)], overall loss: -853.5526733398438
Iteration: 173, named\_losses: [('ActivationMax Loss', -1786.3438), ('TV(2.0)
Loss', 926.6954)], overall loss: -859.6483764648438
Iteration: 174, named\_losses: [('ActivationMax Loss', -1777.2191), ('TV(2.0)
Loss', 919.5133)], overall loss: -857.705810546875
Iteration: 175, named\_losses: [('ActivationMax Loss', -1781.1141), ('TV(2.0)
Loss', 926.3178)], overall loss: -854.7963256835938
Iteration: 176, named\_losses: [('ActivationMax Loss', -1782.667), ('TV(2.0)
Loss', 926.0495)], overall loss: -856.6174926757812
Iteration: 177, named\_losses: [('ActivationMax Loss', -1780.7076), ('TV(2.0)
Loss', 920.7041)], overall loss: -860.0035400390625
Iteration: 178, named\_losses: [('ActivationMax Loss', -1784.1094), ('TV(2.0)
Loss', 927.5142)], overall loss: -856.5951538085938
Iteration: 179, named\_losses: [('ActivationMax Loss', -1776.0343), ('TV(2.0)
Loss', 917.02496)], overall loss: -859.0093383789062
Iteration: 180, named\_losses: [('ActivationMax Loss', -1788.5261), ('TV(2.0)
Loss', 930.3302)], overall loss: -858.1959228515625
Iteration: 181, named\_losses: [('ActivationMax Loss', -1780.8711), ('TV(2.0)
Loss', 921.6631)], overall loss: -859.2080078125
Iteration: 182, named\_losses: [('ActivationMax Loss', -1790.8639), ('TV(2.0)
Loss', 926.87286)], overall loss: -863.9910278320312
Iteration: 183, named\_losses: [('ActivationMax Loss', -1775.28), ('TV(2.0)
Loss', 922.36206)], overall loss: -852.91796875
Iteration: 184, named\_losses: [('ActivationMax Loss', -1784.8848), ('TV(2.0)
Loss', 919.0549)], overall loss: -865.8298950195312
Iteration: 185, named\_losses: [('ActivationMax Loss', -1779.2463), ('TV(2.0)
Loss', 920.3541)], overall loss: -858.8922119140625
Iteration: 186, named\_losses: [('ActivationMax Loss', -1783.8528), ('TV(2.0)
Loss', 926.1878)], overall loss: -857.6649780273438
Iteration: 187, named\_losses: [('ActivationMax Loss', -1772.612), ('TV(2.0)
Loss', 916.45087)], overall loss: -856.1611938476562
Iteration: 188, named\_losses: [('ActivationMax Loss', -1779.2585), ('TV(2.0)
Loss', 924.32184)], overall loss: -854.9367065429688
Iteration: 189, named\_losses: [('ActivationMax Loss', -1777.1254), ('TV(2.0)
Loss', 918.811)], overall loss: -858.3143920898438
Iteration: 190, named\_losses: [('ActivationMax Loss', -1776.9243), ('TV(2.0)
Loss', 923.0841)], overall loss: -853.8402099609375
Iteration: 191, named\_losses: [('ActivationMax Loss', -1777.0421), ('TV(2.0)
Loss', 916.1661)], overall loss: -860.8760375976562
Iteration: 192, named\_losses: [('ActivationMax Loss', -1776.9667), ('TV(2.0)
Loss', 922.8111)], overall loss: -854.1555786132812
Iteration: 193, named\_losses: [('ActivationMax Loss', -1777.4564), ('TV(2.0)
Loss', 919.84534)], overall loss: -857.611083984375
Iteration: 194, named\_losses: [('ActivationMax Loss', -1785.5187), ('TV(2.0)
Loss', 927.6562)], overall loss: -857.8624877929688
Iteration: 195, named\_losses: [('ActivationMax Loss', -1776.3806), ('TV(2.0)
Loss', 917.8089)], overall loss: -858.5717163085938
Iteration: 196, named\_losses: [('ActivationMax Loss', -1789.1107), ('TV(2.0)
Loss', 929.4763)], overall loss: -859.6343994140625
Iteration: 197, named\_losses: [('ActivationMax Loss', -1776.1287), ('TV(2.0)
Loss', 920.37494)], overall loss: -855.7537231445312
Iteration: 198, named\_losses: [('ActivationMax Loss', -1783.1157), ('TV(2.0)
Loss', 922.4617)], overall loss: -860.654052734375
Iteration: 199, named\_losses: [('ActivationMax Loss', -1781.4087), ('TV(2.0)
Loss', 924.1254)], overall loss: -857.2832641601562
Iteration: 200, named\_losses: [('ActivationMax Loss', -1788.0239), ('TV(2.0)
Loss', 926.79425)], overall loss: -861.2296752929688
Iteration: 201, named\_losses: [('ActivationMax Loss', -1782.8563), ('TV(2.0)
Loss', 921.9799)], overall loss: -860.8764038085938
Iteration: 202, named\_losses: [('ActivationMax Loss', -1790.6989), ('TV(2.0)
Loss', 933.4914)], overall loss: -857.2074584960938
Iteration: 203, named\_losses: [('ActivationMax Loss', -1786.9172), ('TV(2.0)
Loss', 928.16223)], overall loss: -858.7550048828125
Iteration: 204, named\_losses: [('ActivationMax Loss', -1787.4955), ('TV(2.0)
Loss', 928.508)], overall loss: -858.9874877929688
Iteration: 205, named\_losses: [('ActivationMax Loss', -1781.4592), ('TV(2.0)
Loss', 925.4239)], overall loss: -856.0353393554688
Iteration: 206, named\_losses: [('ActivationMax Loss', -1784.45), ('TV(2.0)
Loss', 922.11176)], overall loss: -862.3381958007812
Iteration: 207, named\_losses: [('ActivationMax Loss', -1786.6995), ('TV(2.0)
Loss', 926.3235)], overall loss: -860.3759765625
Iteration: 208, named\_losses: [('ActivationMax Loss', -1782.1794), ('TV(2.0)
Loss', 920.3594)], overall loss: -861.820068359375
Iteration: 209, named\_losses: [('ActivationMax Loss', -1783.3657), ('TV(2.0)
Loss', 925.101)], overall loss: -858.2647094726562
Iteration: 210, named\_losses: [('ActivationMax Loss', -1779.0298), ('TV(2.0)
Loss', 919.9838)], overall loss: -859.0459594726562
Iteration: 211, named\_losses: [('ActivationMax Loss', -1777.4858), ('TV(2.0)
Loss', 915.7917)], overall loss: -861.6941528320312
Iteration: 212, named\_losses: [('ActivationMax Loss', -1785.1555), ('TV(2.0)
Loss', 930.757)], overall loss: -854.3984985351562
Iteration: 213, named\_losses: [('ActivationMax Loss', -1781.204), ('TV(2.0)
Loss', 921.4743)], overall loss: -859.7296752929688
Iteration: 214, named\_losses: [('ActivationMax Loss', -1780.5852), ('TV(2.0)
Loss', 923.23987)], overall loss: -857.3453369140625
Iteration: 215, named\_losses: [('ActivationMax Loss', -1778.8047), ('TV(2.0)
Loss', 921.5512)], overall loss: -857.2534790039062
Iteration: 216, named\_losses: [('ActivationMax Loss', -1784.427), ('TV(2.0)
Loss', 923.5872)], overall loss: -860.8397827148438
Iteration: 217, named\_losses: [('ActivationMax Loss', -1779.8838), ('TV(2.0)
Loss', 919.8364)], overall loss: -860.04736328125
Iteration: 218, named\_losses: [('ActivationMax Loss', -1782.7518), ('TV(2.0)
Loss', 924.3401)], overall loss: -858.4117431640625
Iteration: 219, named\_losses: [('ActivationMax Loss', -1778.192), ('TV(2.0)
Loss', 917.3307)], overall loss: -860.861328125
Iteration: 220, named\_losses: [('ActivationMax Loss', -1781.138), ('TV(2.0)
Loss', 921.88043)], overall loss: -859.2575073242188
Iteration: 221, named\_losses: [('ActivationMax Loss', -1784.1942), ('TV(2.0)
Loss', 920.31683)], overall loss: -863.8773803710938
Iteration: 222, named\_losses: [('ActivationMax Loss', -1788.0006), ('TV(2.0)
Loss', 927.9088)], overall loss: -860.091796875
Iteration: 223, named\_losses: [('ActivationMax Loss', -1786.2373), ('TV(2.0)
Loss', 923.2448)], overall loss: -862.9924926757812
Iteration: 224, named\_losses: [('ActivationMax Loss', -1786.1659), ('TV(2.0)
Loss', 925.8376)], overall loss: -860.3283081054688
Iteration: 225, named\_losses: [('ActivationMax Loss', -1787.6274), ('TV(2.0)
Loss', 920.4063)], overall loss: -867.2211303710938
Iteration: 226, named\_losses: [('ActivationMax Loss', -1786.7687), ('TV(2.0)
Loss', 927.01465)], overall loss: -859.7540283203125
Iteration: 227, named\_losses: [('ActivationMax Loss', -1781.3198), ('TV(2.0)
Loss', 915.34546)], overall loss: -865.974365234375
Iteration: 228, named\_losses: [('ActivationMax Loss', -1788.8811), ('TV(2.0)
Loss', 922.04865)], overall loss: -866.8324584960938
Iteration: 229, named\_losses: [('ActivationMax Loss', -1776.5078), ('TV(2.0)
Loss', 912.45593)], overall loss: -864.0518798828125
Iteration: 230, named\_losses: [('ActivationMax Loss', -1786.6088), ('TV(2.0)
Loss', 924.44073)], overall loss: -862.1680297851562
Iteration: 231, named\_losses: [('ActivationMax Loss', -1778.1134), ('TV(2.0)
Loss', 915.4746)], overall loss: -862.6387939453125
Iteration: 232, named\_losses: [('ActivationMax Loss', -1785.1464), ('TV(2.0)
Loss', 917.8337)], overall loss: -867.3126831054688
Iteration: 233, named\_losses: [('ActivationMax Loss', -1782.361), ('TV(2.0)
Loss', 913.36176)], overall loss: -868.9992065429688
Iteration: 234, named\_losses: [('ActivationMax Loss', -1783.9071), ('TV(2.0)
Loss', 920.7823)], overall loss: -863.1248168945312
Iteration: 235, named\_losses: [('ActivationMax Loss', -1785.2236), ('TV(2.0)
Loss', 922.45044)], overall loss: -862.773193359375
Iteration: 236, named\_losses: [('ActivationMax Loss', -1785.3158), ('TV(2.0)
Loss', 920.98083)], overall loss: -864.3349609375
Iteration: 237, named\_losses: [('ActivationMax Loss', -1783.2664), ('TV(2.0)
Loss', 917.4914)], overall loss: -865.7749633789062
Iteration: 238, named\_losses: [('ActivationMax Loss', -1789.2587), ('TV(2.0)
Loss', 926.6506)], overall loss: -862.6080932617188
Iteration: 239, named\_losses: [('ActivationMax Loss', -1790.7946), ('TV(2.0)
Loss', 923.72156)], overall loss: -867.072998046875
Iteration: 240, named\_losses: [('ActivationMax Loss', -1787.3966), ('TV(2.0)
Loss', 922.84296)], overall loss: -864.5536499023438
Iteration: 241, named\_losses: [('ActivationMax Loss', -1782.3875), ('TV(2.0)
Loss', 917.7674)], overall loss: -864.6200561523438
Iteration: 242, named\_losses: [('ActivationMax Loss', -1790.3557), ('TV(2.0)
Loss', 926.7892)], overall loss: -863.5665283203125
Iteration: 243, named\_losses: [('ActivationMax Loss', -1787.908), ('TV(2.0)
Loss', 922.4837)], overall loss: -865.4242553710938
Iteration: 244, named\_losses: [('ActivationMax Loss', -1791.2115), ('TV(2.0)
Loss', 930.12164)], overall loss: -861.0899047851562
Iteration: 245, named\_losses: [('ActivationMax Loss', -1790.8466), ('TV(2.0)
Loss', 921.352)], overall loss: -869.4945678710938
Iteration: 246, named\_losses: [('ActivationMax Loss', -1789.7833), ('TV(2.0)
Loss', 926.57764)], overall loss: -863.2056884765625
Iteration: 247, named\_losses: [('ActivationMax Loss', -1791.9896), ('TV(2.0)
Loss', 925.99994)], overall loss: -865.9896850585938
Iteration: 248, named\_losses: [('ActivationMax Loss', -1791.9065), ('TV(2.0)
Loss', 928.2016)], overall loss: -863.7048950195312
Iteration: 249, named\_losses: [('ActivationMax Loss', -1796.791), ('TV(2.0)
Loss', 933.0487)], overall loss: -863.7423095703125
Iteration: 250, named\_losses: [('ActivationMax Loss', -1793.9606), ('TV(2.0)
Loss', 928.35626)], overall loss: -865.6043090820312
Iteration: 251, named\_losses: [('ActivationMax Loss', -1794.9644), ('TV(2.0)
Loss', 929.3618)], overall loss: -865.6025390625
Iteration: 252, named\_losses: [('ActivationMax Loss', -1794.287), ('TV(2.0)
Loss', 928.0588)], overall loss: -866.2282104492188
Iteration: 253, named\_losses: [('ActivationMax Loss', -1787.0071), ('TV(2.0)
Loss', 921.45026)], overall loss: -865.5568237304688
Iteration: 254, named\_losses: [('ActivationMax Loss', -1789.8502), ('TV(2.0)
Loss', 927.18097)], overall loss: -862.6692504882812
Iteration: 255, named\_losses: [('ActivationMax Loss', -1793.5093), ('TV(2.0)
Loss', 928.309)], overall loss: -865.2002563476562
Iteration: 256, named\_losses: [('ActivationMax Loss', -1794.1934), ('TV(2.0)
Loss', 928.6669)], overall loss: -865.5264892578125
Iteration: 257, named\_losses: [('ActivationMax Loss', -1791.3473), ('TV(2.0)
Loss', 927.498)], overall loss: -863.8493041992188
Iteration: 258, named\_losses: [('ActivationMax Loss', -1792.3365), ('TV(2.0)
Loss', 927.6046)], overall loss: -864.73193359375
Iteration: 259, named\_losses: [('ActivationMax Loss', -1793.2385), ('TV(2.0)
Loss', 924.34375)], overall loss: -868.894775390625
Iteration: 260, named\_losses: [('ActivationMax Loss', -1795.3722), ('TV(2.0)
Loss', 929.0833)], overall loss: -866.2888793945312
Iteration: 261, named\_losses: [('ActivationMax Loss', -1788.8607), ('TV(2.0)
Loss', 921.14307)], overall loss: -867.7176513671875
Iteration: 262, named\_losses: [('ActivationMax Loss', -1800.8928), ('TV(2.0)
Loss', 937.0376)], overall loss: -863.855224609375
Iteration: 263, named\_losses: [('ActivationMax Loss', -1787.4388), ('TV(2.0)
Loss', 921.3137)], overall loss: -866.1251220703125
Iteration: 264, named\_losses: [('ActivationMax Loss', -1791.1844), ('TV(2.0)
Loss', 929.18195)], overall loss: -862.0025024414062
Iteration: 265, named\_losses: [('ActivationMax Loss', -1792.0457), ('TV(2.0)
Loss', 924.5368)], overall loss: -867.5088500976562
Iteration: 266, named\_losses: [('ActivationMax Loss', -1797.8748), ('TV(2.0)
Loss', 931.34753)], overall loss: -866.5272216796875
Iteration: 267, named\_losses: [('ActivationMax Loss', -1794.1027), ('TV(2.0)
Loss', 926.22253)], overall loss: -867.880126953125
Iteration: 268, named\_losses: [('ActivationMax Loss', -1791.397), ('TV(2.0)
Loss', 928.2493)], overall loss: -863.1476440429688
Iteration: 269, named\_losses: [('ActivationMax Loss', -1791.6678), ('TV(2.0)
Loss', 922.78516)], overall loss: -868.8826904296875
Iteration: 270, named\_losses: [('ActivationMax Loss', -1794.6284), ('TV(2.0)
Loss', 930.2826)], overall loss: -864.3458251953125
Iteration: 271, named\_losses: [('ActivationMax Loss', -1794.3267), ('TV(2.0)
Loss', 923.6022)], overall loss: -870.7244873046875
Iteration: 272, named\_losses: [('ActivationMax Loss', -1792.8058), ('TV(2.0)
Loss', 929.5619)], overall loss: -863.243896484375
Iteration: 273, named\_losses: [('ActivationMax Loss', -1788.9182), ('TV(2.0)
Loss', 921.908)], overall loss: -867.0101928710938
Iteration: 274, named\_losses: [('ActivationMax Loss', -1796.5508), ('TV(2.0)
Loss', 929.3936)], overall loss: -867.1571655273438
Iteration: 275, named\_losses: [('ActivationMax Loss', -1797.3589), ('TV(2.0)
Loss', 929.2176)], overall loss: -868.1412963867188
Iteration: 276, named\_losses: [('ActivationMax Loss', -1800.6882), ('TV(2.0)
Loss', 934.89105)], overall loss: -865.7971801757812
Iteration: 277, named\_losses: [('ActivationMax Loss', -1795.3037), ('TV(2.0)
Loss', 927.07935)], overall loss: -868.224365234375
Iteration: 278, named\_losses: [('ActivationMax Loss', -1798.6934), ('TV(2.0)
Loss', 934.8818)], overall loss: -863.8115844726562
Iteration: 279, named\_losses: [('ActivationMax Loss', -1794.3809), ('TV(2.0)
Loss', 926.52826)], overall loss: -867.8526000976562
Iteration: 280, named\_losses: [('ActivationMax Loss', -1798.504), ('TV(2.0)
Loss', 932.18506)], overall loss: -866.3189697265625
Iteration: 281, named\_losses: [('ActivationMax Loss', -1786.6559), ('TV(2.0)
Loss', 921.36523)], overall loss: -865.2906494140625
Iteration: 282, named\_losses: [('ActivationMax Loss', -1801.3433), ('TV(2.0)
Loss', 933.8129)], overall loss: -867.5303344726562
Iteration: 283, named\_losses: [('ActivationMax Loss', -1790.9874), ('TV(2.0)
Loss', 924.9054)], overall loss: -866.08203125
Iteration: 284, named\_losses: [('ActivationMax Loss', -1796.9346), ('TV(2.0)
Loss', 932.8323)], overall loss: -864.102294921875
Iteration: 285, named\_losses: [('ActivationMax Loss', -1792.6696), ('TV(2.0)
Loss', 924.27673)], overall loss: -868.392822265625
Iteration: 286, named\_losses: [('ActivationMax Loss', -1801.6647), ('TV(2.0)
Loss', 931.82495)], overall loss: -869.8397216796875
Iteration: 287, named\_losses: [('ActivationMax Loss', -1799.2506), ('TV(2.0)
Loss', 935.78284)], overall loss: -863.4677734375
Iteration: 288, named\_losses: [('ActivationMax Loss', -1797.6622), ('TV(2.0)
Loss', 930.17194)], overall loss: -867.4902954101562
Iteration: 289, named\_losses: [('ActivationMax Loss', -1796.007), ('TV(2.0)
Loss', 933.4664)], overall loss: -862.5405883789062
Iteration: 290, named\_losses: [('ActivationMax Loss', -1796.1829), ('TV(2.0)
Loss', 926.1851)], overall loss: -869.9977416992188
Iteration: 291, named\_losses: [('ActivationMax Loss', -1791.6826), ('TV(2.0)
Loss', 926.15497)], overall loss: -865.5276489257812
Iteration: 292, named\_losses: [('ActivationMax Loss', -1796.0728), ('TV(2.0)
Loss', 927.3772)], overall loss: -868.695556640625
Iteration: 293, named\_losses: [('ActivationMax Loss', -1792.9648), ('TV(2.0)
Loss', 927.4318)], overall loss: -865.5330200195312
Iteration: 294, named\_losses: [('ActivationMax Loss', -1799.2947), ('TV(2.0)
Loss', 932.53125)], overall loss: -866.763427734375
Iteration: 295, named\_losses: [('ActivationMax Loss', -1793.5458), ('TV(2.0)
Loss', 923.8872)], overall loss: -869.6585693359375
Iteration: 296, named\_losses: [('ActivationMax Loss', -1798.2609), ('TV(2.0)
Loss', 933.4635)], overall loss: -864.79736328125
Iteration: 297, named\_losses: [('ActivationMax Loss', -1799.306), ('TV(2.0)
Loss', 928.6541)], overall loss: -870.6519165039062
Iteration: 298, named\_losses: [('ActivationMax Loss', -1795.6725), ('TV(2.0)
Loss', 932.4895)], overall loss: -863.1829833984375
Iteration: 299, named\_losses: [('ActivationMax Loss', -1795.2223), ('TV(2.0)
Loss', 928.2991)], overall loss: -866.9232177734375
Iteration: 300, named\_losses: [('ActivationMax Loss', -1800.1659), ('TV(2.0)
Loss', 934.17426)], overall loss: -865.9916381835938
Iteration: 301, named\_losses: [('ActivationMax Loss', -1794.1279), ('TV(2.0)
Loss', 926.70953)], overall loss: -867.4183959960938
Iteration: 302, named\_losses: [('ActivationMax Loss', -1798.9501), ('TV(2.0)
Loss', 929.54816)], overall loss: -869.4019165039062
Iteration: 303, named\_losses: [('ActivationMax Loss', -1796.3287), ('TV(2.0)
Loss', 930.2878)], overall loss: -866.0409545898438
Iteration: 304, named\_losses: [('ActivationMax Loss', -1796.3445), ('TV(2.0)
Loss', 925.3735)], overall loss: -870.9710083007812
Iteration: 305, named\_losses: [('ActivationMax Loss', -1796.7427), ('TV(2.0)
Loss', 928.72485)], overall loss: -868.017822265625
Iteration: 306, named\_losses: [('ActivationMax Loss', -1798.4036), ('TV(2.0)
Loss', 929.40027)], overall loss: -869.0032958984375
Iteration: 307, named\_losses: [('ActivationMax Loss', -1799.536), ('TV(2.0)
Loss', 930.29504)], overall loss: -869.240966796875
Iteration: 308, named\_losses: [('ActivationMax Loss', -1795.0199), ('TV(2.0)
Loss', 929.46265)], overall loss: -865.5572509765625
Iteration: 309, named\_losses: [('ActivationMax Loss', -1799.604), ('TV(2.0)
Loss', 929.60156)], overall loss: -870.00244140625
Iteration: 310, named\_losses: [('ActivationMax Loss', -1797.3728), ('TV(2.0)
Loss', 933.2525)], overall loss: -864.1203002929688
Iteration: 311, named\_losses: [('ActivationMax Loss', -1796.479), ('TV(2.0)
Loss', 927.2592)], overall loss: -869.2197875976562
Iteration: 312, named\_losses: [('ActivationMax Loss', -1793.6731), ('TV(2.0)
Loss', 931.2776)], overall loss: -862.3955078125
Iteration: 313, named\_losses: [('ActivationMax Loss', -1799.737), ('TV(2.0)
Loss', 929.14435)], overall loss: -870.5927124023438
Iteration: 314, named\_losses: [('ActivationMax Loss', -1791.643), ('TV(2.0)
Loss', 930.5565)], overall loss: -861.08642578125
Iteration: 315, named\_losses: [('ActivationMax Loss', -1804.4905), ('TV(2.0)
Loss', 936.98346)], overall loss: -867.5070190429688
Iteration: 316, named\_losses: [('ActivationMax Loss', -1793.0627), ('TV(2.0)
Loss', 930.5558)], overall loss: -862.5069580078125
Iteration: 317, named\_losses: [('ActivationMax Loss', -1801.2981), ('TV(2.0)
Loss', 934.40704)], overall loss: -866.8910522460938
Iteration: 318, named\_losses: [('ActivationMax Loss', -1791.9679), ('TV(2.0)
Loss', 925.9325)], overall loss: -866.035400390625
Iteration: 319, named\_losses: [('ActivationMax Loss', -1800.8612), ('TV(2.0)
Loss', 933.15735)], overall loss: -867.703857421875
Iteration: 320, named\_losses: [('ActivationMax Loss', -1790.4634), ('TV(2.0)
Loss', 928.38165)], overall loss: -862.0817260742188
Iteration: 321, named\_losses: [('ActivationMax Loss', -1801.7373), ('TV(2.0)
Loss', 931.4923)], overall loss: -870.2449951171875
Iteration: 322, named\_losses: [('ActivationMax Loss', -1801.2039), ('TV(2.0)
Loss', 941.33594)], overall loss: -859.867919921875
Iteration: 323, named\_losses: [('ActivationMax Loss', -1809.3276), ('TV(2.0)
Loss', 936.6764)], overall loss: -872.6512451171875
Iteration: 324, named\_losses: [('ActivationMax Loss', -1791.5186), ('TV(2.0)
Loss', 931.2458)], overall loss: -860.2727661132812
Iteration: 325, named\_losses: [('ActivationMax Loss', -1803.1914), ('TV(2.0)
Loss', 931.7774)], overall loss: -871.4140014648438
Iteration: 326, named\_losses: [('ActivationMax Loss', -1801.2162), ('TV(2.0)
Loss', 937.716)], overall loss: -863.5001831054688
Iteration: 327, named\_losses: [('ActivationMax Loss', -1800.6647), ('TV(2.0)
Loss', 930.94525)], overall loss: -869.7194213867188
Iteration: 328, named\_losses: [('ActivationMax Loss', -1799.6478), ('TV(2.0)
Loss', 933.2233)], overall loss: -866.4244995117188
Iteration: 329, named\_losses: [('ActivationMax Loss', -1796.9613), ('TV(2.0)
Loss', 927.6497)], overall loss: -869.3115844726562
Iteration: 330, named\_losses: [('ActivationMax Loss', -1804.8551), ('TV(2.0)
Loss', 938.90753)], overall loss: -865.9475708007812
Iteration: 331, named\_losses: [('ActivationMax Loss', -1800.3241), ('TV(2.0)
Loss', 929.64014)], overall loss: -870.6839599609375
Iteration: 332, named\_losses: [('ActivationMax Loss', -1801.5975), ('TV(2.0)
Loss', 936.13354)], overall loss: -865.4639892578125
Iteration: 333, named\_losses: [('ActivationMax Loss', -1788.721), ('TV(2.0)
Loss', 926.0963)], overall loss: -862.6246337890625
Iteration: 334, named\_losses: [('ActivationMax Loss', -1805.5559), ('TV(2.0)
Loss', 935.4983)], overall loss: -870.0576171875
Iteration: 335, named\_losses: [('ActivationMax Loss', -1795.672), ('TV(2.0)
Loss', 928.55396)], overall loss: -867.1180419921875
Iteration: 336, named\_losses: [('ActivationMax Loss', -1804.4045), ('TV(2.0)
Loss', 936.8602)], overall loss: -867.5443115234375
Iteration: 337, named\_losses: [('ActivationMax Loss', -1800.5897), ('TV(2.0)
Loss', 931.9705)], overall loss: -868.6192016601562
Iteration: 338, named\_losses: [('ActivationMax Loss', -1805.7778), ('TV(2.0)
Loss', 935.68933)], overall loss: -870.0885009765625
Iteration: 339, named\_losses: [('ActivationMax Loss', -1792.4393), ('TV(2.0)
Loss', 928.7071)], overall loss: -863.7322387695312
Iteration: 340, named\_losses: [('ActivationMax Loss', -1802.8187), ('TV(2.0)
Loss', 939.4373)], overall loss: -863.3814086914062
Iteration: 341, named\_losses: [('ActivationMax Loss', -1795.9485), ('TV(2.0)
Loss', 933.6477)], overall loss: -862.30078125
Iteration: 342, named\_losses: [('ActivationMax Loss', -1795.9812), ('TV(2.0)
Loss', 929.906)], overall loss: -866.0751953125
Iteration: 343, named\_losses: [('ActivationMax Loss', -1798.7208), ('TV(2.0)
Loss', 933.8053)], overall loss: -864.91552734375
Iteration: 344, named\_losses: [('ActivationMax Loss', -1798.0955), ('TV(2.0)
Loss', 931.1874)], overall loss: -866.9080810546875
Iteration: 345, named\_losses: [('ActivationMax Loss', -1800.863), ('TV(2.0)
Loss', 931.2485)], overall loss: -869.6145629882812
Iteration: 346, named\_losses: [('ActivationMax Loss', -1799.0834), ('TV(2.0)
Loss', 932.1527)], overall loss: -866.9306640625
Iteration: 347, named\_losses: [('ActivationMax Loss', -1801.9083), ('TV(2.0)
Loss', 931.87036)], overall loss: -870.0379638671875
Iteration: 348, named\_losses: [('ActivationMax Loss', -1799.3591), ('TV(2.0)
Loss', 931.1245)], overall loss: -868.234619140625
Iteration: 349, named\_losses: [('ActivationMax Loss', -1799.9506), ('TV(2.0)
Loss', 930.99945)], overall loss: -868.9511108398438
Iteration: 350, named\_losses: [('ActivationMax Loss', -1798.1584), ('TV(2.0)
Loss', 931.94354)], overall loss: -866.2149047851562
Iteration: 351, named\_losses: [('ActivationMax Loss', -1800.5326), ('TV(2.0)
Loss', 929.32666)], overall loss: -871.2059326171875
Iteration: 352, named\_losses: [('ActivationMax Loss', -1797.0077), ('TV(2.0)
Loss', 929.10583)], overall loss: -867.90185546875
Iteration: 353, named\_losses: [('ActivationMax Loss', -1795.7197), ('TV(2.0)
Loss', 923.55054)], overall loss: -872.169189453125
Iteration: 354, named\_losses: [('ActivationMax Loss', -1795.8082), ('TV(2.0)
Loss', 927.4349)], overall loss: -868.3733520507812
Iteration: 355, named\_losses: [('ActivationMax Loss', -1793.5208), ('TV(2.0)
Loss', 927.5168)], overall loss: -866.0039672851562
Iteration: 356, named\_losses: [('ActivationMax Loss', -1803.4254), ('TV(2.0)
Loss', 935.3014)], overall loss: -868.1240234375
Iteration: 357, named\_losses: [('ActivationMax Loss', -1793.2305), ('TV(2.0)
Loss', 923.1325)], overall loss: -870.0979614257812
Iteration: 358, named\_losses: [('ActivationMax Loss', -1799.1079), ('TV(2.0)
Loss', 932.66437)], overall loss: -866.4435424804688
Iteration: 359, named\_losses: [('ActivationMax Loss', -1796.6622), ('TV(2.0)
Loss', 925.42834)], overall loss: -871.23388671875
Iteration: 360, named\_losses: [('ActivationMax Loss', -1797.8823), ('TV(2.0)
Loss', 930.77625)], overall loss: -867.1060791015625
Iteration: 361, named\_losses: [('ActivationMax Loss', -1797.0457), ('TV(2.0)
Loss', 925.63715)], overall loss: -871.4085083007812
Iteration: 362, named\_losses: [('ActivationMax Loss', -1796.3615), ('TV(2.0)
Loss', 926.73694)], overall loss: -869.62451171875
Iteration: 363, named\_losses: [('ActivationMax Loss', -1794.3326), ('TV(2.0)
Loss', 925.69495)], overall loss: -868.6376953125
Iteration: 364, named\_losses: [('ActivationMax Loss', -1802.727), ('TV(2.0)
Loss', 931.39185)], overall loss: -871.335205078125
Iteration: 365, named\_losses: [('ActivationMax Loss', -1801.1508), ('TV(2.0)
Loss', 931.7834)], overall loss: -869.3673706054688
Iteration: 366, named\_losses: [('ActivationMax Loss', -1801.334), ('TV(2.0)
Loss', 933.6543)], overall loss: -867.6796875
Iteration: 367, named\_losses: [('ActivationMax Loss', -1795.2799), ('TV(2.0)
Loss', 923.19403)], overall loss: -872.0858764648438
Iteration: 368, named\_losses: [('ActivationMax Loss', -1800.4615), ('TV(2.0)
Loss', 933.06415)], overall loss: -867.3973999023438
Iteration: 369, named\_losses: [('ActivationMax Loss', -1796.1528), ('TV(2.0)
Loss', 927.1805)], overall loss: -868.9723510742188
Iteration: 370, named\_losses: [('ActivationMax Loss', -1801.4307), ('TV(2.0)
Loss', 934.2908)], overall loss: -867.139892578125
Iteration: 371, named\_losses: [('ActivationMax Loss', -1802.2866), ('TV(2.0)
Loss', 931.08124)], overall loss: -871.2053833007812
Iteration: 372, named\_losses: [('ActivationMax Loss', -1805.0876), ('TV(2.0)
Loss', 941.2025)], overall loss: -863.8851318359375
Iteration: 373, named\_losses: [('ActivationMax Loss', -1804.9161), ('TV(2.0)
Loss', 932.3165)], overall loss: -872.599609375
Iteration: 374, named\_losses: [('ActivationMax Loss', -1809.2555), ('TV(2.0)
Loss', 942.2704)], overall loss: -866.985107421875
Iteration: 375, named\_losses: [('ActivationMax Loss', -1803.4485), ('TV(2.0)
Loss', 928.72784)], overall loss: -874.7206420898438
Iteration: 376, named\_losses: [('ActivationMax Loss', -1805.187), ('TV(2.0)
Loss', 943.1427)], overall loss: -862.0443115234375
Iteration: 377, named\_losses: [('ActivationMax Loss', -1804.0891), ('TV(2.0)
Loss', 930.3311)], overall loss: -873.7579956054688
Iteration: 378, named\_losses: [('ActivationMax Loss', -1806.6685), ('TV(2.0)
Loss', 940.16327)], overall loss: -866.5051879882812
Iteration: 379, named\_losses: [('ActivationMax Loss', -1802.8718), ('TV(2.0)
Loss', 931.23254)], overall loss: -871.6392822265625
Iteration: 380, named\_losses: [('ActivationMax Loss', -1810.5051), ('TV(2.0)
Loss', 937.48096)], overall loss: -873.024169921875
Iteration: 381, named\_losses: [('ActivationMax Loss', -1803.0271), ('TV(2.0)
Loss', 932.1168)], overall loss: -870.9102783203125
Iteration: 382, named\_losses: [('ActivationMax Loss', -1808.7704), ('TV(2.0)
Loss', 934.2046)], overall loss: -874.5657958984375
Iteration: 383, named\_losses: [('ActivationMax Loss', -1805.8357), ('TV(2.0)
Loss', 937.1999)], overall loss: -868.6358032226562
Iteration: 384, named\_losses: [('ActivationMax Loss', -1808.5859), ('TV(2.0)
Loss', 935.1787)], overall loss: -873.4072265625
Iteration: 385, named\_losses: [('ActivationMax Loss', -1803.9377), ('TV(2.0)
Loss', 936.81885)], overall loss: -867.118896484375
Iteration: 386, named\_losses: [('ActivationMax Loss', -1810.3744), ('TV(2.0)
Loss', 936.79785)], overall loss: -873.5765380859375
Iteration: 387, named\_losses: [('ActivationMax Loss', -1808.202), ('TV(2.0)
Loss', 941.11884)], overall loss: -867.0831909179688
Iteration: 388, named\_losses: [('ActivationMax Loss', -1808.6953), ('TV(2.0)
Loss', 934.8832)], overall loss: -873.8121337890625
Iteration: 389, named\_losses: [('ActivationMax Loss', -1804.7441), ('TV(2.0)
Loss', 936.2628)], overall loss: -868.4813232421875
Iteration: 390, named\_losses: [('ActivationMax Loss', -1807.8602), ('TV(2.0)
Loss', 935.7522)], overall loss: -872.1080322265625
Iteration: 391, named\_losses: [('ActivationMax Loss', -1802.0247), ('TV(2.0)
Loss', 932.9841)], overall loss: -869.0405883789062
Iteration: 392, named\_losses: [('ActivationMax Loss', -1806.9207), ('TV(2.0)
Loss', 934.5434)], overall loss: -872.3772583007812
Iteration: 393, named\_losses: [('ActivationMax Loss', -1799.0997), ('TV(2.0)
Loss', 930.3355)], overall loss: -868.7642211914062
Iteration: 394, named\_losses: [('ActivationMax Loss', -1808.1106), ('TV(2.0)
Loss', 938.8053)], overall loss: -869.3052978515625
Iteration: 395, named\_losses: [('ActivationMax Loss', -1803.9813), ('TV(2.0)
Loss', 935.14526)], overall loss: -868.8360595703125
Iteration: 396, named\_losses: [('ActivationMax Loss', -1811.7817), ('TV(2.0)
Loss', 938.01807)], overall loss: -873.763671875
Iteration: 397, named\_losses: [('ActivationMax Loss', -1807.3269), ('TV(2.0)
Loss', 938.22253)], overall loss: -869.1043701171875
Iteration: 398, named\_losses: [('ActivationMax Loss', -1805.4343), ('TV(2.0)
Loss', 932.75476)], overall loss: -872.6795654296875
Iteration: 399, named\_losses: [('ActivationMax Loss', -1805.9967), ('TV(2.0)
Loss', 936.635)], overall loss: -869.3616943359375
Iteration: 400, named\_losses: [('ActivationMax Loss', -1807.9926), ('TV(2.0)
Loss', 939.3443)], overall loss: -868.6482543945312
Iteration: 401, named\_losses: [('ActivationMax Loss', -1810.0414), ('TV(2.0)
Loss', 938.0617)], overall loss: -871.9796752929688
Iteration: 402, named\_losses: [('ActivationMax Loss', -1803.3177), ('TV(2.0)
Loss', 935.8935)], overall loss: -867.4242553710938
Iteration: 403, named\_losses: [('ActivationMax Loss', -1806.7493), ('TV(2.0)
Loss', 935.70557)], overall loss: -871.043701171875
Iteration: 404, named\_losses: [('ActivationMax Loss', -1808.386), ('TV(2.0)
Loss', 939.07263)], overall loss: -869.3133544921875
Iteration: 405, named\_losses: [('ActivationMax Loss', -1809.7894), ('TV(2.0)
Loss', 936.26434)], overall loss: -873.5250854492188
Iteration: 406, named\_losses: [('ActivationMax Loss', -1804.7571), ('TV(2.0)
Loss', 932.5235)], overall loss: -872.2335815429688
Iteration: 407, named\_losses: [('ActivationMax Loss', -1803.0076), ('TV(2.0)
Loss', 931.9184)], overall loss: -871.0891723632812
Iteration: 408, named\_losses: [('ActivationMax Loss', -1805.1931), ('TV(2.0)
Loss', 932.7404)], overall loss: -872.4526977539062
Iteration: 409, named\_losses: [('ActivationMax Loss', -1804.259), ('TV(2.0)
Loss', 937.6568)], overall loss: -866.6022338867188
Iteration: 410, named\_losses: [('ActivationMax Loss', -1808.3878), ('TV(2.0)
Loss', 934.0543)], overall loss: -874.33349609375
Iteration: 411, named\_losses: [('ActivationMax Loss', -1805.7865), ('TV(2.0)
Loss', 937.52924)], overall loss: -868.2572631835938
Iteration: 412, named\_losses: [('ActivationMax Loss', -1807.1637), ('TV(2.0)
Loss', 936.4348)], overall loss: -870.7288818359375
Iteration: 413, named\_losses: [('ActivationMax Loss', -1805.3661), ('TV(2.0)
Loss', 937.9915)], overall loss: -867.3745727539062
Iteration: 414, named\_losses: [('ActivationMax Loss', -1808.686), ('TV(2.0)
Loss', 936.9269)], overall loss: -871.7591552734375
Iteration: 415, named\_losses: [('ActivationMax Loss', -1809.1176), ('TV(2.0)
Loss', 935.51184)], overall loss: -873.605712890625
Iteration: 416, named\_losses: [('ActivationMax Loss', -1806.126), ('TV(2.0)
Loss', 937.2596)], overall loss: -868.8663940429688
Iteration: 417, named\_losses: [('ActivationMax Loss', -1809.7456), ('TV(2.0)
Loss', 937.2737)], overall loss: -872.471923828125
Iteration: 418, named\_losses: [('ActivationMax Loss', -1812.0223), ('TV(2.0)
Loss', 947.14105)], overall loss: -864.8812866210938
Iteration: 419, named\_losses: [('ActivationMax Loss', -1809.4032), ('TV(2.0)
Loss', 934.6165)], overall loss: -874.7866821289062
Iteration: 420, named\_losses: [('ActivationMax Loss', -1812.2543), ('TV(2.0)
Loss', 943.30237)], overall loss: -868.951904296875
Iteration: 421, named\_losses: [('ActivationMax Loss', -1805.1282), ('TV(2.0)
Loss', 932.7042)], overall loss: -872.4239501953125
Iteration: 422, named\_losses: [('ActivationMax Loss', -1798.9972), ('TV(2.0)
Loss', 933.18805)], overall loss: -865.8091430664062
Iteration: 423, named\_losses: [('ActivationMax Loss', -1807.656), ('TV(2.0)
Loss', 935.0385)], overall loss: -872.6174926757812
Iteration: 424, named\_losses: [('ActivationMax Loss', -1808.7063), ('TV(2.0)
Loss', 937.39514)], overall loss: -871.3111572265625
Iteration: 425, named\_losses: [('ActivationMax Loss', -1801.9789), ('TV(2.0)
Loss', 932.2024)], overall loss: -869.7764892578125
Iteration: 426, named\_losses: [('ActivationMax Loss', -1804.8452), ('TV(2.0)
Loss', 936.8886)], overall loss: -867.9566040039062
Iteration: 427, named\_losses: [('ActivationMax Loss', -1802.7388), ('TV(2.0)
Loss', 934.74335)], overall loss: -867.9954223632812
Iteration: 428, named\_losses: [('ActivationMax Loss', -1803.1542), ('TV(2.0)
Loss', 934.892)], overall loss: -868.2621459960938
Iteration: 429, named\_losses: [('ActivationMax Loss', -1801.7866), ('TV(2.0)
Loss', 930.4161)], overall loss: -871.3705444335938
Iteration: 430, named\_losses: [('ActivationMax Loss', -1805.9214), ('TV(2.0)
Loss', 935.57404)], overall loss: -870.3473510742188
Iteration: 431, named\_losses: [('ActivationMax Loss', -1806.277), ('TV(2.0)
Loss', 937.80457)], overall loss: -868.472412109375
Iteration: 432, named\_losses: [('ActivationMax Loss', -1810.5602), ('TV(2.0)
Loss', 935.66534)], overall loss: -874.8948364257812
Iteration: 433, named\_losses: [('ActivationMax Loss', -1808.6252), ('TV(2.0)
Loss', 940.0996)], overall loss: -868.525634765625
Iteration: 434, named\_losses: [('ActivationMax Loss', -1806.535), ('TV(2.0)
Loss', 934.69446)], overall loss: -871.840576171875
Iteration: 435, named\_losses: [('ActivationMax Loss', -1812.3341), ('TV(2.0)
Loss', 938.454)], overall loss: -873.880126953125
Iteration: 436, named\_losses: [('ActivationMax Loss', -1812.9259), ('TV(2.0)
Loss', 940.92035)], overall loss: -872.0055541992188
Iteration: 437, named\_losses: [('ActivationMax Loss', -1804.9851), ('TV(2.0)
Loss', 934.6842)], overall loss: -870.3009033203125
Iteration: 438, named\_losses: [('ActivationMax Loss', -1807.6996), ('TV(2.0)
Loss', 936.16907)], overall loss: -871.530517578125
Iteration: 439, named\_losses: [('ActivationMax Loss', -1808.3367), ('TV(2.0)
Loss', 936.4843)], overall loss: -871.8523559570312
Iteration: 440, named\_losses: [('ActivationMax Loss', -1801.7863), ('TV(2.0)
Loss', 932.9797)], overall loss: -868.8065795898438
Iteration: 441, named\_losses: [('ActivationMax Loss', -1807.8765), ('TV(2.0)
Loss', 935.89233)], overall loss: -871.984130859375
Iteration: 442, named\_losses: [('ActivationMax Loss', -1806.3448), ('TV(2.0)
Loss', 939.69006)], overall loss: -866.65478515625
Iteration: 443, named\_losses: [('ActivationMax Loss', -1804.9174), ('TV(2.0)
Loss', 933.792)], overall loss: -871.1253662109375
Iteration: 444, named\_losses: [('ActivationMax Loss', -1808.1929), ('TV(2.0)
Loss', 936.30756)], overall loss: -871.8853149414062
Iteration: 445, named\_losses: [('ActivationMax Loss', -1801.247), ('TV(2.0)
Loss', 933.1963)], overall loss: -868.0506591796875
Iteration: 446, named\_losses: [('ActivationMax Loss', -1808.0682), ('TV(2.0)
Loss', 934.28107)], overall loss: -873.7871704101562
Iteration: 447, named\_losses: [('ActivationMax Loss', -1798.4113), ('TV(2.0)
Loss', 928.6995)], overall loss: -869.7117309570312
Iteration: 448, named\_losses: [('ActivationMax Loss', -1809.1909), ('TV(2.0)
Loss', 938.2722)], overall loss: -870.918701171875
Iteration: 449, named\_losses: [('ActivationMax Loss', -1802.2234), ('TV(2.0)
Loss', 936.24396)], overall loss: -865.9794311523438
Iteration: 450, named\_losses: [('ActivationMax Loss', -1813.1616), ('TV(2.0)
Loss', 938.53503)], overall loss: -874.6265869140625
Iteration: 451, named\_losses: [('ActivationMax Loss', -1809.3457), ('TV(2.0)
Loss', 940.7517)], overall loss: -868.593994140625
Iteration: 452, named\_losses: [('ActivationMax Loss', -1811.0464), ('TV(2.0)
Loss', 938.5755)], overall loss: -872.4708862304688
Iteration: 453, named\_losses: [('ActivationMax Loss', -1811.5308), ('TV(2.0)
Loss', 942.19904)], overall loss: -869.3317260742188
Iteration: 454, named\_losses: [('ActivationMax Loss', -1813.1875), ('TV(2.0)
Loss', 939.2249)], overall loss: -873.9625854492188
Iteration: 455, named\_losses: [('ActivationMax Loss', -1810.9246), ('TV(2.0)
Loss', 940.57135)], overall loss: -870.3532104492188
Iteration: 456, named\_losses: [('ActivationMax Loss', -1808.5436), ('TV(2.0)
Loss', 936.9658)], overall loss: -871.5777587890625
Iteration: 457, named\_losses: [('ActivationMax Loss', -1808.3634), ('TV(2.0)
Loss', 940.5863)], overall loss: -867.777099609375
Iteration: 458, named\_losses: [('ActivationMax Loss', -1812.1848), ('TV(2.0)
Loss', 935.752)], overall loss: -876.4328002929688
Iteration: 459, named\_losses: [('ActivationMax Loss', -1813.5363), ('TV(2.0)
Loss', 942.10095)], overall loss: -871.435302734375
Iteration: 460, named\_losses: [('ActivationMax Loss', -1807.0736), ('TV(2.0)
Loss', 931.529)], overall loss: -875.5446166992188
Iteration: 461, named\_losses: [('ActivationMax Loss', -1809.2206), ('TV(2.0)
Loss', 939.2027)], overall loss: -870.0178833007812
Iteration: 462, named\_losses: [('ActivationMax Loss', -1803.7347), ('TV(2.0)
Loss', 932.87946)], overall loss: -870.8552856445312
Iteration: 463, named\_losses: [('ActivationMax Loss', -1817.1497), ('TV(2.0)
Loss', 946.08496)], overall loss: -871.064697265625
Iteration: 464, named\_losses: [('ActivationMax Loss', -1808.0222), ('TV(2.0)
Loss', 933.68427)], overall loss: -874.3379516601562
Iteration: 465, named\_losses: [('ActivationMax Loss', -1818.0908), ('TV(2.0)
Loss', 946.3389)], overall loss: -871.7518920898438
Iteration: 466, named\_losses: [('ActivationMax Loss', -1810.2085), ('TV(2.0)
Loss', 937.47723)], overall loss: -872.7312622070312
Iteration: 467, named\_losses: [('ActivationMax Loss', -1819.1473), ('TV(2.0)
Loss', 951.214)], overall loss: -867.933349609375
Iteration: 468, named\_losses: [('ActivationMax Loss', -1804.3993), ('TV(2.0)
Loss', 933.8508)], overall loss: -870.5484619140625
Iteration: 469, named\_losses: [('ActivationMax Loss', -1821.0729), ('TV(2.0)
Loss', 945.25885)], overall loss: -875.8140258789062
Iteration: 470, named\_losses: [('ActivationMax Loss', -1811.0232), ('TV(2.0)
Loss', 939.7175)], overall loss: -871.3056640625
Iteration: 471, named\_losses: [('ActivationMax Loss', -1813.485), ('TV(2.0)
Loss', 942.8058)], overall loss: -870.67919921875
Iteration: 472, named\_losses: [('ActivationMax Loss', -1805.4827), ('TV(2.0)
Loss', 933.8172)], overall loss: -871.6654663085938
Iteration: 473, named\_losses: [('ActivationMax Loss', -1812.9922), ('TV(2.0)
Loss', 941.55896)], overall loss: -871.4332275390625
Iteration: 474, named\_losses: [('ActivationMax Loss', -1798.6475), ('TV(2.0)
Loss', 927.0192)], overall loss: -871.6282348632812
Iteration: 475, named\_losses: [('ActivationMax Loss', -1814.432), ('TV(2.0)
Loss', 940.7589)], overall loss: -873.673095703125
Iteration: 476, named\_losses: [('ActivationMax Loss', -1811.0918), ('TV(2.0)
Loss', 934.6381)], overall loss: -876.4536743164062
Iteration: 477, named\_losses: [('ActivationMax Loss', -1816.5306), ('TV(2.0)
Loss', 938.42114)], overall loss: -878.1094970703125
Iteration: 478, named\_losses: [('ActivationMax Loss', -1805.0836), ('TV(2.0)
Loss', 931.82117)], overall loss: -873.262451171875
Iteration: 479, named\_losses: [('ActivationMax Loss', -1812.2146), ('TV(2.0)
Loss', 935.62744)], overall loss: -876.587158203125
Iteration: 480, named\_losses: [('ActivationMax Loss', -1805.4902), ('TV(2.0)
Loss', 931.279)], overall loss: -874.2112426757812
Iteration: 481, named\_losses: [('ActivationMax Loss', -1808.5924), ('TV(2.0)
Loss', 930.928)], overall loss: -877.6644287109375
Iteration: 482, named\_losses: [('ActivationMax Loss', -1808.3374), ('TV(2.0)
Loss', 930.188)], overall loss: -878.1494140625
Iteration: 483, named\_losses: [('ActivationMax Loss', -1811.6737), ('TV(2.0)
Loss', 930.45984)], overall loss: -881.2138671875
Iteration: 484, named\_losses: [('ActivationMax Loss', -1811.9648), ('TV(2.0)
Loss', 933.9093)], overall loss: -878.0555419921875
Iteration: 485, named\_losses: [('ActivationMax Loss', -1810.9114), ('TV(2.0)
Loss', 930.532)], overall loss: -880.37939453125
Iteration: 486, named\_losses: [('ActivationMax Loss', -1817.1202), ('TV(2.0)
Loss', 936.6607)], overall loss: -880.4595336914062
Iteration: 487, named\_losses: [('ActivationMax Loss', -1809.4845), ('TV(2.0)
Loss', 928.4513)], overall loss: -881.033203125
Iteration: 488, named\_losses: [('ActivationMax Loss', -1812.31), ('TV(2.0)
Loss', 934.4042)], overall loss: -877.9058837890625
Iteration: 489, named\_losses: [('ActivationMax Loss', -1814.3629), ('TV(2.0)
Loss', 933.1744)], overall loss: -881.1885375976562
Iteration: 490, named\_losses: [('ActivationMax Loss', -1813.7559), ('TV(2.0)
Loss', 932.0864)], overall loss: -881.66943359375
Iteration: 491, named\_losses: [('ActivationMax Loss', -1817.8763), ('TV(2.0)
Loss', 936.3876)], overall loss: -881.48876953125
Iteration: 492, named\_losses: [('ActivationMax Loss', -1811.7367), ('TV(2.0)
Loss', 934.0597)], overall loss: -877.677001953125
Iteration: 493, named\_losses: [('ActivationMax Loss', -1818.7863), ('TV(2.0)
Loss', 940.19507)], overall loss: -878.5911865234375
Iteration: 494, named\_losses: [('ActivationMax Loss', -1815.4614), ('TV(2.0)
Loss', 933.9882)], overall loss: -881.4732055664062
Iteration: 495, named\_losses: [('ActivationMax Loss', -1822.682), ('TV(2.0)
Loss', 940.5275)], overall loss: -882.1544799804688
Iteration: 496, named\_losses: [('ActivationMax Loss', -1809.4618), ('TV(2.0)
Loss', 929.6414)], overall loss: -879.8203735351562
Iteration: 497, named\_losses: [('ActivationMax Loss', -1818.4937), ('TV(2.0)
Loss', 936.62775)], overall loss: -881.8659057617188
Iteration: 498, named\_losses: [('ActivationMax Loss', -1813.8844), ('TV(2.0)
Loss', 932.30457)], overall loss: -881.579833984375
Iteration: 499, named\_losses: [('ActivationMax Loss', -1823.2954), ('TV(2.0)
Loss', 940.24023)], overall loss: -883.05517578125
Iteration: 500, named\_losses: [('ActivationMax Loss', -1810.5464), ('TV(2.0)
Loss', 935.71326)], overall loss: -874.8331298828125
Iteration: 501, named\_losses: [('ActivationMax Loss', -1820.8524), ('TV(2.0)
Loss', 937.12946)], overall loss: -883.7229614257812
Iteration: 502, named\_losses: [('ActivationMax Loss', -1811.3656), ('TV(2.0)
Loss', 937.76514)], overall loss: -873.6004638671875
Iteration: 503, named\_losses: [('ActivationMax Loss', -1825.0342), ('TV(2.0)
Loss', 939.06494)], overall loss: -885.96923828125
Iteration: 504, named\_losses: [('ActivationMax Loss', -1813.4646), ('TV(2.0)
Loss', 936.4093)], overall loss: -877.0552978515625
Iteration: 505, named\_losses: [('ActivationMax Loss', -1827.8964), ('TV(2.0)
Loss', 939.5581)], overall loss: -888.3382568359375
Iteration: 506, named\_losses: [('ActivationMax Loss', -1827.0192), ('TV(2.0)
Loss', 946.6026)], overall loss: -880.4165649414062
Iteration: 507, named\_losses: [('ActivationMax Loss', -1829.2712), ('TV(2.0)
Loss', 939.7468)], overall loss: -889.5244140625
Iteration: 508, named\_losses: [('ActivationMax Loss', -1830.0276), ('TV(2.0)
Loss', 945.05475)], overall loss: -884.9728393554688
Iteration: 509, named\_losses: [('ActivationMax Loss', -1830.2358), ('TV(2.0)
Loss', 941.9962)], overall loss: -888.2396240234375
Iteration: 510, named\_losses: [('ActivationMax Loss', -1830.8842), ('TV(2.0)
Loss', 945.5596)], overall loss: -885.3245849609375
Iteration: 511, named\_losses: [('ActivationMax Loss', -1834.9562), ('TV(2.0)
Loss', 953.01526)], overall loss: -881.94091796875
Iteration: 512, named\_losses: [('ActivationMax Loss', -1829.2288), ('TV(2.0)
Loss', 944.9514)], overall loss: -884.27734375
Iteration: 513, named\_losses: [('ActivationMax Loss', -1835.8772), ('TV(2.0)
Loss', 951.2014)], overall loss: -884.67578125
Iteration: 514, named\_losses: [('ActivationMax Loss', -1833.7534), ('TV(2.0)
Loss', 948.938)], overall loss: -884.8154296875
Iteration: 515, named\_losses: [('ActivationMax Loss', -1830.2198), ('TV(2.0)
Loss', 945.73535)], overall loss: -884.4844970703125
Iteration: 516, named\_losses: [('ActivationMax Loss', -1837.0234), ('TV(2.0)
Loss', 953.9671)], overall loss: -883.0563354492188
Iteration: 517, named\_losses: [('ActivationMax Loss', -1838.6505), ('TV(2.0)
Loss', 949.3346)], overall loss: -889.31591796875
Iteration: 518, named\_losses: [('ActivationMax Loss', -1835.3303), ('TV(2.0)
Loss', 951.50806)], overall loss: -883.822265625
Iteration: 519, named\_losses: [('ActivationMax Loss', -1837.8353), ('TV(2.0)
Loss', 947.20233)], overall loss: -890.6329956054688
Iteration: 520, named\_losses: [('ActivationMax Loss', -1836.5497), ('TV(2.0)
Loss', 948.9315)], overall loss: -887.6181640625
Iteration: 521, named\_losses: [('ActivationMax Loss', -1839.3367), ('TV(2.0)
Loss', 947.90576)], overall loss: -891.430908203125
Iteration: 522, named\_losses: [('ActivationMax Loss', -1832.3738), ('TV(2.0)
Loss', 946.64624)], overall loss: -885.7275390625
Iteration: 523, named\_losses: [('ActivationMax Loss', -1832.6943), ('TV(2.0)
Loss', 944.66956)], overall loss: -888.0247802734375
Iteration: 524, named\_losses: [('ActivationMax Loss', -1834.343), ('TV(2.0)
Loss', 944.81213)], overall loss: -889.5308837890625
Iteration: 525, named\_losses: [('ActivationMax Loss', -1835.3217), ('TV(2.0)
Loss', 944.2198)], overall loss: -891.1018676757812
Iteration: 526, named\_losses: [('ActivationMax Loss', -1833.8524), ('TV(2.0)
Loss', 944.6137)], overall loss: -889.2387084960938
Iteration: 527, named\_losses: [('ActivationMax Loss', -1835.472), ('TV(2.0)
Loss', 941.17194)], overall loss: -894.3001098632812
Iteration: 528, named\_losses: [('ActivationMax Loss', -1834.668), ('TV(2.0)
Loss', 942.0581)], overall loss: -892.60986328125
Iteration: 529, named\_losses: [('ActivationMax Loss', -1836.2075), ('TV(2.0)
Loss', 943.3637)], overall loss: -892.8438110351562
Iteration: 530, named\_losses: [('ActivationMax Loss', -1835.1055), ('TV(2.0)
Loss', 940.753)], overall loss: -894.3524780273438
Iteration: 531, named\_losses: [('ActivationMax Loss', -1840.6476), ('TV(2.0)
Loss', 943.6565)], overall loss: -896.9910888671875
Iteration: 532, named\_losses: [('ActivationMax Loss', -1842.6791), ('TV(2.0)
Loss', 945.0919)], overall loss: -897.587158203125
Iteration: 533, named\_losses: [('ActivationMax Loss', -1840.6951), ('TV(2.0)
Loss', 945.88055)], overall loss: -894.8145141601562
Iteration: 534, named\_losses: [('ActivationMax Loss', -1842.1675), ('TV(2.0)
Loss', 945.31506)], overall loss: -896.8524169921875
Iteration: 535, named\_losses: [('ActivationMax Loss', -1846.4233), ('TV(2.0)
Loss', 955.4937)], overall loss: -890.9296264648438
Iteration: 536, named\_losses: [('ActivationMax Loss', -1848.5565), ('TV(2.0)
Loss', 953.2196)], overall loss: -895.3369140625
Iteration: 537, named\_losses: [('ActivationMax Loss', -1843.6997), ('TV(2.0)
Loss', 950.0785)], overall loss: -893.6212158203125
Iteration: 538, named\_losses: [('ActivationMax Loss', -1848.3386), ('TV(2.0)
Loss', 956.1844)], overall loss: -892.1542358398438
Iteration: 539, named\_losses: [('ActivationMax Loss', -1846.1628), ('TV(2.0)
Loss', 955.07184)], overall loss: -891.0910034179688
Iteration: 540, named\_losses: [('ActivationMax Loss', -1852.1143), ('TV(2.0)
Loss', 957.03)], overall loss: -895.084228515625
Iteration: 541, named\_losses: [('ActivationMax Loss', -1842.4729), ('TV(2.0)
Loss', 953.35974)], overall loss: -889.1131591796875
Iteration: 542, named\_losses: [('ActivationMax Loss', -1845.2434), ('TV(2.0)
Loss', 951.2345)], overall loss: -894.0089111328125
Iteration: 543, named\_losses: [('ActivationMax Loss', -1850.7532), ('TV(2.0)
Loss', 956.5722)], overall loss: -894.1809692382812
Iteration: 544, named\_losses: [('ActivationMax Loss', -1844.323), ('TV(2.0)
Loss', 949.5803)], overall loss: -894.74267578125
Iteration: 545, named\_losses: [('ActivationMax Loss', -1848.5338), ('TV(2.0)
Loss', 954.6592)], overall loss: -893.8746337890625
Iteration: 546, named\_losses: [('ActivationMax Loss', -1841.8313), ('TV(2.0)
Loss', 949.75116)], overall loss: -892.0801391601562
Iteration: 547, named\_losses: [('ActivationMax Loss', -1849.8947), ('TV(2.0)
Loss', 958.6721)], overall loss: -891.2225341796875
Iteration: 548, named\_losses: [('ActivationMax Loss', -1846.1606), ('TV(2.0)
Loss', 948.18835)], overall loss: -897.9722900390625
Iteration: 549, named\_losses: [('ActivationMax Loss', -1847.809), ('TV(2.0)
Loss', 954.9104)], overall loss: -892.8985595703125
Iteration: 550, named\_losses: [('ActivationMax Loss', -1848.3772), ('TV(2.0)
Loss', 952.4005)], overall loss: -895.9766845703125
Iteration: 551, named\_losses: [('ActivationMax Loss', -1848.3223), ('TV(2.0)
Loss', 952.874)], overall loss: -895.4482421875
Iteration: 552, named\_losses: [('ActivationMax Loss', -1847.1228), ('TV(2.0)
Loss', 949.4266)], overall loss: -897.6962280273438
Iteration: 553, named\_losses: [('ActivationMax Loss', -1851.9534), ('TV(2.0)
Loss', 957.9422)], overall loss: -894.0111694335938
Iteration: 554, named\_losses: [('ActivationMax Loss', -1842.3008), ('TV(2.0)
Loss', 947.0995)], overall loss: -895.2012939453125
Iteration: 555, named\_losses: [('ActivationMax Loss', -1850.3778), ('TV(2.0)
Loss', 955.92413)], overall loss: -894.4536743164062
Iteration: 556, named\_losses: [('ActivationMax Loss', -1853.9231), ('TV(2.0)
Loss', 956.45105)], overall loss: -897.4720458984375
Iteration: 557, named\_losses: [('ActivationMax Loss', -1850.6383), ('TV(2.0)
Loss', 953.455)], overall loss: -897.1832885742188
Iteration: 558, named\_losses: [('ActivationMax Loss', -1853.7877), ('TV(2.0)
Loss', 957.2156)], overall loss: -896.5721435546875
Iteration: 559, named\_losses: [('ActivationMax Loss', -1850.2632), ('TV(2.0)
Loss', 956.38257)], overall loss: -893.880615234375
Iteration: 560, named\_losses: [('ActivationMax Loss', -1854.7852), ('TV(2.0)
Loss', 955.85236)], overall loss: -898.9328002929688
Iteration: 561, named\_losses: [('ActivationMax Loss', -1853.3418), ('TV(2.0)
Loss', 956.8961)], overall loss: -896.4456787109375
Iteration: 562, named\_losses: [('ActivationMax Loss', -1850.3032), ('TV(2.0)
Loss', 956.48395)], overall loss: -893.8192749023438
Iteration: 563, named\_losses: [('ActivationMax Loss', -1852.6229), ('TV(2.0)
Loss', 956.863)], overall loss: -895.7599487304688
Iteration: 564, named\_losses: [('ActivationMax Loss', -1851.301), ('TV(2.0)
Loss', 954.8553)], overall loss: -896.4457397460938
Iteration: 565, named\_losses: [('ActivationMax Loss', -1852.7993), ('TV(2.0)
Loss', 959.37396)], overall loss: -893.4253540039062
Iteration: 566, named\_losses: [('ActivationMax Loss', -1849.7822), ('TV(2.0)
Loss', 953.4336)], overall loss: -896.3486328125
Iteration: 567, named\_losses: [('ActivationMax Loss', -1848.6552), ('TV(2.0)
Loss', 954.28577)], overall loss: -894.369384765625
Iteration: 568, named\_losses: [('ActivationMax Loss', -1848.0325), ('TV(2.0)
Loss', 954.0093)], overall loss: -894.023193359375
Iteration: 569, named\_losses: [('ActivationMax Loss', -1854.8068), ('TV(2.0)
Loss', 958.94525)], overall loss: -895.8615112304688
Iteration: 570, named\_losses: [('ActivationMax Loss', -1857.3179), ('TV(2.0)
Loss', 958.86597)], overall loss: -898.451904296875
Iteration: 571, named\_losses: [('ActivationMax Loss', -1859.051), ('TV(2.0)
Loss', 961.28326)], overall loss: -897.7677612304688
Iteration: 572, named\_losses: [('ActivationMax Loss', -1856.0765), ('TV(2.0)
Loss', 959.2968)], overall loss: -896.7797241210938
Iteration: 573, named\_losses: [('ActivationMax Loss', -1855.7794), ('TV(2.0)
Loss', 961.1222)], overall loss: -894.6572265625
Iteration: 574, named\_losses: [('ActivationMax Loss', -1861.5201), ('TV(2.0)
Loss', 960.79474)], overall loss: -900.7254028320312
Iteration: 575, named\_losses: [('ActivationMax Loss', -1855.2039), ('TV(2.0)
Loss', 955.9453)], overall loss: -899.258544921875
Iteration: 576, named\_losses: [('ActivationMax Loss', -1856.2299), ('TV(2.0)
Loss', 956.05035)], overall loss: -900.1795043945312
Iteration: 577, named\_losses: [('ActivationMax Loss', -1850.1753), ('TV(2.0)
Loss', 952.89496)], overall loss: -897.2803344726562
Iteration: 578, named\_losses: [('ActivationMax Loss', -1859.2379), ('TV(2.0)
Loss', 957.7042)], overall loss: -901.53369140625
Iteration: 579, named\_losses: [('ActivationMax Loss', -1853.5094), ('TV(2.0)
Loss', 952.1822)], overall loss: -901.3272094726562
Iteration: 580, named\_losses: [('ActivationMax Loss', -1865.1605), ('TV(2.0)
Loss', 962.66925)], overall loss: -902.4912719726562
Iteration: 581, named\_losses: [('ActivationMax Loss', -1859.3901), ('TV(2.0)
Loss', 961.70166)], overall loss: -897.6884765625
Iteration: 582, named\_losses: [('ActivationMax Loss', -1858.581), ('TV(2.0)
Loss', 958.8261)], overall loss: -899.7549438476562
Iteration: 583, named\_losses: [('ActivationMax Loss', -1858.9968), ('TV(2.0)
Loss', 958.27576)], overall loss: -900.7210693359375
Iteration: 584, named\_losses: [('ActivationMax Loss', -1855.574), ('TV(2.0)
Loss', 958.6431)], overall loss: -896.9308471679688
Iteration: 585, named\_losses: [('ActivationMax Loss', -1864.518), ('TV(2.0)
Loss', 963.4004)], overall loss: -901.1175537109375
Iteration: 586, named\_losses: [('ActivationMax Loss', -1859.6772), ('TV(2.0)
Loss', 961.64966)], overall loss: -898.027587890625
Iteration: 587, named\_losses: [('ActivationMax Loss', -1858.3184), ('TV(2.0)
Loss', 956.0902)], overall loss: -902.2281494140625
Iteration: 588, named\_losses: [('ActivationMax Loss', -1859.8696), ('TV(2.0)
Loss', 963.359)], overall loss: -896.5106201171875
Iteration: 589, named\_losses: [('ActivationMax Loss', -1864.8706), ('TV(2.0)
Loss', 961.6552)], overall loss: -903.2153930664062
Iteration: 590, named\_losses: [('ActivationMax Loss', -1860.1472), ('TV(2.0)
Loss', 963.1364)], overall loss: -897.0108032226562
Iteration: 591, named\_losses: [('ActivationMax Loss', -1858.7792), ('TV(2.0)
Loss', 960.4567)], overall loss: -898.3224487304688
Iteration: 592, named\_losses: [('ActivationMax Loss', -1859.9996), ('TV(2.0)
Loss', 959.4144)], overall loss: -900.585205078125
Iteration: 593, named\_losses: [('ActivationMax Loss', -1863.6954), ('TV(2.0)
Loss', 964.63586)], overall loss: -899.0595703125
Iteration: 594, named\_losses: [('ActivationMax Loss', -1863.8469), ('TV(2.0)
Loss', 964.0052)], overall loss: -899.8417358398438
Iteration: 595, named\_losses: [('ActivationMax Loss', -1866.9037), ('TV(2.0)
Loss', 968.80347)], overall loss: -898.1002197265625
Iteration: 596, named\_losses: [('ActivationMax Loss', -1860.749), ('TV(2.0)
Loss', 964.53143)], overall loss: -896.2175903320312
Iteration: 597, named\_losses: [('ActivationMax Loss', -1873.942), ('TV(2.0)
Loss', 970.4368)], overall loss: -903.5051879882812
Iteration: 598, named\_losses: [('ActivationMax Loss', -1852.9806), ('TV(2.0)
Loss', 955.9928)], overall loss: -896.98779296875
Iteration: 599, named\_losses: [('ActivationMax Loss', -1868.5774), ('TV(2.0)
Loss', 968.46375)], overall loss: -900.1136474609375
Iteration: 600, named\_losses: [('ActivationMax Loss', -1857.5769), ('TV(2.0)
Loss', 960.82605)], overall loss: -896.7508544921875
Iteration: 601, named\_losses: [('ActivationMax Loss', -1868.4645), ('TV(2.0)
Loss', 969.7316)], overall loss: -898.7328491210938
Iteration: 602, named\_losses: [('ActivationMax Loss', -1857.5706), ('TV(2.0)
Loss', 957.21545)], overall loss: -900.3551025390625
Iteration: 603, named\_losses: [('ActivationMax Loss', -1866.919), ('TV(2.0)
Loss', 968.9996)], overall loss: -897.9193725585938
Iteration: 604, named\_losses: [('ActivationMax Loss', -1859.363), ('TV(2.0)
Loss', 960.40186)], overall loss: -898.961181640625
Iteration: 605, named\_losses: [('ActivationMax Loss', -1863.3798), ('TV(2.0)
Loss', 966.5435)], overall loss: -896.8362426757812
Iteration: 606, named\_losses: [('ActivationMax Loss', -1861.336), ('TV(2.0)
Loss', 959.7975)], overall loss: -901.53857421875
Iteration: 607, named\_losses: [('ActivationMax Loss', -1865.8335), ('TV(2.0)
Loss', 965.0818)], overall loss: -900.751708984375
Iteration: 608, named\_losses: [('ActivationMax Loss', -1856.4839), ('TV(2.0)
Loss', 958.5414)], overall loss: -897.9425048828125
Iteration: 609, named\_losses: [('ActivationMax Loss', -1872.0166), ('TV(2.0)
Loss', 972.5154)], overall loss: -899.501220703125
Iteration: 610, named\_losses: [('ActivationMax Loss', -1865.0619), ('TV(2.0)
Loss', 963.5015)], overall loss: -901.5603637695312
Iteration: 611, named\_losses: [('ActivationMax Loss', -1868.5303), ('TV(2.0)
Loss', 971.7705)], overall loss: -896.759765625
Iteration: 612, named\_losses: [('ActivationMax Loss', -1867.2783), ('TV(2.0)
Loss', 966.40454)], overall loss: -900.873779296875
Iteration: 613, named\_losses: [('ActivationMax Loss', -1871.5652), ('TV(2.0)
Loss', 973.2272)], overall loss: -898.3380126953125
Iteration: 614, named\_losses: [('ActivationMax Loss', -1868.6906), ('TV(2.0)
Loss', 968.60724)], overall loss: -900.0833129882812
Iteration: 615, named\_losses: [('ActivationMax Loss', -1867.78), ('TV(2.0)
Loss', 965.8131)], overall loss: -901.9669189453125
Iteration: 616, named\_losses: [('ActivationMax Loss', -1864.4545), ('TV(2.0)
Loss', 961.2091)], overall loss: -903.245361328125
Iteration: 617, named\_losses: [('ActivationMax Loss', -1864.594), ('TV(2.0)
Loss', 969.892)], overall loss: -894.7019653320312
Iteration: 618, named\_losses: [('ActivationMax Loss', -1862.8308), ('TV(2.0)
Loss', 962.35974)], overall loss: -900.4710693359375
Iteration: 619, named\_losses: [('ActivationMax Loss', -1865.6957), ('TV(2.0)
Loss', 970.103)], overall loss: -895.5926513671875
Iteration: 620, named\_losses: [('ActivationMax Loss', -1861.1764), ('TV(2.0)
Loss', 960.7571)], overall loss: -900.4193115234375
Iteration: 621, named\_losses: [('ActivationMax Loss', -1869.0344), ('TV(2.0)
Loss', 969.2949)], overall loss: -899.739501953125
Iteration: 622, named\_losses: [('ActivationMax Loss', -1860.9165), ('TV(2.0)
Loss', 959.88007)], overall loss: -901.0364379882812
Iteration: 623, named\_losses: [('ActivationMax Loss', -1861.1005), ('TV(2.0)
Loss', 962.4399)], overall loss: -898.6605834960938
Iteration: 624, named\_losses: [('ActivationMax Loss', -1856.5994), ('TV(2.0)
Loss', 957.27057)], overall loss: -899.3287963867188
Iteration: 625, named\_losses: [('ActivationMax Loss', -1865.7908), ('TV(2.0)
Loss', 966.99384)], overall loss: -898.7969360351562
Iteration: 626, named\_losses: [('ActivationMax Loss', -1861.3942), ('TV(2.0)
Loss', 960.0648)], overall loss: -901.329345703125
Iteration: 627, named\_losses: [('ActivationMax Loss', -1869.7732), ('TV(2.0)
Loss', 965.20685)], overall loss: -904.5663452148438
Iteration: 628, named\_losses: [('ActivationMax Loss', -1861.5892), ('TV(2.0)
Loss', 966.2661)], overall loss: -895.3231201171875
Iteration: 629, named\_losses: [('ActivationMax Loss', -1868.7706), ('TV(2.0)
Loss', 966.5063)], overall loss: -902.2643432617188
Iteration: 630, named\_losses: [('ActivationMax Loss', -1863.7725), ('TV(2.0)
Loss', 966.2228)], overall loss: -897.5496826171875
Iteration: 631, named\_losses: [('ActivationMax Loss', -1867.1), ('TV(2.0) Loss',
965.37286)], overall loss: -901.7271118164062
Iteration: 632, named\_losses: [('ActivationMax Loss', -1863.4036), ('TV(2.0)
Loss', 968.207)], overall loss: -895.1965942382812
Iteration: 633, named\_losses: [('ActivationMax Loss', -1870.0958), ('TV(2.0)
Loss', 965.3998)], overall loss: -904.696044921875
Iteration: 634, named\_losses: [('ActivationMax Loss', -1866.8773), ('TV(2.0)
Loss', 968.0455)], overall loss: -898.8318481445312
Iteration: 635, named\_losses: [('ActivationMax Loss', -1859.0565), ('TV(2.0)
Loss', 960.6892)], overall loss: -898.3673095703125
Iteration: 636, named\_losses: [('ActivationMax Loss', -1862.7501), ('TV(2.0)
Loss', 960.6982)], overall loss: -902.0519409179688
Iteration: 637, named\_losses: [('ActivationMax Loss', -1860.7974), ('TV(2.0)
Loss', 967.6448)], overall loss: -893.152587890625
Iteration: 638, named\_losses: [('ActivationMax Loss', -1860.5768), ('TV(2.0)
Loss', 960.8095)], overall loss: -899.7672729492188
Iteration: 639, named\_losses: [('ActivationMax Loss', -1863.4092), ('TV(2.0)
Loss', 961.62805)], overall loss: -901.7811279296875
Iteration: 640, named\_losses: [('ActivationMax Loss', -1863.017), ('TV(2.0)
Loss', 962.7365)], overall loss: -900.2804565429688
Iteration: 641, named\_losses: [('ActivationMax Loss', -1861.4275), ('TV(2.0)
Loss', 962.1729)], overall loss: -899.2545776367188
Iteration: 642, named\_losses: [('ActivationMax Loss', -1863.852), ('TV(2.0)
Loss', 962.44305)], overall loss: -901.4089965820312
Iteration: 643, named\_losses: [('ActivationMax Loss', -1866.2657), ('TV(2.0)
Loss', 967.6021)], overall loss: -898.6636352539062
Iteration: 644, named\_losses: [('ActivationMax Loss', -1872.331), ('TV(2.0)
Loss', 971.5413)], overall loss: -900.7897338867188
Iteration: 645, named\_losses: [('ActivationMax Loss', -1862.9127), ('TV(2.0)
Loss', 966.82446)], overall loss: -896.0882568359375
Iteration: 646, named\_losses: [('ActivationMax Loss', -1866.7914), ('TV(2.0)
Loss', 964.95044)], overall loss: -901.8409423828125
Iteration: 647, named\_losses: [('ActivationMax Loss', -1869.128), ('TV(2.0)
Loss', 974.34247)], overall loss: -894.7855834960938
Iteration: 648, named\_losses: [('ActivationMax Loss', -1869.0516), ('TV(2.0)
Loss', 965.8717)], overall loss: -903.179931640625
Iteration: 649, named\_losses: [('ActivationMax Loss', -1860.8341), ('TV(2.0)
Loss', 964.48016)], overall loss: -896.3539428710938
Iteration: 650, named\_losses: [('ActivationMax Loss', -1865.5706), ('TV(2.0)
Loss', 963.38245)], overall loss: -902.1881103515625
Iteration: 651, named\_losses: [('ActivationMax Loss', -1862.1578), ('TV(2.0)
Loss', 969.0226)], overall loss: -893.13525390625
Iteration: 652, named\_losses: [('ActivationMax Loss', -1861.8228), ('TV(2.0)
Loss', 961.20233)], overall loss: -900.6204223632812
Iteration: 653, named\_losses: [('ActivationMax Loss', -1868.7137), ('TV(2.0)
Loss', 970.5203)], overall loss: -898.1934204101562
Iteration: 654, named\_losses: [('ActivationMax Loss', -1869.9321), ('TV(2.0)
Loss', 966.6145)], overall loss: -903.317626953125
Iteration: 655, named\_losses: [('ActivationMax Loss', -1872.8246), ('TV(2.0)
Loss', 974.1829)], overall loss: -898.6416625976562
Iteration: 656, named\_losses: [('ActivationMax Loss', -1868.6156), ('TV(2.0)
Loss', 965.26306)], overall loss: -903.3525390625
Iteration: 657, named\_losses: [('ActivationMax Loss', -1863.0874), ('TV(2.0)
Loss', 969.4069)], overall loss: -893.6804809570312
Iteration: 658, named\_losses: [('ActivationMax Loss', -1864.845), ('TV(2.0)
Loss', 962.20013)], overall loss: -902.6448364257812
Iteration: 659, named\_losses: [('ActivationMax Loss', -1869.927), ('TV(2.0)
Loss', 970.03424)], overall loss: -899.8927612304688
Iteration: 660, named\_losses: [('ActivationMax Loss', -1859.7307), ('TV(2.0)
Loss', 960.47815)], overall loss: -899.2525634765625
Iteration: 661, named\_losses: [('ActivationMax Loss', -1864.1046), ('TV(2.0)
Loss', 967.2073)], overall loss: -896.8973388671875
Iteration: 662, named\_losses: [('ActivationMax Loss', -1860.7905), ('TV(2.0)
Loss', 960.6409)], overall loss: -900.1495971679688
Iteration: 663, named\_losses: [('ActivationMax Loss', -1872.3547), ('TV(2.0)
Loss', 973.57227)], overall loss: -898.782470703125
Iteration: 664, named\_losses: [('ActivationMax Loss', -1858.2388), ('TV(2.0)
Loss', 958.3026)], overall loss: -899.9361572265625
Iteration: 665, named\_losses: [('ActivationMax Loss', -1871.6398), ('TV(2.0)
Loss', 976.3972)], overall loss: -895.2425537109375
Iteration: 666, named\_losses: [('ActivationMax Loss', -1865.3514), ('TV(2.0)
Loss', 963.0556)], overall loss: -902.2958374023438
Iteration: 667, named\_losses: [('ActivationMax Loss', -1871.1868), ('TV(2.0)
Loss', 970.88715)], overall loss: -900.2996215820312
Iteration: 668, named\_losses: [('ActivationMax Loss', -1863.8386), ('TV(2.0)
Loss', 963.0556)], overall loss: -900.7830200195312
Iteration: 669, named\_losses: [('ActivationMax Loss', -1869.0186), ('TV(2.0)
Loss', 971.47327)], overall loss: -897.5452880859375
Iteration: 670, named\_losses: [('ActivationMax Loss', -1869.7998), ('TV(2.0)
Loss', 968.70355)], overall loss: -901.0962524414062
Iteration: 671, named\_losses: [('ActivationMax Loss', -1860.3025), ('TV(2.0)
Loss', 961.99866)], overall loss: -898.3038330078125
Iteration: 672, named\_losses: [('ActivationMax Loss', -1865.7832), ('TV(2.0)
Loss', 967.3775)], overall loss: -898.4057006835938
Iteration: 673, named\_losses: [('ActivationMax Loss', -1872.1082), ('TV(2.0)
Loss', 972.7954)], overall loss: -899.312744140625
Iteration: 674, named\_losses: [('ActivationMax Loss', -1862.5673), ('TV(2.0)
Loss', 963.88727)], overall loss: -898.6799926757812
Iteration: 675, named\_losses: [('ActivationMax Loss', -1875.3516), ('TV(2.0)
Loss', 973.3112)], overall loss: -902.0403442382812
Iteration: 676, named\_losses: [('ActivationMax Loss', -1856.6926), ('TV(2.0)
Loss', 959.55396)], overall loss: -897.138671875
Iteration: 677, named\_losses: [('ActivationMax Loss', -1870.0056), ('TV(2.0)
Loss', 970.5928)], overall loss: -899.412841796875
Iteration: 678, named\_losses: [('ActivationMax Loss', -1859.74), ('TV(2.0)
Loss', 963.6127)], overall loss: -896.1273193359375
Iteration: 679, named\_losses: [('ActivationMax Loss', -1872.439), ('TV(2.0)
Loss', 970.824)], overall loss: -901.614990234375
Iteration: 680, named\_losses: [('ActivationMax Loss', -1867.9939), ('TV(2.0)
Loss', 968.2318)], overall loss: -899.7620849609375
Iteration: 681, named\_losses: [('ActivationMax Loss', -1873.2124), ('TV(2.0)
Loss', 972.1171)], overall loss: -901.0952758789062
Iteration: 682, named\_losses: [('ActivationMax Loss', -1869.6986), ('TV(2.0)
Loss', 966.55707)], overall loss: -903.1415405273438
Iteration: 683, named\_losses: [('ActivationMax Loss', -1869.571), ('TV(2.0)
Loss', 971.3795)], overall loss: -898.1915283203125
Iteration: 684, named\_losses: [('ActivationMax Loss', -1862.9503), ('TV(2.0)
Loss', 964.06915)], overall loss: -898.8811645507812
Iteration: 685, named\_losses: [('ActivationMax Loss', -1865.9602), ('TV(2.0)
Loss', 968.6365)], overall loss: -897.32373046875
Iteration: 686, named\_losses: [('ActivationMax Loss', -1864.501), ('TV(2.0)
Loss', 963.6785)], overall loss: -900.8224487304688
Iteration: 687, named\_losses: [('ActivationMax Loss', -1868.5902), ('TV(2.0)
Loss', 968.29987)], overall loss: -900.2903442382812
Iteration: 688, named\_losses: [('ActivationMax Loss', -1862.5336), ('TV(2.0)
Loss', 962.20233)], overall loss: -900.3312377929688
Iteration: 689, named\_losses: [('ActivationMax Loss', -1865.8082), ('TV(2.0)
Loss', 966.87317)], overall loss: -898.93505859375
Iteration: 690, named\_losses: [('ActivationMax Loss', -1865.0524), ('TV(2.0)
Loss', 965.4362)], overall loss: -899.6161499023438
Iteration: 691, named\_losses: [('ActivationMax Loss', -1866.4557), ('TV(2.0)
Loss', 966.3359)], overall loss: -900.1198120117188
Iteration: 692, named\_losses: [('ActivationMax Loss', -1860.2377), ('TV(2.0)
Loss', 959.1457)], overall loss: -901.0919799804688
Iteration: 693, named\_losses: [('ActivationMax Loss', -1874.7222), ('TV(2.0)
Loss', 969.82806)], overall loss: -904.8941040039062
Iteration: 694, named\_losses: [('ActivationMax Loss', -1856.1715), ('TV(2.0)
Loss', 957.7183)], overall loss: -898.4531860351562
Iteration: 695, named\_losses: [('ActivationMax Loss', -1872.4329), ('TV(2.0)
Loss', 971.0709)], overall loss: -901.3619384765625
Iteration: 696, named\_losses: [('ActivationMax Loss', -1864.0354), ('TV(2.0)
Loss', 963.8811)], overall loss: -900.154296875
Iteration: 697, named\_losses: [('ActivationMax Loss', -1873.3525), ('TV(2.0)
Loss', 973.3675)], overall loss: -899.9850463867188
Iteration: 698, named\_losses: [('ActivationMax Loss', -1866.7134), ('TV(2.0)
Loss', 968.86255)], overall loss: -897.850830078125
Iteration: 699, named\_losses: [('ActivationMax Loss', -1870.3201), ('TV(2.0)
Loss', 968.9569)], overall loss: -901.3631591796875
Iteration: 700, named\_losses: [('ActivationMax Loss', -1860.9308), ('TV(2.0)
Loss', 966.15717)], overall loss: -894.7736206054688
Iteration: 701, named\_losses: [('ActivationMax Loss', -1875.1329), ('TV(2.0)
Loss', 972.67914)], overall loss: -902.4537963867188
Iteration: 702, named\_losses: [('ActivationMax Loss', -1865.5363), ('TV(2.0)
Loss', 967.713)], overall loss: -897.8232421875
Iteration: 703, named\_losses: [('ActivationMax Loss', -1872.7867), ('TV(2.0)
Loss', 975.744)], overall loss: -897.042724609375
Iteration: 704, named\_losses: [('ActivationMax Loss', -1862.7579), ('TV(2.0)
Loss', 963.33167)], overall loss: -899.42626953125
Iteration: 705, named\_losses: [('ActivationMax Loss', -1866.8091), ('TV(2.0)
Loss', 968.9713)], overall loss: -897.8377685546875
Iteration: 706, named\_losses: [('ActivationMax Loss', -1866.9058), ('TV(2.0)
Loss', 966.4952)], overall loss: -900.4105834960938
Iteration: 707, named\_losses: [('ActivationMax Loss', -1873.1166), ('TV(2.0)
Loss', 974.7274)], overall loss: -898.38916015625
Iteration: 708, named\_losses: [('ActivationMax Loss', -1865.0172), ('TV(2.0)
Loss', 961.7787)], overall loss: -903.238525390625
Iteration: 709, named\_losses: [('ActivationMax Loss', -1870.2732), ('TV(2.0)
Loss', 969.9823)], overall loss: -900.2908935546875
Iteration: 710, named\_losses: [('ActivationMax Loss', -1865.8431), ('TV(2.0)
Loss', 962.36115)], overall loss: -903.4819946289062
Iteration: 711, named\_losses: [('ActivationMax Loss', -1870.9806), ('TV(2.0)
Loss', 973.78485)], overall loss: -897.1957397460938
Iteration: 712, named\_losses: [('ActivationMax Loss', -1860.4706), ('TV(2.0)
Loss', 956.19244)], overall loss: -904.2781372070312
Iteration: 713, named\_losses: [('ActivationMax Loss', -1872.9811), ('TV(2.0)
Loss', 971.25665)], overall loss: -901.7244262695312
Iteration: 714, named\_losses: [('ActivationMax Loss', -1868.2391), ('TV(2.0)
Loss', 966.6051)], overall loss: -901.634033203125
Iteration: 715, named\_losses: [('ActivationMax Loss', -1866.3147), ('TV(2.0)
Loss', 969.7296)], overall loss: -896.5850830078125
Iteration: 716, named\_losses: [('ActivationMax Loss', -1863.0555), ('TV(2.0)
Loss', 961.3277)], overall loss: -901.7278442382812
Iteration: 717, named\_losses: [('ActivationMax Loss', -1871.8834), ('TV(2.0)
Loss', 974.29443)], overall loss: -897.5889892578125
Iteration: 718, named\_losses: [('ActivationMax Loss', -1870.5052), ('TV(2.0)
Loss', 968.7386)], overall loss: -901.7666625976562
Iteration: 719, named\_losses: [('ActivationMax Loss', -1869.3594), ('TV(2.0)
Loss', 971.92377)], overall loss: -897.4356079101562
Iteration: 720, named\_losses: [('ActivationMax Loss', -1867.2683), ('TV(2.0)
Loss', 962.6056)], overall loss: -904.6627197265625
Iteration: 721, named\_losses: [('ActivationMax Loss', -1868.7587), ('TV(2.0)
Loss', 972.73645)], overall loss: -896.022216796875
Iteration: 722, named\_losses: [('ActivationMax Loss', -1864.7826), ('TV(2.0)
Loss', 963.1686)], overall loss: -901.614013671875
Iteration: 723, named\_losses: [('ActivationMax Loss', -1867.3076), ('TV(2.0)
Loss', 969.3614)], overall loss: -897.9462280273438
Iteration: 724, named\_losses: [('ActivationMax Loss', -1862.471), ('TV(2.0)
Loss', 960.9317)], overall loss: -901.5392456054688
Iteration: 725, named\_losses: [('ActivationMax Loss', -1867.2255), ('TV(2.0)
Loss', 967.5196)], overall loss: -899.7058715820312
Iteration: 726, named\_losses: [('ActivationMax Loss', -1859.9894), ('TV(2.0)
Loss', 960.434)], overall loss: -899.5553588867188
Iteration: 727, named\_losses: [('ActivationMax Loss', -1869.279), ('TV(2.0)
Loss', 965.1003)], overall loss: -904.1787719726562
Iteration: 728, named\_losses: [('ActivationMax Loss', -1862.1384), ('TV(2.0)
Loss', 963.72797)], overall loss: -898.4104614257812
Iteration: 729, named\_losses: [('ActivationMax Loss', -1872.358), ('TV(2.0)
Loss', 969.0438)], overall loss: -903.314208984375
Iteration: 730, named\_losses: [('ActivationMax Loss', -1865.3743), ('TV(2.0)
Loss', 964.5545)], overall loss: -900.8197631835938
Iteration: 731, named\_losses: [('ActivationMax Loss', -1874.6443), ('TV(2.0)
Loss', 970.8147)], overall loss: -903.82958984375
Iteration: 732, named\_losses: [('ActivationMax Loss', -1862.0217), ('TV(2.0)
Loss', 967.2596)], overall loss: -894.7621459960938
Iteration: 733, named\_losses: [('ActivationMax Loss', -1876.1622), ('TV(2.0)
Loss', 975.72504)], overall loss: -900.4371948242188
Iteration: 734, named\_losses: [('ActivationMax Loss', -1862.516), ('TV(2.0)
Loss', 962.5457)], overall loss: -899.9702758789062
Iteration: 735, named\_losses: [('ActivationMax Loss', -1873.3058), ('TV(2.0)
Loss', 970.2759)], overall loss: -903.0299072265625
Iteration: 736, named\_losses: [('ActivationMax Loss', -1858.4474), ('TV(2.0)
Loss', 959.9827)], overall loss: -898.4646606445312
Iteration: 737, named\_losses: [('ActivationMax Loss', -1871.2048), ('TV(2.0)
Loss', 969.21173)], overall loss: -901.9931030273438
Iteration: 738, named\_losses: [('ActivationMax Loss', -1863.3843), ('TV(2.0)
Loss', 965.87854)], overall loss: -897.5057373046875
Iteration: 739, named\_losses: [('ActivationMax Loss', -1870.2645), ('TV(2.0)
Loss', 965.7188)], overall loss: -904.5457153320312
Iteration: 740, named\_losses: [('ActivationMax Loss', -1857.1327), ('TV(2.0)
Loss', 960.66046)], overall loss: -896.4722290039062
Iteration: 741, named\_losses: [('ActivationMax Loss', -1875.5968), ('TV(2.0)
Loss', 972.32666)], overall loss: -903.2701416015625
Iteration: 742, named\_losses: [('ActivationMax Loss', -1860.8986), ('TV(2.0)
Loss', 963.5028)], overall loss: -897.395751953125
Iteration: 743, named\_losses: [('ActivationMax Loss', -1874.4088), ('TV(2.0)
Loss', 972.29706)], overall loss: -902.1117553710938
Iteration: 744, named\_losses: [('ActivationMax Loss', -1865.5637), ('TV(2.0)
Loss', 962.8338)], overall loss: -902.7299194335938
Iteration: 745, named\_losses: [('ActivationMax Loss', -1870.3931), ('TV(2.0)
Loss', 969.006)], overall loss: -901.3870849609375
Iteration: 746, named\_losses: [('ActivationMax Loss', -1867.5847), ('TV(2.0)
Loss', 964.7945)], overall loss: -902.7902221679688
Iteration: 747, named\_losses: [('ActivationMax Loss', -1869.2434), ('TV(2.0)
Loss', 970.05505)], overall loss: -899.1883544921875
Iteration: 748, named\_losses: [('ActivationMax Loss', -1867.1338), ('TV(2.0)
Loss', 964.5658)], overall loss: -902.5679931640625
Iteration: 749, named\_losses: [('ActivationMax Loss', -1868.5614), ('TV(2.0)
Loss', 965.08276)], overall loss: -903.4786376953125
Iteration: 750, named\_losses: [('ActivationMax Loss', -1864.7516), ('TV(2.0)
Loss', 963.4785)], overall loss: -901.2730712890625
Iteration: 751, named\_losses: [('ActivationMax Loss', -1870.718), ('TV(2.0)
Loss', 971.28394)], overall loss: -899.43408203125
Iteration: 752, named\_losses: [('ActivationMax Loss', -1866.5536), ('TV(2.0)
Loss', 965.34955)], overall loss: -901.2040405273438
Iteration: 753, named\_losses: [('ActivationMax Loss', -1870.3043), ('TV(2.0)
Loss', 967.7763)], overall loss: -902.5280151367188
Iteration: 754, named\_losses: [('ActivationMax Loss', -1866.7217), ('TV(2.0)
Loss', 968.2793)], overall loss: -898.4423828125
Iteration: 755, named\_losses: [('ActivationMax Loss', -1873.4062), ('TV(2.0)
Loss', 969.3923)], overall loss: -904.0139770507812
Iteration: 756, named\_losses: [('ActivationMax Loss', -1869.1385), ('TV(2.0)
Loss', 972.7069)], overall loss: -896.431640625
Iteration: 757, named\_losses: [('ActivationMax Loss', -1874.9684), ('TV(2.0)
Loss', 972.8022)], overall loss: -902.1661987304688
Iteration: 758, named\_losses: [('ActivationMax Loss', -1863.5896), ('TV(2.0)
Loss', 964.8043)], overall loss: -898.7852783203125
Iteration: 759, named\_losses: [('ActivationMax Loss', -1869.1217), ('TV(2.0)
Loss', 968.6398)], overall loss: -900.48193359375
Iteration: 760, named\_losses: [('ActivationMax Loss', -1858.595), ('TV(2.0)
Loss', 959.6486)], overall loss: -898.9463500976562
Iteration: 761, named\_losses: [('ActivationMax Loss', -1868.463), ('TV(2.0)
Loss', 968.49805)], overall loss: -899.9649658203125
Iteration: 762, named\_losses: [('ActivationMax Loss', -1860.611), ('TV(2.0)
Loss', 959.4448)], overall loss: -901.1661376953125
Iteration: 763, named\_losses: [('ActivationMax Loss', -1874.3354), ('TV(2.0)
Loss', 972.6535)], overall loss: -901.6819458007812
Iteration: 764, named\_losses: [('ActivationMax Loss', -1867.2399), ('TV(2.0)
Loss', 962.2909)], overall loss: -904.948974609375
Iteration: 765, named\_losses: [('ActivationMax Loss', -1869.8401), ('TV(2.0)
Loss', 970.0713)], overall loss: -899.768798828125
Iteration: 766, named\_losses: [('ActivationMax Loss', -1868.4064), ('TV(2.0)
Loss', 965.4044)], overall loss: -903.001953125
Iteration: 767, named\_losses: [('ActivationMax Loss', -1872.079), ('TV(2.0)
Loss', 965.9795)], overall loss: -906.0994873046875
Iteration: 768, named\_losses: [('ActivationMax Loss', -1860.7035), ('TV(2.0)
Loss', 963.4265)], overall loss: -897.2769775390625
Iteration: 769, named\_losses: [('ActivationMax Loss', -1873.3528), ('TV(2.0)
Loss', 967.9943)], overall loss: -905.3584594726562
Iteration: 770, named\_losses: [('ActivationMax Loss', -1860.4141), ('TV(2.0)
Loss', 960.8232)], overall loss: -899.5908813476562
Iteration: 771, named\_losses: [('ActivationMax Loss', -1876.8793), ('TV(2.0)
Loss', 971.6419)], overall loss: -905.2373657226562
Iteration: 772, named\_losses: [('ActivationMax Loss', -1861.8176), ('TV(2.0)
Loss', 963.4985)], overall loss: -898.3191528320312
Iteration: 773, named\_losses: [('ActivationMax Loss', -1876.7228), ('TV(2.0)
Loss', 973.32367)], overall loss: -903.3991088867188
Iteration: 774, named\_losses: [('ActivationMax Loss', -1863.5394), ('TV(2.0)
Loss', 965.0458)], overall loss: -898.49365234375
Iteration: 775, named\_losses: [('ActivationMax Loss', -1877.6875), ('TV(2.0)
Loss', 975.8101)], overall loss: -901.8773803710938
Iteration: 776, named\_losses: [('ActivationMax Loss', -1865.6595), ('TV(2.0)
Loss', 967.827)], overall loss: -897.83251953125
Iteration: 777, named\_losses: [('ActivationMax Loss', -1871.3932), ('TV(2.0)
Loss', 972.1221)], overall loss: -899.2711181640625
Iteration: 778, named\_losses: [('ActivationMax Loss', -1863.7235), ('TV(2.0)
Loss', 962.9698)], overall loss: -900.7537231445312
Iteration: 779, named\_losses: [('ActivationMax Loss', -1870.5432), ('TV(2.0)
Loss', 968.69635)], overall loss: -901.8468627929688
Iteration: 780, named\_losses: [('ActivationMax Loss', -1862.5573), ('TV(2.0)
Loss', 963.4727)], overall loss: -899.0845336914062
Iteration: 781, named\_losses: [('ActivationMax Loss', -1877.6691), ('TV(2.0)
Loss', 976.5044)], overall loss: -901.1646728515625
Iteration: 782, named\_losses: [('ActivationMax Loss', -1868.5847), ('TV(2.0)
Loss', 966.45856)], overall loss: -902.1261596679688
Iteration: 783, named\_losses: [('ActivationMax Loss', -1870.8453), ('TV(2.0)
Loss', 969.8977)], overall loss: -900.9476318359375
Iteration: 784, named\_losses: [('ActivationMax Loss', -1868.5712), ('TV(2.0)
Loss', 967.314)], overall loss: -901.2571411132812
Iteration: 785, named\_losses: [('ActivationMax Loss', -1870.7129), ('TV(2.0)
Loss', 972.2731)], overall loss: -898.4398193359375
Iteration: 786, named\_losses: [('ActivationMax Loss', -1868.7394), ('TV(2.0)
Loss', 967.30426)], overall loss: -901.4351196289062
Iteration: 787, named\_losses: [('ActivationMax Loss', -1869.7915), ('TV(2.0)
Loss', 966.71246)], overall loss: -903.0790405273438
Iteration: 788, named\_losses: [('ActivationMax Loss', -1863.6375), ('TV(2.0)
Loss', 964.68396)], overall loss: -898.9534912109375
Iteration: 789, named\_losses: [('ActivationMax Loss', -1875.5426), ('TV(2.0)
Loss', 973.2873)], overall loss: -902.2553100585938
Iteration: 790, named\_losses: [('ActivationMax Loss', -1868.5779), ('TV(2.0)
Loss', 965.8799)], overall loss: -902.697998046875
Iteration: 791, named\_losses: [('ActivationMax Loss', -1869.7524), ('TV(2.0)
Loss', 963.67664)], overall loss: -906.0758056640625
Iteration: 792, named\_losses: [('ActivationMax Loss', -1868.1494), ('TV(2.0)
Loss', 969.54553)], overall loss: -898.6038818359375
Iteration: 793, named\_losses: [('ActivationMax Loss', -1873.1188), ('TV(2.0)
Loss', 971.8889)], overall loss: -901.2298583984375
Iteration: 794, named\_losses: [('ActivationMax Loss', -1865.8605), ('TV(2.0)
Loss', 963.8825)], overall loss: -901.9779663085938
Iteration: 795, named\_losses: [('ActivationMax Loss', -1870.0979), ('TV(2.0)
Loss', 970.8353)], overall loss: -899.2625732421875
Iteration: 796, named\_losses: [('ActivationMax Loss', -1868.7385), ('TV(2.0)
Loss', 968.4588)], overall loss: -900.2797241210938
Iteration: 797, named\_losses: [('ActivationMax Loss', -1874.9622), ('TV(2.0)
Loss', 972.0924)], overall loss: -902.8697509765625
Iteration: 798, named\_losses: [('ActivationMax Loss', -1864.9425), ('TV(2.0)
Loss', 966.4024)], overall loss: -898.5401000976562
Iteration: 799, named\_losses: [('ActivationMax Loss', -1870.6152), ('TV(2.0)
Loss', 972.35126)], overall loss: -898.2639770507812
Iteration: 800, named\_losses: [('ActivationMax Loss', -1864.9784), ('TV(2.0)
Loss', 965.3916)], overall loss: -899.5867919921875
Iteration: 801, named\_losses: [('ActivationMax Loss', -1873.1108), ('TV(2.0)
Loss', 972.27344)], overall loss: -900.83740234375
Iteration: 802, named\_losses: [('ActivationMax Loss', -1869.8412), ('TV(2.0)
Loss', 967.47687)], overall loss: -902.3643188476562
Iteration: 803, named\_losses: [('ActivationMax Loss', -1877.5237), ('TV(2.0)
Loss', 972.18097)], overall loss: -905.3427124023438
Iteration: 804, named\_losses: [('ActivationMax Loss', -1864.8839), ('TV(2.0)
Loss', 964.2303)], overall loss: -900.6536254882812
Iteration: 805, named\_losses: [('ActivationMax Loss', -1877.1685), ('TV(2.0)
Loss', 975.97217)], overall loss: -901.1962890625
Iteration: 806, named\_losses: [('ActivationMax Loss', -1867.1313), ('TV(2.0)
Loss', 968.47974)], overall loss: -898.651611328125
Iteration: 807, named\_losses: [('ActivationMax Loss', -1879.0583), ('TV(2.0)
Loss', 977.1354)], overall loss: -901.9229736328125
Iteration: 808, named\_losses: [('ActivationMax Loss', -1867.4685), ('TV(2.0)
Loss', 969.2186)], overall loss: -898.2498779296875
Iteration: 809, named\_losses: [('ActivationMax Loss', -1877.9917), ('TV(2.0)
Loss', 973.7437)], overall loss: -904.2479858398438
Iteration: 810, named\_losses: [('ActivationMax Loss', -1862.4829), ('TV(2.0)
Loss', 961.9517)], overall loss: -900.5311889648438
Iteration: 811, named\_losses: [('ActivationMax Loss', -1875.4612), ('TV(2.0)
Loss', 972.09656)], overall loss: -903.3646240234375
Iteration: 812, named\_losses: [('ActivationMax Loss', -1862.9879), ('TV(2.0)
Loss', 961.4254)], overall loss: -901.5625
Iteration: 813, named\_losses: [('ActivationMax Loss', -1879.3583), ('TV(2.0)
Loss', 978.5744)], overall loss: -900.7838745117188
Iteration: 814, named\_losses: [('ActivationMax Loss', -1871.3876), ('TV(2.0)
Loss', 967.9691)], overall loss: -903.41845703125
Iteration: 815, named\_losses: [('ActivationMax Loss', -1879.6719), ('TV(2.0)
Loss', 976.0472)], overall loss: -903.6246948242188
Iteration: 816, named\_losses: [('ActivationMax Loss', -1869.6385), ('TV(2.0)
Loss', 965.6564)], overall loss: -903.982177734375
Iteration: 817, named\_losses: [('ActivationMax Loss', -1875.6012), ('TV(2.0)
Loss', 973.4437)], overall loss: -902.157470703125
Iteration: 818, named\_losses: [('ActivationMax Loss', -1870.7019), ('TV(2.0)
Loss', 967.04724)], overall loss: -903.6546630859375
Iteration: 819, named\_losses: [('ActivationMax Loss', -1878.0732), ('TV(2.0)
Loss', 973.87024)], overall loss: -904.2030029296875
Iteration: 820, named\_losses: [('ActivationMax Loss', -1865.619), ('TV(2.0)
Loss', 964.38354)], overall loss: -901.2354736328125
Iteration: 821, named\_losses: [('ActivationMax Loss', -1872.5195), ('TV(2.0)
Loss', 971.44745)], overall loss: -901.0720825195312
Iteration: 822, named\_losses: [('ActivationMax Loss', -1868.4753), ('TV(2.0)
Loss', 967.4583)], overall loss: -901.0170288085938
Iteration: 823, named\_losses: [('ActivationMax Loss', -1883.9357), ('TV(2.0)
Loss', 980.1261)], overall loss: -903.8095703125
Iteration: 824, named\_losses: [('ActivationMax Loss', -1868.6002), ('TV(2.0)
Loss', 964.52496)], overall loss: -904.0752563476562
Iteration: 825, named\_losses: [('ActivationMax Loss', -1876.732), ('TV(2.0)
Loss', 972.8601)], overall loss: -903.8719482421875
Iteration: 826, named\_losses: [('ActivationMax Loss', -1866.518), ('TV(2.0)
Loss', 966.5819)], overall loss: -899.93603515625
Iteration: 827, named\_losses: [('ActivationMax Loss', -1877.1383), ('TV(2.0)
Loss', 974.80347)], overall loss: -902.3348388671875
Iteration: 828, named\_losses: [('ActivationMax Loss', -1871.125), ('TV(2.0)
Loss', 967.64453)], overall loss: -903.48046875
Iteration: 829, named\_losses: [('ActivationMax Loss', -1874.4923), ('TV(2.0)
Loss', 972.13306)], overall loss: -902.3592529296875
Iteration: 830, named\_losses: [('ActivationMax Loss', -1866.4208), ('TV(2.0)
Loss', 963.4048)], overall loss: -903.0159912109375
Iteration: 831, named\_losses: [('ActivationMax Loss', -1879.4792), ('TV(2.0)
Loss', 979.3045)], overall loss: -900.1747436523438
Iteration: 832, named\_losses: [('ActivationMax Loss', -1869.3112), ('TV(2.0)
Loss', 967.7716)], overall loss: -901.53955078125
Iteration: 833, named\_losses: [('ActivationMax Loss', -1879.768), ('TV(2.0)
Loss', 981.59656)], overall loss: -898.17138671875
Iteration: 834, named\_losses: [('ActivationMax Loss', -1868.5547), ('TV(2.0)
Loss', 965.9464)], overall loss: -902.6082763671875
Iteration: 835, named\_losses: [('ActivationMax Loss', -1876.183), ('TV(2.0)
Loss', 978.6811)], overall loss: -897.5018920898438
Iteration: 836, named\_losses: [('ActivationMax Loss', -1867.2449), ('TV(2.0)
Loss', 962.871)], overall loss: -904.3739013671875
Iteration: 837, named\_losses: [('ActivationMax Loss', -1875.1815), ('TV(2.0)
Loss', 975.0763)], overall loss: -900.105224609375
Iteration: 838, named\_losses: [('ActivationMax Loss', -1865.273), ('TV(2.0)
Loss', 965.3992)], overall loss: -899.873779296875
Iteration: 839, named\_losses: [('ActivationMax Loss', -1880.8854), ('TV(2.0)
Loss', 975.1332)], overall loss: -905.752197265625
Iteration: 840, named\_losses: [('ActivationMax Loss', -1867.9827), ('TV(2.0)
Loss', 964.7534)], overall loss: -903.229248046875
Iteration: 841, named\_losses: [('ActivationMax Loss', -1883.6083), ('TV(2.0)
Loss', 976.4392)], overall loss: -907.1690673828125
Iteration: 842, named\_losses: [('ActivationMax Loss', -1873.1516), ('TV(2.0)
Loss', 969.38196)], overall loss: -903.7696533203125
Iteration: 843, named\_losses: [('ActivationMax Loss', -1881.3445), ('TV(2.0)
Loss', 973.92175)], overall loss: -907.4227294921875
Iteration: 844, named\_losses: [('ActivationMax Loss', -1866.3987), ('TV(2.0)
Loss', 961.2622)], overall loss: -905.136474609375
Iteration: 845, named\_losses: [('ActivationMax Loss', -1875.1035), ('TV(2.0)
Loss', 967.90375)], overall loss: -907.1997680664062
Iteration: 846, named\_losses: [('ActivationMax Loss', -1875.8165), ('TV(2.0)
Loss', 970.218)], overall loss: -905.5985107421875
Iteration: 847, named\_losses: [('ActivationMax Loss', -1881.1866), ('TV(2.0)
Loss', 973.6439)], overall loss: -907.542724609375
Iteration: 848, named\_losses: [('ActivationMax Loss', -1881.2865), ('TV(2.0)
Loss', 973.5335)], overall loss: -907.7529907226562
Iteration: 849, named\_losses: [('ActivationMax Loss', -1886.0798), ('TV(2.0)
Loss', 978.18146)], overall loss: -907.8983764648438
Iteration: 850, named\_losses: [('ActivationMax Loss', -1885.6282), ('TV(2.0)
Loss', 976.5844)], overall loss: -909.0437622070312
Iteration: 851, named\_losses: [('ActivationMax Loss', -1883.3113), ('TV(2.0)
Loss', 974.93536)], overall loss: -908.3759155273438
Iteration: 852, named\_losses: [('ActivationMax Loss', -1880.061), ('TV(2.0)
Loss', 968.73193)], overall loss: -911.3291015625
Iteration: 853, named\_losses: [('ActivationMax Loss', -1883.0809), ('TV(2.0)
Loss', 973.17267)], overall loss: -909.9082641601562
Iteration: 854, named\_losses: [('ActivationMax Loss', -1875.8212), ('TV(2.0)
Loss', 965.6987)], overall loss: -910.1224365234375
Iteration: 855, named\_losses: [('ActivationMax Loss', -1886.0115), ('TV(2.0)
Loss', 978.488)], overall loss: -907.5234985351562
Iteration: 856, named\_losses: [('ActivationMax Loss', -1883.8341), ('TV(2.0)
Loss', 973.8628)], overall loss: -909.9713134765625
Iteration: 857, named\_losses: [('ActivationMax Loss', -1887.3928), ('TV(2.0)
Loss', 981.2998)], overall loss: -906.093017578125
Iteration: 858, named\_losses: [('ActivationMax Loss', -1877.5214), ('TV(2.0)
Loss', 964.6129)], overall loss: -912.908447265625
Iteration: 859, named\_losses: [('ActivationMax Loss', -1887.3176), ('TV(2.0)
Loss', 980.3809)], overall loss: -906.9367065429688
Iteration: 860, named\_losses: [('ActivationMax Loss', -1871.3741), ('TV(2.0)
Loss', 962.78595)], overall loss: -908.5881958007812
Iteration: 861, named\_losses: [('ActivationMax Loss', -1885.2467), ('TV(2.0)
Loss', 979.1161)], overall loss: -906.130615234375
Iteration: 862, named\_losses: [('ActivationMax Loss', -1872.1115), ('TV(2.0)
Loss', 964.6472)], overall loss: -907.4642333984375
Iteration: 863, named\_losses: [('ActivationMax Loss', -1884.8337), ('TV(2.0)
Loss', 978.0581)], overall loss: -906.775634765625
Iteration: 864, named\_losses: [('ActivationMax Loss', -1874.9305), ('TV(2.0)
Loss', 964.16364)], overall loss: -910.7669067382812
Iteration: 865, named\_losses: [('ActivationMax Loss', -1882.0874), ('TV(2.0)
Loss', 976.355)], overall loss: -905.732421875
Iteration: 866, named\_losses: [('ActivationMax Loss', -1882.1134), ('TV(2.0)
Loss', 973.7099)], overall loss: -908.4035034179688
Iteration: 867, named\_losses: [('ActivationMax Loss', -1882.8909), ('TV(2.0)
Loss', 974.304)], overall loss: -908.5868530273438
Iteration: 868, named\_losses: [('ActivationMax Loss', -1874.0183), ('TV(2.0)
Loss', 966.7582)], overall loss: -907.2601318359375
Iteration: 869, named\_losses: [('ActivationMax Loss', -1885.7498), ('TV(2.0)
Loss', 979.8654)], overall loss: -905.8843383789062
Iteration: 870, named\_losses: [('ActivationMax Loss', -1884.1035), ('TV(2.0)
Loss', 974.4485)], overall loss: -909.655029296875
Iteration: 871, named\_losses: [('ActivationMax Loss', -1882.0464), ('TV(2.0)
Loss', 975.00635)], overall loss: -907.0400390625
Iteration: 872, named\_losses: [('ActivationMax Loss', -1878.3196), ('TV(2.0)
Loss', 968.5696)], overall loss: -909.75
Iteration: 873, named\_losses: [('ActivationMax Loss', -1884.54), ('TV(2.0)
Loss', 978.5359)], overall loss: -906.004150390625
Iteration: 874, named\_losses: [('ActivationMax Loss', -1877.5295), ('TV(2.0)
Loss', 966.1469)], overall loss: -911.3826293945312
Iteration: 875, named\_losses: [('ActivationMax Loss', -1876.4294), ('TV(2.0)
Loss', 968.8316)], overall loss: -907.5978393554688
Iteration: 876, named\_losses: [('ActivationMax Loss', -1874.8125), ('TV(2.0)
Loss', 966.0067)], overall loss: -908.8057861328125
Iteration: 877, named\_losses: [('ActivationMax Loss', -1879.436), ('TV(2.0)
Loss', 971.29443)], overall loss: -908.1416015625
Iteration: 878, named\_losses: [('ActivationMax Loss', -1877.4019), ('TV(2.0)
Loss', 970.52374)], overall loss: -906.8781127929688
Iteration: 879, named\_losses: [('ActivationMax Loss', -1885.8988), ('TV(2.0)
Loss', 976.4683)], overall loss: -909.4304809570312
Iteration: 880, named\_losses: [('ActivationMax Loss', -1876.9908), ('TV(2.0)
Loss', 971.7829)], overall loss: -905.2079467773438
Iteration: 881, named\_losses: [('ActivationMax Loss', -1886.4731), ('TV(2.0)
Loss', 977.2822)], overall loss: -909.19091796875
Iteration: 882, named\_losses: [('ActivationMax Loss', -1882.1378), ('TV(2.0)
Loss', 972.50775)], overall loss: -909.6300659179688
Iteration: 883, named\_losses: [('ActivationMax Loss', -1880.4801), ('TV(2.0)
Loss', 971.9655)], overall loss: -908.5145874023438
Iteration: 884, named\_losses: [('ActivationMax Loss', -1880.5616), ('TV(2.0)
Loss', 973.92834)], overall loss: -906.63330078125
Iteration: 885, named\_losses: [('ActivationMax Loss', -1875.7615), ('TV(2.0)
Loss', 969.48)], overall loss: -906.281494140625
Iteration: 886, named\_losses: [('ActivationMax Loss', -1883.1926), ('TV(2.0)
Loss', 974.2908)], overall loss: -908.90185546875
Iteration: 887, named\_losses: [('ActivationMax Loss', -1872.7598), ('TV(2.0)
Loss', 967.3397)], overall loss: -905.4200439453125
Iteration: 888, named\_losses: [('ActivationMax Loss', -1881.2676), ('TV(2.0)
Loss', 973.35596)], overall loss: -907.91162109375
Iteration: 889, named\_losses: [('ActivationMax Loss', -1880.269), ('TV(2.0)
Loss', 974.503)], overall loss: -905.7660522460938
Iteration: 890, named\_losses: [('ActivationMax Loss', -1885.1923), ('TV(2.0)
Loss', 976.20026)], overall loss: -908.9920043945312
Iteration: 891, named\_losses: [('ActivationMax Loss', -1880.0812), ('TV(2.0)
Loss', 975.96295)], overall loss: -904.1182250976562
Iteration: 892, named\_losses: [('ActivationMax Loss', -1884.6431), ('TV(2.0)
Loss', 972.931)], overall loss: -911.7120361328125
Iteration: 893, named\_losses: [('ActivationMax Loss', -1881.0122), ('TV(2.0)
Loss', 976.3006)], overall loss: -904.7116088867188
Iteration: 894, named\_losses: [('ActivationMax Loss', -1882.0474), ('TV(2.0)
Loss', 975.9125)], overall loss: -906.1348876953125
Iteration: 895, named\_losses: [('ActivationMax Loss', -1888.1462), ('TV(2.0)
Loss', 976.14624)], overall loss: -912.0
Iteration: 896, named\_losses: [('ActivationMax Loss', -1879.7018), ('TV(2.0)
Loss', 971.8866)], overall loss: -907.815185546875
Iteration: 897, named\_losses: [('ActivationMax Loss', -1884.9946), ('TV(2.0)
Loss', 978.3473)], overall loss: -906.6473388671875
Iteration: 898, named\_losses: [('ActivationMax Loss', -1880.8907), ('TV(2.0)
Loss', 970.06696)], overall loss: -910.8237915039062
Iteration: 899, named\_losses: [('ActivationMax Loss', -1887.3967), ('TV(2.0)
Loss', 977.66376)], overall loss: -909.7329711914062
Iteration: 900, named\_losses: [('ActivationMax Loss', -1881.9397), ('TV(2.0)
Loss', 972.21716)], overall loss: -909.7225341796875
Iteration: 901, named\_losses: [('ActivationMax Loss', -1883.1384), ('TV(2.0)
Loss', 977.1742)], overall loss: -905.9642333984375
Iteration: 902, named\_losses: [('ActivationMax Loss', -1875.9384), ('TV(2.0)
Loss', 967.2112)], overall loss: -908.7271728515625
Iteration: 903, named\_losses: [('ActivationMax Loss', -1881.6566), ('TV(2.0)
Loss', 975.3862)], overall loss: -906.2703857421875
Iteration: 904, named\_losses: [('ActivationMax Loss', -1874.5604), ('TV(2.0)
Loss', 969.5656)], overall loss: -904.9948120117188
Iteration: 905, named\_losses: [('ActivationMax Loss', -1885.6558), ('TV(2.0)
Loss', 977.26825)], overall loss: -908.3875122070312
Iteration: 906, named\_losses: [('ActivationMax Loss', -1884.3427), ('TV(2.0)
Loss', 974.22656)], overall loss: -910.1160888671875
Iteration: 907, named\_losses: [('ActivationMax Loss', -1886.6333), ('TV(2.0)
Loss', 981.1086)], overall loss: -905.5247192382812
Iteration: 908, named\_losses: [('ActivationMax Loss', -1879.9401), ('TV(2.0)
Loss', 973.567)], overall loss: -906.373046875
Iteration: 909, named\_losses: [('ActivationMax Loss', -1884.6802), ('TV(2.0)
Loss', 977.61615)], overall loss: -907.0640258789062
Iteration: 910, named\_losses: [('ActivationMax Loss', -1886.8981), ('TV(2.0)
Loss', 975.68616)], overall loss: -911.2119140625
Iteration: 911, named\_losses: [('ActivationMax Loss', -1885.6611), ('TV(2.0)
Loss', 980.23596)], overall loss: -905.4251708984375
Iteration: 912, named\_losses: [('ActivationMax Loss', -1886.8975), ('TV(2.0)
Loss', 975.6903)], overall loss: -911.2071533203125
Iteration: 913, named\_losses: [('ActivationMax Loss', -1883.5815), ('TV(2.0)
Loss', 974.83527)], overall loss: -908.7462768554688
Iteration: 914, named\_losses: [('ActivationMax Loss', -1876.7848), ('TV(2.0)
Loss', 971.58765)], overall loss: -905.1971435546875
Iteration: 915, named\_losses: [('ActivationMax Loss', -1882.9014), ('TV(2.0)
Loss', 973.2019)], overall loss: -909.699462890625
Iteration: 916, named\_losses: [('ActivationMax Loss', -1882.2745), ('TV(2.0)
Loss', 974.9813)], overall loss: -907.293212890625
Iteration: 917, named\_losses: [('ActivationMax Loss', -1887.338), ('TV(2.0)
Loss', 975.81866)], overall loss: -911.5193481445312
Iteration: 918, named\_losses: [('ActivationMax Loss', -1882.0775), ('TV(2.0)
Loss', 974.93976)], overall loss: -907.1377563476562
Iteration: 919, named\_losses: [('ActivationMax Loss', -1886.1777), ('TV(2.0)
Loss', 981.25287)], overall loss: -904.9248657226562
Iteration: 920, named\_losses: [('ActivationMax Loss', -1884.5621), ('TV(2.0)
Loss', 975.71106)], overall loss: -908.85107421875
Iteration: 921, named\_losses: [('ActivationMax Loss', -1882.2177), ('TV(2.0)
Loss', 972.38525)], overall loss: -909.8323974609375
Iteration: 922, named\_losses: [('ActivationMax Loss', -1878.9938), ('TV(2.0)
Loss', 967.60876)], overall loss: -911.385009765625
Iteration: 923, named\_losses: [('ActivationMax Loss', -1889.6006), ('TV(2.0)
Loss', 981.2386)], overall loss: -908.3619995117188
Iteration: 924, named\_losses: [('ActivationMax Loss', -1879.17), ('TV(2.0)
Loss', 970.90845)], overall loss: -908.2615966796875
Iteration: 925, named\_losses: [('ActivationMax Loss', -1890.2382), ('TV(2.0)
Loss', 979.3837)], overall loss: -910.8544311523438
Iteration: 926, named\_losses: [('ActivationMax Loss', -1877.7307), ('TV(2.0)
Loss', 971.73376)], overall loss: -905.9969482421875
Iteration: 927, named\_losses: [('ActivationMax Loss', -1888.9006), ('TV(2.0)
Loss', 980.227)], overall loss: -908.6736450195312
Iteration: 928, named\_losses: [('ActivationMax Loss', -1877.8306), ('TV(2.0)
Loss', 967.6881)], overall loss: -910.1424560546875
Iteration: 929, named\_losses: [('ActivationMax Loss', -1886.0677), ('TV(2.0)
Loss', 981.47266)], overall loss: -904.5950927734375
Iteration: 930, named\_losses: [('ActivationMax Loss', -1883.1392), ('TV(2.0)
Loss', 970.57715)], overall loss: -912.56201171875
Iteration: 931, named\_losses: [('ActivationMax Loss', -1886.4138), ('TV(2.0)
Loss', 980.6466)], overall loss: -905.7672119140625
Iteration: 932, named\_losses: [('ActivationMax Loss', -1881.6326), ('TV(2.0)
Loss', 971.5497)], overall loss: -910.0828857421875
Iteration: 933, named\_losses: [('ActivationMax Loss', -1890.1135), ('TV(2.0)
Loss', 980.23584)], overall loss: -909.877685546875
Iteration: 934, named\_losses: [('ActivationMax Loss', -1883.3694), ('TV(2.0)
Loss', 972.264)], overall loss: -911.1054077148438
Iteration: 935, named\_losses: [('ActivationMax Loss', -1891.532), ('TV(2.0)
Loss', 979.1541)], overall loss: -912.3778686523438
Iteration: 936, named\_losses: [('ActivationMax Loss', -1883.8231), ('TV(2.0)
Loss', 971.99255)], overall loss: -911.83056640625
Iteration: 937, named\_losses: [('ActivationMax Loss', -1889.9978), ('TV(2.0)
Loss', 982.07526)], overall loss: -907.9225463867188
Iteration: 938, named\_losses: [('ActivationMax Loss', -1886.1265), ('TV(2.0)
Loss', 977.3572)], overall loss: -908.769287109375
Iteration: 939, named\_losses: [('ActivationMax Loss', -1889.6514), ('TV(2.0)
Loss', 975.90216)], overall loss: -913.7492065429688
Iteration: 940, named\_losses: [('ActivationMax Loss', -1885.5132), ('TV(2.0)
Loss', 978.2035)], overall loss: -907.3096923828125
Iteration: 941, named\_losses: [('ActivationMax Loss', -1885.3115), ('TV(2.0)
Loss', 976.50806)], overall loss: -908.803466796875
Iteration: 942, named\_losses: [('ActivationMax Loss', -1886.2198), ('TV(2.0)
Loss', 977.9296)], overall loss: -908.2902221679688
Iteration: 943, named\_losses: [('ActivationMax Loss', -1883.6501), ('TV(2.0)
Loss', 972.4854)], overall loss: -911.1647338867188
Iteration: 944, named\_losses: [('ActivationMax Loss', -1883.4988), ('TV(2.0)
Loss', 978.41425)], overall loss: -905.0845336914062
Iteration: 945, named\_losses: [('ActivationMax Loss', -1887.7559), ('TV(2.0)
Loss', 975.7589)], overall loss: -911.9969482421875
Iteration: 946, named\_losses: [('ActivationMax Loss', -1885.4044), ('TV(2.0)
Loss', 971.124)], overall loss: -914.2803955078125
Iteration: 947, named\_losses: [('ActivationMax Loss', -1886.3672), ('TV(2.0)
Loss', 973.6669)], overall loss: -912.7003173828125
Iteration: 948, named\_losses: [('ActivationMax Loss', -1881.2203), ('TV(2.0)
Loss', 975.5419)], overall loss: -905.678466796875
Iteration: 949, named\_losses: [('ActivationMax Loss', -1881.9224), ('TV(2.0)
Loss', 973.2359)], overall loss: -908.6864624023438
Iteration: 950, named\_losses: [('ActivationMax Loss', -1881.8838), ('TV(2.0)
Loss', 973.7731)], overall loss: -908.1107177734375
Iteration: 951, named\_losses: [('ActivationMax Loss', -1888.0812), ('TV(2.0)
Loss', 977.45264)], overall loss: -910.6285400390625
Iteration: 952, named\_losses: [('ActivationMax Loss', -1880.1128), ('TV(2.0)
Loss', 971.34216)], overall loss: -908.7706298828125
Iteration: 953, named\_losses: [('ActivationMax Loss', -1882.7141), ('TV(2.0)
Loss', 975.52313)], overall loss: -907.1909790039062
Iteration: 954, named\_losses: [('ActivationMax Loss', -1882.0602), ('TV(2.0)
Loss', 972.9474)], overall loss: -909.11279296875
Iteration: 955, named\_losses: [('ActivationMax Loss', -1889.1567), ('TV(2.0)
Loss', 977.70483)], overall loss: -911.451904296875
Iteration: 956, named\_losses: [('ActivationMax Loss', -1881.4825), ('TV(2.0)
Loss', 971.39734)], overall loss: -910.085205078125
Iteration: 957, named\_losses: [('ActivationMax Loss', -1884.7579), ('TV(2.0)
Loss', 975.7835)], overall loss: -908.9744262695312
Iteration: 958, named\_losses: [('ActivationMax Loss', -1887.0312), ('TV(2.0)
Loss', 975.69135)], overall loss: -911.3399047851562
Iteration: 959, named\_losses: [('ActivationMax Loss', -1882.3411), ('TV(2.0)
Loss', 976.47046)], overall loss: -905.87060546875
Iteration: 960, named\_losses: [('ActivationMax Loss', -1883.48), ('TV(2.0)
Loss', 974.0424)], overall loss: -909.4375610351562
Iteration: 961, named\_losses: [('ActivationMax Loss', -1880.2751), ('TV(2.0)
Loss', 971.97125)], overall loss: -908.3038940429688
Iteration: 962, named\_losses: [('ActivationMax Loss', -1881.0018), ('TV(2.0)
Loss', 971.4002)], overall loss: -909.6016235351562
Iteration: 963, named\_losses: [('ActivationMax Loss', -1885.8151), ('TV(2.0)
Loss', 976.28577)], overall loss: -909.529296875
Iteration: 964, named\_losses: [('ActivationMax Loss', -1884.0114), ('TV(2.0)
Loss', 972.32416)], overall loss: -911.6871948242188
Iteration: 965, named\_losses: [('ActivationMax Loss', -1894.1655), ('TV(2.0)
Loss', 982.16925)], overall loss: -911.9962768554688
Iteration: 966, named\_losses: [('ActivationMax Loss', -1886.1841), ('TV(2.0)
Loss', 975.494)], overall loss: -910.6900634765625
Iteration: 967, named\_losses: [('ActivationMax Loss', -1883.361), ('TV(2.0)
Loss', 976.88763)], overall loss: -906.4733276367188
Iteration: 968, named\_losses: [('ActivationMax Loss', -1886.3988), ('TV(2.0)
Loss', 970.01215)], overall loss: -916.3866577148438
Iteration: 969, named\_losses: [('ActivationMax Loss', -1885.7397), ('TV(2.0)
Loss', 980.5148)], overall loss: -905.2249755859375
Iteration: 970, named\_losses: [('ActivationMax Loss', -1888.3699), ('TV(2.0)
Loss', 974.5016)], overall loss: -913.8682861328125
Iteration: 971, named\_losses: [('ActivationMax Loss', -1888.807), ('TV(2.0)
Loss', 979.57623)], overall loss: -909.2307739257812
Iteration: 972, named\_losses: [('ActivationMax Loss', -1883.5524), ('TV(2.0)
Loss', 973.34247)], overall loss: -910.2098999023438
Iteration: 973, named\_losses: [('ActivationMax Loss', -1887.3695), ('TV(2.0)
Loss', 976.57086)], overall loss: -910.7986450195312
Iteration: 974, named\_losses: [('ActivationMax Loss', -1884.9865), ('TV(2.0)
Loss', 975.17255)], overall loss: -909.8139038085938
Iteration: 975, named\_losses: [('ActivationMax Loss', -1885.416), ('TV(2.0)
Loss', 974.6608)], overall loss: -910.7551879882812
Iteration: 976, named\_losses: [('ActivationMax Loss', -1882.5509), ('TV(2.0)
Loss', 972.39056)], overall loss: -910.1603393554688
Iteration: 977, named\_losses: [('ActivationMax Loss', -1884.8265), ('TV(2.0)
Loss', 973.86615)], overall loss: -910.9603881835938
Iteration: 978, named\_losses: [('ActivationMax Loss', -1881.0442), ('TV(2.0)
Loss', 967.8645)], overall loss: -913.1796875
Iteration: 979, named\_losses: [('ActivationMax Loss', -1892.7628), ('TV(2.0)
Loss', 982.64215)], overall loss: -910.1206665039062
Iteration: 980, named\_losses: [('ActivationMax Loss', -1879.7676), ('TV(2.0)
Loss', 970.2762)], overall loss: -909.4913940429688
Iteration: 981, named\_losses: [('ActivationMax Loss', -1886.1346), ('TV(2.0)
Loss', 974.1655)], overall loss: -911.9691162109375
Iteration: 982, named\_losses: [('ActivationMax Loss', -1882.531), ('TV(2.0)
Loss', 970.5994)], overall loss: -911.9315795898438
Iteration: 983, named\_losses: [('ActivationMax Loss', -1889.1656), ('TV(2.0)
Loss', 981.5794)], overall loss: -907.5862426757812
Iteration: 984, named\_losses: [('ActivationMax Loss', -1887.0714), ('TV(2.0)
Loss', 975.81506)], overall loss: -911.25634765625
Iteration: 985, named\_losses: [('ActivationMax Loss', -1888.0304), ('TV(2.0)
Loss', 975.20215)], overall loss: -912.8282470703125
Iteration: 986, named\_losses: [('ActivationMax Loss', -1882.2566), ('TV(2.0)
Loss', 974.10803)], overall loss: -908.1485595703125
Iteration: 987, named\_losses: [('ActivationMax Loss', -1886.492), ('TV(2.0)
Loss', 976.7716)], overall loss: -909.7203369140625
Iteration: 988, named\_losses: [('ActivationMax Loss', -1881.4005), ('TV(2.0)
Loss', 972.44916)], overall loss: -908.9513549804688
Iteration: 989, named\_losses: [('ActivationMax Loss', -1894.8596), ('TV(2.0)
Loss', 985.1898)], overall loss: -909.6697998046875
Iteration: 990, named\_losses: [('ActivationMax Loss', -1883.9542), ('TV(2.0)
Loss', 974.6123)], overall loss: -909.3419189453125
Iteration: 991, named\_losses: [('ActivationMax Loss', -1896.1295), ('TV(2.0)
Loss', 983.96124)], overall loss: -912.1682739257812
Iteration: 992, named\_losses: [('ActivationMax Loss', -1881.964), ('TV(2.0)
Loss', 972.79724)], overall loss: -909.166748046875
Iteration: 993, named\_losses: [('ActivationMax Loss', -1893.2584), ('TV(2.0)
Loss', 978.44385)], overall loss: -914.8145751953125
Iteration: 994, named\_losses: [('ActivationMax Loss', -1885.7642), ('TV(2.0)
Loss', 972.96094)], overall loss: -912.80322265625
Iteration: 995, named\_losses: [('ActivationMax Loss', -1892.4442), ('TV(2.0)
Loss', 983.3328)], overall loss: -909.1113891601562
Iteration: 996, named\_losses: [('ActivationMax Loss', -1888.4454), ('TV(2.0)
Loss', 980.5321)], overall loss: -907.913330078125
Iteration: 997, named\_losses: [('ActivationMax Loss', -1890.5228), ('TV(2.0)
Loss', 982.1003)], overall loss: -908.4225463867188
Iteration: 998, named\_losses: [('ActivationMax Loss', -1884.481), ('TV(2.0)
Loss', 973.9667)], overall loss: -910.5142822265625
Iteration: 999, named\_losses: [('ActivationMax Loss', -1880.1327), ('TV(2.0)
Loss', 972.67395)], overall loss: -907.458740234375
Iteration: 1000, named\_losses: [('ActivationMax Loss', -1875.9835), ('TV(2.0)
Loss', 966.7787)], overall loss: -909.204833984375
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.image.AxesImage at 0x7f188a34fb00>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Q7_files/Q7_42_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{tunning-total-variation-parameter}{%
\subsection*{7.4.2 Tunning total variation
parameter}\label{tunning-total-variation-parameter}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{filter\PYZus{}idx} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{tv\PYZus{}weight} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{16}\PY{p}{]}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Lets turn off verbose output this time to avoid clutter and just see the output.}
    \PY{n}{img} \PY{o}{=} \PY{n}{visualize\PYZus{}activation}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{layer\PYZus{}idx}\PY{p}{,} \PY{n}{filter\PYZus{}indices}\PY{o}{=}\PY{n}{filter\PYZus{}idx}\PY{p}{,} \PY{n}{input\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.}\PY{p}{,} \PY{l+m+mf}{1.}\PY{p}{)}\PY{p}{,} 
                               \PY{n}{tv\PYZus{}weight}\PY{o}{=}\PY{n}{tv\PYZus{}weight}\PY{p}{,} \PY{n}{lp\PYZus{}norm\PYZus{}weight}\PY{o}{=}\PY{l+m+mf}{10.}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}plt.figure()}
    \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{img}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Q7_files/Q7_44_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Total variation = 8 seems to be a good value (default=10)

    \hypertarget{tunning-l-p-norm-parameter}{%
\subsection*{7.4.3 Tunning L-p norm
parameter}\label{tunning-l-p-norm-parameter}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{filter\PYZus{}idx} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{lpnorm} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{16}\PY{p}{]}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Lets turn off verbose output this time to avoid clutter and just see the output.}
    \PY{n}{img} \PY{o}{=} \PY{n}{visualize\PYZus{}activation}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{layer\PYZus{}idx}\PY{p}{,} \PY{n}{filter\PYZus{}indices}\PY{o}{=}\PY{n}{filter\PYZus{}idx}\PY{p}{,} \PY{n}{input\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.}\PY{p}{,} \PY{l+m+mf}{1.}\PY{p}{)}\PY{p}{,} 
                               \PY{n}{tv\PYZus{}weight}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{,} \PY{n}{lp\PYZus{}norm\PYZus{}weight}\PY{o}{=}\PY{n}{lpnorm}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}plt.figure()}
    \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{img}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Q7_files/Q7_47_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    L-p norm = 10 seems to be a good value (default=10)

    \hypertarget{visualizing-input-that-maximizes-the-output-of-class-0}{%
\subsection*{7.4.4 Visualizing input that maximizes the output of class
0}\label{visualizing-input-that-maximizes-the-output-of-class-0}}

(Tunned parameters: total variation = 8, L-p norm = 10)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{filter\PYZus{}idx} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{img} \PY{o}{=} \PY{n}{visualize\PYZus{}activation}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{layer\PYZus{}idx}\PY{p}{,}
    \PY{n}{filter\PYZus{}indices}\PY{o}{=}\PY{n}{filter\PYZus{}idx}\PY{p}{,} \PY{n}{input\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.}\PY{p}{,} \PY{l+m+mf}{1.}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
    \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{tv\PYZus{}weight}\PY{o}{=}\PY{l+m+mf}{8.}\PY{p}{,} \PY{n}{lp\PYZus{}norm\PYZus{}weight}\PY{o}{=}\PY{l+m+mf}{10.}\PY{p}{)}
\PY{c+c1}{\PYZsh{}plot.imshow(img.squeeze(), cmap=\PYZsq{}seismic\PYZsq{}, interpolation=\PYZsq{}nearest\PYZsq{})}
\PY{n}{plot}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{img}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration: 1, named\_losses: [('ActivationMax Loss', -0.009474453),
 ('L-6.0 Norm Loss', 0.019827215),
 ('TV(2.0) Loss', 0.072719716)], overall loss: 0.0830724760890007
Iteration: 2, named\_losses: [('ActivationMax Loss', 69.395546),
 ('L-6.0 Norm Loss', 0.16917577),
 ('TV(2.0) Loss', 404.9101)], overall loss: 474.4748229980469
Iteration: 3, named\_losses: [('ActivationMax Loss', -127.113914),
 ('L-6.0 Norm Loss', 0.2202492),
 ('TV(2.0) Loss', 266.17004)], overall loss: 139.27638244628906
Iteration: 4, named\_losses: [('ActivationMax Loss', -339.49835),
 ('L-6.0 Norm Loss', 0.22044517),
 ('TV(2.0) Loss', 218.71342)], overall loss: -120.56446838378906
Iteration: 5, named\_losses: [('ActivationMax Loss', -524.5647),
 ('L-6.0 Norm Loss', 0.24100398),
 ('TV(2.0) Loss', 235.12477)], overall loss: -289.19891357421875
Iteration: 6, named\_losses: [('ActivationMax Loss', -669.2671),
 ('L-6.0 Norm Loss', 0.28415734),
 ('TV(2.0) Loss', 273.10907)], overall loss: -395.87384033203125
Iteration: 7, named\_losses: [('ActivationMax Loss', -806.33636),
 ('L-6.0 Norm Loss', 0.31900427),
 ('TV(2.0) Loss', 317.26352)], overall loss: -488.7538146972656
Iteration: 8, named\_losses: [('ActivationMax Loss', -884.602),
 ('L-6.0 Norm Loss', 0.35897893),
 ('TV(2.0) Loss', 361.45364)], overall loss: -522.789306640625
Iteration: 9, named\_losses: [('ActivationMax Loss', -983.7149),
 ('L-6.0 Norm Loss', 0.37513143),
 ('TV(2.0) Loss', 404.03726)], overall loss: -579.302490234375
Iteration: 10, named\_losses: [('ActivationMax Loss', -1053.8236),
 ('L-6.0 Norm Loss', 0.4072881),
 ('TV(2.0) Loss', 442.70096)], overall loss: -610.71533203125
Iteration: 11, named\_losses: [('ActivationMax Loss', -1127.1771),
 ('L-6.0 Norm Loss', 0.4258074),
 ('TV(2.0) Loss', 475.82272)], overall loss: -650.9285888671875
Iteration: 12, named\_losses: [('ActivationMax Loss', -1182.1348),
 ('L-6.0 Norm Loss', 0.44942507),
 ('TV(2.0) Loss', 501.98236)], overall loss: -679.7029418945312
Iteration: 13, named\_losses: [('ActivationMax Loss', -1248.6685),
 ('L-6.0 Norm Loss', 0.46894586),
 ('TV(2.0) Loss', 535.92523)], overall loss: -712.2742309570312
Iteration: 14, named\_losses: [('ActivationMax Loss', -1288.0822),
 ('L-6.0 Norm Loss', 0.49537432),
 ('TV(2.0) Loss', 551.44543)], overall loss: -736.141357421875
Iteration: 15, named\_losses: [('ActivationMax Loss', -1336.3568),
 ('L-6.0 Norm Loss', 0.5140778),
 ('TV(2.0) Loss', 582.0341)], overall loss: -753.8086547851562
Iteration: 16, named\_losses: [('ActivationMax Loss', -1376.9962),
 ('L-6.0 Norm Loss', 0.53010845),
 ('TV(2.0) Loss', 603.82043)], overall loss: -772.6456298828125
Iteration: 17, named\_losses: [('ActivationMax Loss', -1418.6409),
 ('L-6.0 Norm Loss', 0.54453),
 ('TV(2.0) Loss', 628.5745)], overall loss: -789.5217895507812
Iteration: 18, named\_losses: [('ActivationMax Loss', -1459.5913),
 ('L-6.0 Norm Loss', 0.56494105),
 ('TV(2.0) Loss', 650.4844)], overall loss: -808.5419921875
Iteration: 19, named\_losses: [('ActivationMax Loss', -1490.2969),
 ('L-6.0 Norm Loss', 0.5759895),
 ('TV(2.0) Loss', 664.05817)], overall loss: -825.6626586914062
Iteration: 20, named\_losses: [('ActivationMax Loss', -1518.5415),
 ('L-6.0 Norm Loss', 0.5957742),
 ('TV(2.0) Loss', 679.0164)], overall loss: -838.9292602539062
Iteration: 21, named\_losses: [('ActivationMax Loss', -1548.7172),
 ('L-6.0 Norm Loss', 0.6046154),
 ('TV(2.0) Loss', 698.65894)], overall loss: -849.45361328125
Iteration: 22, named\_losses: [('ActivationMax Loss', -1577.0917),
 ('L-6.0 Norm Loss', 0.62041956),
 ('TV(2.0) Loss', 717.1759)], overall loss: -859.29541015625
Iteration: 23, named\_losses: [('ActivationMax Loss', -1605.3274),
 ('L-6.0 Norm Loss', 0.63695693),
 ('TV(2.0) Loss', 726.8751)], overall loss: -877.8153076171875
Iteration: 24, named\_losses: [('ActivationMax Loss', -1623.7697),
 ('L-6.0 Norm Loss', 0.65175486),
 ('TV(2.0) Loss', 751.1199)], overall loss: -871.998046875
Iteration: 25, named\_losses: [('ActivationMax Loss', -1651.8385),
 ('L-6.0 Norm Loss', 0.661353),
 ('TV(2.0) Loss', 762.22864)], overall loss: -888.948486328125
Iteration: 26, named\_losses: [('ActivationMax Loss', -1663.5018),
 ('L-6.0 Norm Loss', 0.6768377),
 ('TV(2.0) Loss', 775.79767)], overall loss: -887.0272827148438
Iteration: 27, named\_losses: [('ActivationMax Loss', -1687.162),
 ('L-6.0 Norm Loss', 0.68625134),
 ('TV(2.0) Loss', 778.64624)], overall loss: -907.8294677734375
Iteration: 28, named\_losses: [('ActivationMax Loss', -1710.3284),
 ('L-6.0 Norm Loss', 0.69603455),
 ('TV(2.0) Loss', 803.96625)], overall loss: -905.6660766601562
Iteration: 29, named\_losses: [('ActivationMax Loss', -1729.9409),
 ('L-6.0 Norm Loss', 0.70737547),
 ('TV(2.0) Loss', 806.51886)], overall loss: -922.7146606445312
Iteration: 30, named\_losses: [('ActivationMax Loss', -1745.9856),
 ('L-6.0 Norm Loss', 0.718988),
 ('TV(2.0) Loss', 817.4987)], overall loss: -927.7678833007812
Iteration: 31, named\_losses: [('ActivationMax Loss', -1751.5184),
 ('L-6.0 Norm Loss', 0.7256298),
 ('TV(2.0) Loss', 823.5043)], overall loss: -927.28857421875
Iteration: 32, named\_losses: [('ActivationMax Loss', -1769.8345),
 ('L-6.0 Norm Loss', 0.7336786),
 ('TV(2.0) Loss', 834.46576)], overall loss: -934.6350708007812
Iteration: 33, named\_losses: [('ActivationMax Loss', -1786.8877),
 ('L-6.0 Norm Loss', 0.7467415),
 ('TV(2.0) Loss', 846.37823)], overall loss: -939.7627563476562
Iteration: 34, named\_losses: [('ActivationMax Loss', -1806.4807),
 ('L-6.0 Norm Loss', 0.74795216),
 ('TV(2.0) Loss', 867.3482)], overall loss: -938.3845825195312
Iteration: 35, named\_losses: [('ActivationMax Loss', -1816.9448),
 ('L-6.0 Norm Loss', 0.7610598),
 ('TV(2.0) Loss', 867.673)], overall loss: -948.5107421875
Iteration: 36, named\_losses: [('ActivationMax Loss', -1829.2703),
 ('L-6.0 Norm Loss', 0.7636953),
 ('TV(2.0) Loss', 880.13464)], overall loss: -948.3719482421875
Iteration: 37, named\_losses: [('ActivationMax Loss', -1838.4392),
 ('L-6.0 Norm Loss', 0.7764633),
 ('TV(2.0) Loss', 880.7962)], overall loss: -956.8665161132812
Iteration: 38, named\_losses: [('ActivationMax Loss', -1845.2535),
 ('L-6.0 Norm Loss', 0.7833338),
 ('TV(2.0) Loss', 893.54877)], overall loss: -950.9214477539062
Iteration: 39, named\_losses: [('ActivationMax Loss', -1851.4534),
 ('L-6.0 Norm Loss', 0.78487235),
 ('TV(2.0) Loss', 887.70374)], overall loss: -962.9647216796875
Iteration: 40, named\_losses: [('ActivationMax Loss', -1858.6078),
 ('L-6.0 Norm Loss', 0.79281336),
 ('TV(2.0) Loss', 904.7707)], overall loss: -953.0442504882812
Iteration: 41, named\_losses: [('ActivationMax Loss', -1877.8792),
 ('L-6.0 Norm Loss', 0.80252796),
 ('TV(2.0) Loss', 912.55237)], overall loss: -964.5242919921875
Iteration: 42, named\_losses: [('ActivationMax Loss', -1875.223),
 ('L-6.0 Norm Loss', 0.8037646),
 ('TV(2.0) Loss', 920.8667)], overall loss: -953.5526123046875
Iteration: 43, named\_losses: [('ActivationMax Loss', -1894.2384),
 ('L-6.0 Norm Loss', 0.8133094),
 ('TV(2.0) Loss', 920.62036)], overall loss: -972.8046875
Iteration: 44, named\_losses: [('ActivationMax Loss', -1886.7635),
 ('L-6.0 Norm Loss', 0.817789),
 ('TV(2.0) Loss', 925.92975)], overall loss: -960.0160522460938
Iteration: 45, named\_losses: [('ActivationMax Loss', -1907.1517),
 ('L-6.0 Norm Loss', 0.8205437),
 ('TV(2.0) Loss', 929.2069)], overall loss: -977.124267578125
Iteration: 46, named\_losses: [('ActivationMax Loss', -1902.0792),
 ('L-6.0 Norm Loss', 0.8282979),
 ('TV(2.0) Loss', 938.33185)], overall loss: -962.9191284179688
Iteration: 47, named\_losses: [('ActivationMax Loss', -1916.1318),
 ('L-6.0 Norm Loss', 0.8368817),
 ('TV(2.0) Loss', 936.2794)], overall loss: -979.0155029296875
Iteration: 48, named\_losses: [('ActivationMax Loss', -1916.9152),
 ('L-6.0 Norm Loss', 0.84314024),
 ('TV(2.0) Loss', 952.1978)], overall loss: -963.8742065429688
Iteration: 49, named\_losses: [('ActivationMax Loss', -1927.1205),
 ('L-6.0 Norm Loss', 0.84429175),
 ('TV(2.0) Loss', 944.4006)], overall loss: -981.8756713867188
Iteration: 50, named\_losses: [('ActivationMax Loss', -1923.0343),
 ('L-6.0 Norm Loss', 0.8512851),
 ('TV(2.0) Loss', 952.69025)], overall loss: -969.4927368164062
Iteration: 51, named\_losses: [('ActivationMax Loss', -1933.4886),
 ('L-6.0 Norm Loss', 0.85568786),
 ('TV(2.0) Loss', 953.7785)], overall loss: -978.8544311523438
Iteration: 52, named\_losses: [('ActivationMax Loss', -1943.0577),
 ('L-6.0 Norm Loss', 0.85769516),
 ('TV(2.0) Loss', 971.4966)], overall loss: -970.7034912109375
Iteration: 53, named\_losses: [('ActivationMax Loss', -1948.7784),
 ('L-6.0 Norm Loss', 0.8604076),
 ('TV(2.0) Loss', 965.10065)], overall loss: -982.8174438476562
Iteration: 54, named\_losses: [('ActivationMax Loss', -1951.0364),
 ('L-6.0 Norm Loss', 0.8619263),
 ('TV(2.0) Loss', 973.76135)], overall loss: -976.4130859375
Iteration: 55, named\_losses: [('ActivationMax Loss', -1952.6676),
 ('L-6.0 Norm Loss', 0.8701223),
 ('TV(2.0) Loss', 969.7007)], overall loss: -982.0968017578125
Iteration: 56, named\_losses: [('ActivationMax Loss', -1954.8324),
 ('L-6.0 Norm Loss', 0.86567926),
 ('TV(2.0) Loss', 972.55707)], overall loss: -981.4096069335938
Iteration: 57, named\_losses: [('ActivationMax Loss', -1954.5482),
 ('L-6.0 Norm Loss', 0.87422526),
 ('TV(2.0) Loss', 970.02747)], overall loss: -983.646484375
Iteration: 58, named\_losses: [('ActivationMax Loss', -1959.5846),
 ('L-6.0 Norm Loss', 0.8737063),
 ('TV(2.0) Loss', 972.1557)], overall loss: -986.5552368164062
Iteration: 59, named\_losses: [('ActivationMax Loss', -1967.4801),
 ('L-6.0 Norm Loss', 0.87908536),
 ('TV(2.0) Loss', 976.4849)], overall loss: -990.1161499023438
Iteration: 60, named\_losses: [('ActivationMax Loss', -1973.6107),
 ('L-6.0 Norm Loss', 0.882747),
 ('TV(2.0) Loss', 981.27374)], overall loss: -991.4542846679688
Iteration: 61, named\_losses: [('ActivationMax Loss', -1982.7936),
 ('L-6.0 Norm Loss', 0.88411444),
 ('TV(2.0) Loss', 985.73364)], overall loss: -996.17578125
Iteration: 62, named\_losses: [('ActivationMax Loss', -1980.8986),
 ('L-6.0 Norm Loss', 0.8850646),
 ('TV(2.0) Loss', 985.87463)], overall loss: -994.138916015625
Iteration: 63, named\_losses: [('ActivationMax Loss', -1994.9017),
 ('L-6.0 Norm Loss', 0.89215916),
 ('TV(2.0) Loss', 995.8219)], overall loss: -998.1876220703125
Iteration: 64, named\_losses: [('ActivationMax Loss', -1989.2911),
 ('L-6.0 Norm Loss', 0.8920101),
 ('TV(2.0) Loss', 993.542)], overall loss: -994.857177734375
Iteration: 65, named\_losses: [('ActivationMax Loss', -1999.0675),
 ('L-6.0 Norm Loss', 0.89545715),
 ('TV(2.0) Loss', 999.3833)], overall loss: -998.7886962890625
Iteration: 66, named\_losses: [('ActivationMax Loss', -2000.8723),
 ('L-6.0 Norm Loss', 0.8941609),
 ('TV(2.0) Loss', 997.31415)], overall loss: -1002.6640014648438
Iteration: 67, named\_losses: [('ActivationMax Loss', -1998.5875),
 ('L-6.0 Norm Loss', 0.9018254),
 ('TV(2.0) Loss', 993.09827)], overall loss: -1004.58740234375
Iteration: 68, named\_losses: [('ActivationMax Loss', -2001.4172),
 ('L-6.0 Norm Loss', 0.90566164),
 ('TV(2.0) Loss', 998.01434)], overall loss: -1002.4972534179688
Iteration: 69, named\_losses: [('ActivationMax Loss', -2013.701),
 ('L-6.0 Norm Loss', 0.9046115),
 ('TV(2.0) Loss', 1007.58417)], overall loss: -1005.2122192382812
Iteration: 70, named\_losses: [('ActivationMax Loss', -2013.896),
 ('L-6.0 Norm Loss', 0.9105818),
 ('TV(2.0) Loss', 1005.4266)], overall loss: -1007.5588989257812
Iteration: 71, named\_losses: [('ActivationMax Loss', -2016.5463),
 ('L-6.0 Norm Loss', 0.91781974),
 ('TV(2.0) Loss', 1010.67334)], overall loss: -1004.955078125
Iteration: 72, named\_losses: [('ActivationMax Loss', -2018.2673),
 ('L-6.0 Norm Loss', 0.9194032),
 ('TV(2.0) Loss', 1009.73834)], overall loss: -1007.6095581054688
Iteration: 73, named\_losses: [('ActivationMax Loss', -2026.1526),
 ('L-6.0 Norm Loss', 0.92195886),
 ('TV(2.0) Loss', 1013.9276)], overall loss: -1011.302978515625
Iteration: 74, named\_losses: [('ActivationMax Loss', -2023.667),
 ('L-6.0 Norm Loss', 0.92575574),
 ('TV(2.0) Loss', 1012.8012)], overall loss: -1009.9400024414062
Iteration: 75, named\_losses: [('ActivationMax Loss', -2032.1299),
 ('L-6.0 Norm Loss', 0.9279592),
 ('TV(2.0) Loss', 1016.3745)], overall loss: -1014.827392578125
Iteration: 76, named\_losses: [('ActivationMax Loss', -2032.5847),
 ('L-6.0 Norm Loss', 0.9280393),
 ('TV(2.0) Loss', 1020.90717)], overall loss: -1010.7495727539062
Iteration: 77, named\_losses: [('ActivationMax Loss', -2038.6528),
 ('L-6.0 Norm Loss', 0.93092436),
 ('TV(2.0) Loss', 1025.1656)], overall loss: -1012.5562744140625
Iteration: 78, named\_losses: [('ActivationMax Loss', -2037.6989),
 ('L-6.0 Norm Loss', 0.9351503),
 ('TV(2.0) Loss', 1018.0597)], overall loss: -1018.7039794921875
Iteration: 79, named\_losses: [('ActivationMax Loss', -2040.8011),
 ('L-6.0 Norm Loss', 0.93259215),
 ('TV(2.0) Loss', 1018.7031)], overall loss: -1021.1654052734375
Iteration: 80, named\_losses: [('ActivationMax Loss', -2035.3865),
 ('L-6.0 Norm Loss', 0.9374141),
 ('TV(2.0) Loss', 1022.3471)], overall loss: -1012.1019897460938
Iteration: 81, named\_losses: [('ActivationMax Loss', -2049.959),
 ('L-6.0 Norm Loss', 0.9403806),
 ('TV(2.0) Loss', 1028.4163)], overall loss: -1020.602294921875
Iteration: 82, named\_losses: [('ActivationMax Loss', -2047.2838),
 ('L-6.0 Norm Loss', 0.9414582),
 ('TV(2.0) Loss', 1033.3639)], overall loss: -1012.978515625
Iteration: 83, named\_losses: [('ActivationMax Loss', -2060.2483),
 ('L-6.0 Norm Loss', 0.94542557),
 ('TV(2.0) Loss', 1036.893)], overall loss: -1022.4100341796875
Iteration: 84, named\_losses: [('ActivationMax Loss', -2051.4512),
 ('L-6.0 Norm Loss', 0.94462734),
 ('TV(2.0) Loss', 1027.3337)], overall loss: -1023.1728515625
Iteration: 85, named\_losses: [('ActivationMax Loss', -2062.7957),
 ('L-6.0 Norm Loss', 0.9490455),
 ('TV(2.0) Loss', 1038.7755)], overall loss: -1023.0711669921875
Iteration: 86, named\_losses: [('ActivationMax Loss', -2065.1345),
 ('L-6.0 Norm Loss', 0.946713),
 ('TV(2.0) Loss', 1038.6744)], overall loss: -1025.5133056640625
Iteration: 87, named\_losses: [('ActivationMax Loss', -2066.7397),
 ('L-6.0 Norm Loss', 0.9535304),
 ('TV(2.0) Loss', 1046.1483)], overall loss: -1019.6378173828125
Iteration: 88, named\_losses: [('ActivationMax Loss', -2071.3574),
 ('L-6.0 Norm Loss', 0.95797956),
 ('TV(2.0) Loss', 1044.2938)], overall loss: -1026.1055908203125
Iteration: 89, named\_losses: [('ActivationMax Loss', -2069.6975),
 ('L-6.0 Norm Loss', 0.95678675),
 ('TV(2.0) Loss', 1038.9432)], overall loss: -1029.7974853515625
Iteration: 90, named\_losses: [('ActivationMax Loss', -2069.0928),
 ('L-6.0 Norm Loss', 0.9603752),
 ('TV(2.0) Loss', 1044.6742)], overall loss: -1023.4581298828125
Iteration: 91, named\_losses: [('ActivationMax Loss', -2085.1487),
 ('L-6.0 Norm Loss', 0.96330607),
 ('TV(2.0) Loss', 1047.9877)], overall loss: -1036.1976318359375
Iteration: 92, named\_losses: [('ActivationMax Loss', -2075.0842),
 ('L-6.0 Norm Loss', 0.96371055),
 ('TV(2.0) Loss', 1042.3848)], overall loss: -1031.73583984375
Iteration: 93, named\_losses: [('ActivationMax Loss', -2086.4014),
 ('L-6.0 Norm Loss', 0.9677395),
 ('TV(2.0) Loss', 1046.3658)], overall loss: -1039.0677490234375
Iteration: 94, named\_losses: [('ActivationMax Loss', -2083.2434),
 ('L-6.0 Norm Loss', 0.9722326),
 ('TV(2.0) Loss', 1042.8165)], overall loss: -1039.4547119140625
Iteration: 95, named\_losses: [('ActivationMax Loss', -2094.781),
 ('L-6.0 Norm Loss', 0.97133625),
 ('TV(2.0) Loss', 1055.2577)], overall loss: -1038.5518798828125
Iteration: 96, named\_losses: [('ActivationMax Loss', -2094.128),
 ('L-6.0 Norm Loss', 0.97761726),
 ('TV(2.0) Loss', 1048.5695)], overall loss: -1044.5809326171875
Iteration: 97, named\_losses: [('ActivationMax Loss', -2099.6516),
 ('L-6.0 Norm Loss', 0.9785254),
 ('TV(2.0) Loss', 1053.4481)], overall loss: -1045.2249755859375
Iteration: 98, named\_losses: [('ActivationMax Loss', -2100.7004),
 ('L-6.0 Norm Loss', 0.98703086),
 ('TV(2.0) Loss', 1050.0626)], overall loss: -1049.6507568359375
Iteration: 99, named\_losses: [('ActivationMax Loss', -2098.3818),
 ('L-6.0 Norm Loss', 0.9859718),
 ('TV(2.0) Loss', 1053.3757)], overall loss: -1044.02001953125
Iteration: 100, named\_losses: [('ActivationMax Loss', -2108.9968),
 ('L-6.0 Norm Loss', 0.9914922),
 ('TV(2.0) Loss', 1059.5208)], overall loss: -1048.484619140625
Iteration: 101, named\_losses: [('ActivationMax Loss', -2105.9124),
 ('L-6.0 Norm Loss', 0.9908885),
 ('TV(2.0) Loss', 1054.5039)], overall loss: -1050.41748046875
Iteration: 102, named\_losses: [('ActivationMax Loss', -2113.4675),
 ('L-6.0 Norm Loss', 0.9944895),
 ('TV(2.0) Loss', 1062.8735)], overall loss: -1049.599609375
Iteration: 103, named\_losses: [('ActivationMax Loss', -2110.2898),
 ('L-6.0 Norm Loss', 0.99410665),
 ('TV(2.0) Loss', 1059.7178)], overall loss: -1049.577880859375
Iteration: 104, named\_losses: [('ActivationMax Loss', -2119.426),
 ('L-6.0 Norm Loss', 0.99908054),
 ('TV(2.0) Loss', 1075.1387)], overall loss: -1043.288330078125
Iteration: 105, named\_losses: [('ActivationMax Loss', -2121.0737),
 ('L-6.0 Norm Loss', 1.0049895),
 ('TV(2.0) Loss', 1068.5869)], overall loss: -1051.48193359375
Iteration: 106, named\_losses: [('ActivationMax Loss', -2119.2585),
 ('L-6.0 Norm Loss', 1.0023814),
 ('TV(2.0) Loss', 1071.4786)], overall loss: -1046.7774658203125
Iteration: 107, named\_losses: [('ActivationMax Loss', -2126.644),
 ('L-6.0 Norm Loss', 1.0085548),
 ('TV(2.0) Loss', 1074.4526)], overall loss: -1051.182861328125
Iteration: 108, named\_losses: [('ActivationMax Loss', -2136.1484),
 ('L-6.0 Norm Loss', 1.0066154),
 ('TV(2.0) Loss', 1087.0267)], overall loss: -1048.1151123046875
Iteration: 109, named\_losses: [('ActivationMax Loss', -2123.033),
 ('L-6.0 Norm Loss', 1.0119575),
 ('TV(2.0) Loss', 1067.189)], overall loss: -1054.83203125
Iteration: 110, named\_losses: [('ActivationMax Loss', -2133.5613),
 ('L-6.0 Norm Loss', 1.0093089),
 ('TV(2.0) Loss', 1081.2762)], overall loss: -1051.2757568359375
Iteration: 111, named\_losses: [('ActivationMax Loss', -2133.8157),
 ('L-6.0 Norm Loss', 1.0158105),
 ('TV(2.0) Loss', 1077.7778)], overall loss: -1055.02197265625
Iteration: 112, named\_losses: [('ActivationMax Loss', -2144.1719),
 ('L-6.0 Norm Loss', 1.0158852),
 ('TV(2.0) Loss', 1086.4122)], overall loss: -1056.7437744140625
Iteration: 113, named\_losses: [('ActivationMax Loss', -2138.7563),
 ('L-6.0 Norm Loss', 1.0164124),
 ('TV(2.0) Loss', 1081.6864)], overall loss: -1056.0535888671875
Iteration: 114, named\_losses: [('ActivationMax Loss', -2149.72),
 ('L-6.0 Norm Loss', 1.0209647),
 ('TV(2.0) Loss', 1096.4794)], overall loss: -1052.2196044921875
Iteration: 115, named\_losses: [('ActivationMax Loss', -2135.2852),
 ('L-6.0 Norm Loss', 1.0231957),
 ('TV(2.0) Loss', 1078.701)], overall loss: -1055.5609130859375
Iteration: 116, named\_losses: [('ActivationMax Loss', -2148.204),
 ('L-6.0 Norm Loss', 1.019636),
 ('TV(2.0) Loss', 1092.2156)], overall loss: -1054.968994140625
Iteration: 117, named\_losses: [('ActivationMax Loss', -2146.1902),
 ('L-6.0 Norm Loss', 1.0258534),
 ('TV(2.0) Loss', 1085.0286)], overall loss: -1060.1357421875
Iteration: 118, named\_losses: [('ActivationMax Loss', -2157.423),
 ('L-6.0 Norm Loss', 1.0280435),
 ('TV(2.0) Loss', 1096.9453)], overall loss: -1059.44970703125
Iteration: 119, named\_losses: [('ActivationMax Loss', -2157.5237),
 ('L-6.0 Norm Loss', 1.0272332),
 ('TV(2.0) Loss', 1089.0132)], overall loss: -1067.483154296875
Iteration: 120, named\_losses: [('ActivationMax Loss', -2158.093),
 ('L-6.0 Norm Loss', 1.0273492),
 ('TV(2.0) Loss', 1099.5784)], overall loss: -1057.4873046875
Iteration: 121, named\_losses: [('ActivationMax Loss', -2165.3977),
 ('L-6.0 Norm Loss', 1.0292342),
 ('TV(2.0) Loss', 1094.151)], overall loss: -1070.2174072265625
Iteration: 122, named\_losses: [('ActivationMax Loss', -2167.9502),
 ('L-6.0 Norm Loss', 1.0317892),
 ('TV(2.0) Loss', 1108.1906)], overall loss: -1058.7279052734375
Iteration: 123, named\_losses: [('ActivationMax Loss', -2170.9785),
 ('L-6.0 Norm Loss', 1.0328869),
 ('TV(2.0) Loss', 1103.3658)], overall loss: -1066.5797119140625
Iteration: 124, named\_losses: [('ActivationMax Loss', -2173.9333),
 ('L-6.0 Norm Loss', 1.0295068),
 ('TV(2.0) Loss', 1110.2373)], overall loss: -1062.66650390625
Iteration: 125, named\_losses: [('ActivationMax Loss', -2177.6716),
 ('L-6.0 Norm Loss', 1.0350211),
 ('TV(2.0) Loss', 1111.9548)], overall loss: -1064.681884765625
Iteration: 126, named\_losses: [('ActivationMax Loss', -2175.4556),
 ('L-6.0 Norm Loss', 1.0345435),
 ('TV(2.0) Loss', 1105.8715)], overall loss: -1068.5496826171875
Iteration: 127, named\_losses: [('ActivationMax Loss', -2177.2942),
 ('L-6.0 Norm Loss', 1.03308),
 ('TV(2.0) Loss', 1107.5183)], overall loss: -1068.742919921875
Iteration: 128, named\_losses: [('ActivationMax Loss', -2181.6),
 ('L-6.0 Norm Loss', 1.0344235),
 ('TV(2.0) Loss', 1111.1246)], overall loss: -1069.4410400390625
Iteration: 129, named\_losses: [('ActivationMax Loss', -2182.2756),
 ('L-6.0 Norm Loss', 1.0392455),
 ('TV(2.0) Loss', 1108.8401)], overall loss: -1072.396240234375
Iteration: 130, named\_losses: [('ActivationMax Loss', -2178.8074),
 ('L-6.0 Norm Loss', 1.0354953),
 ('TV(2.0) Loss', 1104.9408)], overall loss: -1072.8311767578125
Iteration: 131, named\_losses: [('ActivationMax Loss', -2175.518),
 ('L-6.0 Norm Loss', 1.0373676),
 ('TV(2.0) Loss', 1105.8956)], overall loss: -1068.5850830078125
Iteration: 132, named\_losses: [('ActivationMax Loss', -2187.3618),
 ('L-6.0 Norm Loss', 1.0354028),
 ('TV(2.0) Loss', 1113.301)], overall loss: -1073.025390625
Iteration: 133, named\_losses: [('ActivationMax Loss', -2177.7244),
 ('L-6.0 Norm Loss', 1.0396941),
 ('TV(2.0) Loss', 1101.263)], overall loss: -1075.421630859375
Iteration: 134, named\_losses: [('ActivationMax Loss', -2185.6162),
 ('L-6.0 Norm Loss', 1.0404582),
 ('TV(2.0) Loss', 1108.9045)], overall loss: -1075.671142578125
Iteration: 135, named\_losses: [('ActivationMax Loss', -2182.6143),
 ('L-6.0 Norm Loss', 1.0409493),
 ('TV(2.0) Loss', 1103.3901)], overall loss: -1078.18310546875
Iteration: 136, named\_losses: [('ActivationMax Loss', -2189.4868),
 ('L-6.0 Norm Loss', 1.0462126),
 ('TV(2.0) Loss', 1116.8325)], overall loss: -1071.608154296875
Iteration: 137, named\_losses: [('ActivationMax Loss', -2183.4495),
 ('L-6.0 Norm Loss', 1.0443534),
 ('TV(2.0) Loss', 1102.7877)], overall loss: -1079.6173095703125
Iteration: 138, named\_losses: [('ActivationMax Loss', -2192.3164),
 ('L-6.0 Norm Loss', 1.0466429),
 ('TV(2.0) Loss', 1120.3196)], overall loss: -1070.9501953125
Iteration: 139, named\_losses: [('ActivationMax Loss', -2191.5933),
 ('L-6.0 Norm Loss', 1.0448785),
 ('TV(2.0) Loss', 1108.707)], overall loss: -1081.84130859375
Iteration: 140, named\_losses: [('ActivationMax Loss', -2201.377),
 ('L-6.0 Norm Loss', 1.0499271),
 ('TV(2.0) Loss', 1125.6915)], overall loss: -1074.6353759765625
Iteration: 141, named\_losses: [('ActivationMax Loss', -2196.4436),
 ('L-6.0 Norm Loss', 1.048588),
 ('TV(2.0) Loss', 1114.579)], overall loss: -1080.8160400390625
Iteration: 142, named\_losses: [('ActivationMax Loss', -2199.2996),
 ('L-6.0 Norm Loss', 1.0480965),
 ('TV(2.0) Loss', 1124.392)], overall loss: -1073.8594970703125
Iteration: 143, named\_losses: [('ActivationMax Loss', -2190.9646),
 ('L-6.0 Norm Loss', 1.0511229),
 ('TV(2.0) Loss', 1113.1881)], overall loss: -1076.7254638671875
Iteration: 144, named\_losses: [('ActivationMax Loss', -2203.4553),
 ('L-6.0 Norm Loss', 1.0515532),
 ('TV(2.0) Loss', 1127.9437)], overall loss: -1074.4600830078125
Iteration: 145, named\_losses: [('ActivationMax Loss', -2197.3296),
 ('L-6.0 Norm Loss', 1.0502164),
 ('TV(2.0) Loss', 1121.4225)], overall loss: -1074.8568115234375
Iteration: 146, named\_losses: [('ActivationMax Loss', -2203.1191),
 ('L-6.0 Norm Loss', 1.0533347),
 ('TV(2.0) Loss', 1123.038)], overall loss: -1079.0279541015625
Iteration: 147, named\_losses: [('ActivationMax Loss', -2190.5327),
 ('L-6.0 Norm Loss', 1.0563049),
 ('TV(2.0) Loss', 1114.788)], overall loss: -1074.6883544921875
Iteration: 148, named\_losses: [('ActivationMax Loss', -2207.6682),
 ('L-6.0 Norm Loss', 1.0573996),
 ('TV(2.0) Loss', 1125.7242)], overall loss: -1080.8865966796875
Iteration: 149, named\_losses: [('ActivationMax Loss', -2195.1514),
 ('L-6.0 Norm Loss', 1.0536177),
 ('TV(2.0) Loss', 1121.6539)], overall loss: -1072.4437255859375
Iteration: 150, named\_losses: [('ActivationMax Loss', -2212.753),
 ('L-6.0 Norm Loss', 1.0553186),
 ('TV(2.0) Loss', 1131.7505)], overall loss: -1079.947021484375
Iteration: 151, named\_losses: [('ActivationMax Loss', -2195.232),
 ('L-6.0 Norm Loss', 1.0586023),
 ('TV(2.0) Loss', 1121.447)], overall loss: -1072.726318359375
Iteration: 152, named\_losses: [('ActivationMax Loss', -2212.3196),
 ('L-6.0 Norm Loss', 1.0592332),
 ('TV(2.0) Loss', 1134.6332)], overall loss: -1076.6270751953125
Iteration: 153, named\_losses: [('ActivationMax Loss', -2198.8691),
 ('L-6.0 Norm Loss', 1.0566239),
 ('TV(2.0) Loss', 1123.2574)], overall loss: -1074.5550537109375
Iteration: 154, named\_losses: [('ActivationMax Loss', -2213.8494),
 ('L-6.0 Norm Loss', 1.0592623),
 ('TV(2.0) Loss', 1136.1002)], overall loss: -1076.6898193359375
Iteration: 155, named\_losses: [('ActivationMax Loss', -2208.9507),
 ('L-6.0 Norm Loss', 1.0598983),
 ('TV(2.0) Loss', 1127.5927)], overall loss: -1080.2982177734375
Iteration: 156, named\_losses: [('ActivationMax Loss', -2217.8572),
 ('L-6.0 Norm Loss', 1.0583237),
 ('TV(2.0) Loss', 1138.1958)], overall loss: -1078.60302734375
Iteration: 157, named\_losses: [('ActivationMax Loss', -2201.5955),
 ('L-6.0 Norm Loss', 1.059688),
 ('TV(2.0) Loss', 1126.6893)], overall loss: -1073.8465576171875
Iteration: 158, named\_losses: [('ActivationMax Loss', -2215.8396),
 ('L-6.0 Norm Loss', 1.0576895),
 ('TV(2.0) Loss', 1137.5247)], overall loss: -1077.25732421875
Iteration: 159, named\_losses: [('ActivationMax Loss', -2197.5134),
 ('L-6.0 Norm Loss', 1.0589843),
 ('TV(2.0) Loss', 1126.141)], overall loss: -1070.3133544921875
Iteration: 160, named\_losses: [('ActivationMax Loss', -2215.4849),
 ('L-6.0 Norm Loss', 1.0601723),
 ('TV(2.0) Loss', 1135.0469)], overall loss: -1079.3779296875
Iteration: 161, named\_losses: [('ActivationMax Loss', -2205.0876),
 ('L-6.0 Norm Loss', 1.0608075),
 ('TV(2.0) Loss', 1129.5106)], overall loss: -1074.5162353515625
Iteration: 162, named\_losses: [('ActivationMax Loss', -2213.5364),
 ('L-6.0 Norm Loss', 1.060618),
 ('TV(2.0) Loss', 1128.5942)], overall loss: -1083.881591796875
Iteration: 163, named\_losses: [('ActivationMax Loss', -2207.3076),
 ('L-6.0 Norm Loss', 1.0617199),
 ('TV(2.0) Loss', 1133.3683)], overall loss: -1072.8775634765625
Iteration: 164, named\_losses: [('ActivationMax Loss', -2211.3096),
 ('L-6.0 Norm Loss', 1.0617638),
 ('TV(2.0) Loss', 1137.113)], overall loss: -1073.134765625
Iteration: 165, named\_losses: [('ActivationMax Loss', -2206.4465),
 ('L-6.0 Norm Loss', 1.0625437),
 ('TV(2.0) Loss', 1132.766)], overall loss: -1072.6180419921875
Iteration: 166, named\_losses: [('ActivationMax Loss', -2214.7488),
 ('L-6.0 Norm Loss', 1.0609897),
 ('TV(2.0) Loss', 1133.6835)], overall loss: -1080.0042724609375
Iteration: 167, named\_losses: [('ActivationMax Loss', -2208.73),
 ('L-6.0 Norm Loss', 1.0635848),
 ('TV(2.0) Loss', 1133.965)], overall loss: -1073.7015380859375
Iteration: 168, named\_losses: [('ActivationMax Loss', -2210.741),
 ('L-6.0 Norm Loss', 1.06043),
 ('TV(2.0) Loss', 1132.5684)], overall loss: -1077.112060546875
Iteration: 169, named\_losses: [('ActivationMax Loss', -2208.986),
 ('L-6.0 Norm Loss', 1.059803),
 ('TV(2.0) Loss', 1130.4623)], overall loss: -1077.4639892578125
Iteration: 170, named\_losses: [('ActivationMax Loss', -2218.9937),
 ('L-6.0 Norm Loss', 1.0648766),
 ('TV(2.0) Loss', 1137.126)], overall loss: -1080.802734375
Iteration: 171, named\_losses: [('ActivationMax Loss', -2212.9038),
 ('L-6.0 Norm Loss', 1.0632771),
 ('TV(2.0) Loss', 1133.5859)], overall loss: -1078.254638671875
Iteration: 172, named\_losses: [('ActivationMax Loss', -2218.0837),
 ('L-6.0 Norm Loss', 1.0672485),
 ('TV(2.0) Loss', 1138.3688)], overall loss: -1078.6478271484375
Iteration: 173, named\_losses: [('ActivationMax Loss', -2216.2197),
 ('L-6.0 Norm Loss', 1.0622809),
 ('TV(2.0) Loss', 1136.1626)], overall loss: -1078.994873046875
Iteration: 174, named\_losses: [('ActivationMax Loss', -2216.9062),
 ('L-6.0 Norm Loss', 1.0624301),
 ('TV(2.0) Loss', 1137.4329)], overall loss: -1078.410888671875
Iteration: 175, named\_losses: [('ActivationMax Loss', -2216.2014),
 ('L-6.0 Norm Loss', 1.0646658),
 ('TV(2.0) Loss', 1136.7869)], overall loss: -1078.349853515625
Iteration: 176, named\_losses: [('ActivationMax Loss', -2214.336),
 ('L-6.0 Norm Loss', 1.0663441),
 ('TV(2.0) Loss', 1136.8247)], overall loss: -1076.44482421875
Iteration: 177, named\_losses: [('ActivationMax Loss', -2212.9006),
 ('L-6.0 Norm Loss', 1.0616423),
 ('TV(2.0) Loss', 1136.4061)], overall loss: -1075.4329833984375
Iteration: 178, named\_losses: [('ActivationMax Loss', -2216.4854),
 ('L-6.0 Norm Loss', 1.0679991),
 ('TV(2.0) Loss', 1136.98)], overall loss: -1078.437255859375
Iteration: 179, named\_losses: [('ActivationMax Loss', -2210.3706),
 ('L-6.0 Norm Loss', 1.0670958),
 ('TV(2.0) Loss', 1133.1937)], overall loss: -1076.1097412109375
Iteration: 180, named\_losses: [('ActivationMax Loss', -2221.2744),
 ('L-6.0 Norm Loss', 1.0678009),
 ('TV(2.0) Loss', 1140.0149)], overall loss: -1080.191650390625
Iteration: 181, named\_losses: [('ActivationMax Loss', -2209.9202),
 ('L-6.0 Norm Loss', 1.0669204),
 ('TV(2.0) Loss', 1134.5911)], overall loss: -1074.26220703125
Iteration: 182, named\_losses: [('ActivationMax Loss', -2224.8162),
 ('L-6.0 Norm Loss', 1.0674566),
 ('TV(2.0) Loss', 1146.69)], overall loss: -1077.058837890625
Iteration: 183, named\_losses: [('ActivationMax Loss', -2201.1853),
 ('L-6.0 Norm Loss', 1.0660845),
 ('TV(2.0) Loss', 1127.9878)], overall loss: -1072.13134765625
Iteration: 184, named\_losses: [('ActivationMax Loss', -2222.5828),
 ('L-6.0 Norm Loss', 1.0673684),
 ('TV(2.0) Loss', 1143.1108)], overall loss: -1078.404541015625
Iteration: 185, named\_losses: [('ActivationMax Loss', -2214.0771),
 ('L-6.0 Norm Loss', 1.0703062),
 ('TV(2.0) Loss', 1137.8978)], overall loss: -1075.1090087890625
Iteration: 186, named\_losses: [('ActivationMax Loss', -2215.8174),
 ('L-6.0 Norm Loss', 1.0633198),
 ('TV(2.0) Loss', 1138.7627)], overall loss: -1075.991455078125
Iteration: 187, named\_losses: [('ActivationMax Loss', -2210.2803),
 ('L-6.0 Norm Loss', 1.0680033),
 ('TV(2.0) Loss', 1139.7782)], overall loss: -1069.4339599609375
Iteration: 188, named\_losses: [('ActivationMax Loss', -2223.1213),
 ('L-6.0 Norm Loss', 1.0644947),
 ('TV(2.0) Loss', 1141.4945)], overall loss: -1080.5623779296875
Iteration: 189, named\_losses: [('ActivationMax Loss', -2210.2463),
 ('L-6.0 Norm Loss', 1.0675707),
 ('TV(2.0) Loss', 1131.9135)], overall loss: -1077.2652587890625
Iteration: 190, named\_losses: [('ActivationMax Loss', -2219.041),
 ('L-6.0 Norm Loss', 1.0673287),
 ('TV(2.0) Loss', 1141.9484)], overall loss: -1076.0252685546875
Iteration: 191, named\_losses: [('ActivationMax Loss', -2214.0908),
 ('L-6.0 Norm Loss', 1.070812),
 ('TV(2.0) Loss', 1134.1218)], overall loss: -1078.898193359375
Iteration: 192, named\_losses: [('ActivationMax Loss', -2220.8618),
 ('L-6.0 Norm Loss', 1.0658163),
 ('TV(2.0) Loss', 1145.0101)], overall loss: -1074.7857666015625
Iteration: 193, named\_losses: [('ActivationMax Loss', -2218.3174),
 ('L-6.0 Norm Loss', 1.0676898),
 ('TV(2.0) Loss', 1138.4238)], overall loss: -1078.825927734375
Iteration: 194, named\_losses: [('ActivationMax Loss', -2216.4258),
 ('L-6.0 Norm Loss', 1.0663385),
 ('TV(2.0) Loss', 1138.4054)], overall loss: -1076.9539794921875
Iteration: 195, named\_losses: [('ActivationMax Loss', -2212.7693),
 ('L-6.0 Norm Loss', 1.0698712),
 ('TV(2.0) Loss', 1136.16)], overall loss: -1075.5394287109375
Iteration: 196, named\_losses: [('ActivationMax Loss', -2217.3484),
 ('L-6.0 Norm Loss', 1.069964),
 ('TV(2.0) Loss', 1142.7838)], overall loss: -1073.4945068359375
Iteration: 197, named\_losses: [('ActivationMax Loss', -2223.8694),
 ('L-6.0 Norm Loss', 1.069033),
 ('TV(2.0) Loss', 1140.6919)], overall loss: -1082.1083984375
Iteration: 198, named\_losses: [('ActivationMax Loss', -2218.16),
 ('L-6.0 Norm Loss', 1.0659753),
 ('TV(2.0) Loss', 1148.1123)], overall loss: -1068.981689453125
Iteration: 199, named\_losses: [('ActivationMax Loss', -2217.3926),
 ('L-6.0 Norm Loss', 1.068889),
 ('TV(2.0) Loss', 1138.3402)], overall loss: -1077.9835205078125
Iteration: 200, named\_losses: [('ActivationMax Loss', -2220.816),
 ('L-6.0 Norm Loss', 1.0658938),
 ('TV(2.0) Loss', 1139.3492)], overall loss: -1080.4007568359375
Iteration: 201, named\_losses: [('ActivationMax Loss', -2212.1406),
 ('L-6.0 Norm Loss', 1.0668724),
 ('TV(2.0) Loss', 1134.9486)], overall loss: -1076.1251220703125
Iteration: 202, named\_losses: [('ActivationMax Loss', -2219.6638),
 ('L-6.0 Norm Loss', 1.0695256),
 ('TV(2.0) Loss', 1140.5717)], overall loss: -1078.0225830078125
Iteration: 203, named\_losses: [('ActivationMax Loss', -2210.5317),
 ('L-6.0 Norm Loss', 1.0694779),
 ('TV(2.0) Loss', 1135.5118)], overall loss: -1073.9503173828125
Iteration: 204, named\_losses: [('ActivationMax Loss', -2222.717),
 ('L-6.0 Norm Loss', 1.0690817),
 ('TV(2.0) Loss', 1143.5928)], overall loss: -1078.05517578125
Iteration: 205, named\_losses: [('ActivationMax Loss', -2204.1736),
 ('L-6.0 Norm Loss', 1.0644693),
 ('TV(2.0) Loss', 1131.1132)], overall loss: -1071.9959716796875
Iteration: 206, named\_losses: [('ActivationMax Loss', -2217.617),
 ('L-6.0 Norm Loss', 1.0688841),
 ('TV(2.0) Loss', 1139.7515)], overall loss: -1076.796630859375
Iteration: 207, named\_losses: [('ActivationMax Loss', -2210.6177),
 ('L-6.0 Norm Loss', 1.0703204),
 ('TV(2.0) Loss', 1135.7015)], overall loss: -1073.8458251953125
Iteration: 208, named\_losses: [('ActivationMax Loss', -2220.6956),
 ('L-6.0 Norm Loss', 1.0710266),
 ('TV(2.0) Loss', 1143.1168)], overall loss: -1076.5076904296875
Iteration: 209, named\_losses: [('ActivationMax Loss', -2211.6975),
 ('L-6.0 Norm Loss', 1.0655069),
 ('TV(2.0) Loss', 1139.7344)], overall loss: -1070.897705078125
Iteration: 210, named\_losses: [('ActivationMax Loss', -2219.1636),
 ('L-6.0 Norm Loss', 1.067456),
 ('TV(2.0) Loss', 1141.8156)], overall loss: -1076.2806396484375
Iteration: 211, named\_losses: [('ActivationMax Loss', -2211.5613),
 ('L-6.0 Norm Loss', 1.0706301),
 ('TV(2.0) Loss', 1132.8317)], overall loss: -1077.6590576171875
Iteration: 212, named\_losses: [('ActivationMax Loss', -2215.7974),
 ('L-6.0 Norm Loss', 1.0682224),
 ('TV(2.0) Loss', 1135.3479)], overall loss: -1079.38134765625
Iteration: 213, named\_losses: [('ActivationMax Loss', -2223.6929),
 ('L-6.0 Norm Loss', 1.0710202),
 ('TV(2.0) Loss', 1143.7622)], overall loss: -1078.859619140625
Iteration: 214, named\_losses: [('ActivationMax Loss', -2225.633),
 ('L-6.0 Norm Loss', 1.067015),
 ('TV(2.0) Loss', 1150.4272)], overall loss: -1074.138916015625
Iteration: 215, named\_losses: [('ActivationMax Loss', -2210.896),
 ('L-6.0 Norm Loss', 1.0718188),
 ('TV(2.0) Loss', 1137.7723)], overall loss: -1072.0518798828125
Iteration: 216, named\_losses: [('ActivationMax Loss', -2220.3374),
 ('L-6.0 Norm Loss', 1.0668505),
 ('TV(2.0) Loss', 1141.8386)], overall loss: -1077.431884765625
Iteration: 217, named\_losses: [('ActivationMax Loss', -2211.0295),
 ('L-6.0 Norm Loss', 1.0693806),
 ('TV(2.0) Loss', 1136.0062)], overall loss: -1073.9539794921875
Iteration: 218, named\_losses: [('ActivationMax Loss', -2219.879),
 ('L-6.0 Norm Loss', 1.0647163),
 ('TV(2.0) Loss', 1141.503)], overall loss: -1077.3111572265625
Iteration: 219, named\_losses: [('ActivationMax Loss', -2210.2861),
 ('L-6.0 Norm Loss', 1.0697006),
 ('TV(2.0) Loss', 1134.826)], overall loss: -1074.3905029296875
Iteration: 220, named\_losses: [('ActivationMax Loss', -2223.1313),
 ('L-6.0 Norm Loss', 1.0679162),
 ('TV(2.0) Loss', 1139.2267)], overall loss: -1082.8367919921875
Iteration: 221, named\_losses: [('ActivationMax Loss', -2210.9995),
 ('L-6.0 Norm Loss', 1.0701234),
 ('TV(2.0) Loss', 1136.4099)], overall loss: -1073.51953125
Iteration: 222, named\_losses: [('ActivationMax Loss', -2223.0854),
 ('L-6.0 Norm Loss', 1.0677587),
 ('TV(2.0) Loss', 1143.1871)], overall loss: -1078.8304443359375
Iteration: 223, named\_losses: [('ActivationMax Loss', -2212.3135),
 ('L-6.0 Norm Loss', 1.0719378),
 ('TV(2.0) Loss', 1135.5541)], overall loss: -1075.6873779296875
Iteration: 224, named\_losses: [('ActivationMax Loss', -2221.206),
 ('L-6.0 Norm Loss', 1.069114),
 ('TV(2.0) Loss', 1142.6486)], overall loss: -1077.4884033203125
Iteration: 225, named\_losses: [('ActivationMax Loss', -2210.4187),
 ('L-6.0 Norm Loss', 1.068477),
 ('TV(2.0) Loss', 1135.5342)], overall loss: -1073.816162109375
Iteration: 226, named\_losses: [('ActivationMax Loss', -2225.7515),
 ('L-6.0 Norm Loss', 1.0692221),
 ('TV(2.0) Loss', 1144.0712)], overall loss: -1080.6109619140625
Iteration: 227, named\_losses: [('ActivationMax Loss', -2216.1853),
 ('L-6.0 Norm Loss', 1.0724889),
 ('TV(2.0) Loss', 1136.078)], overall loss: -1079.0347900390625
Iteration: 228, named\_losses: [('ActivationMax Loss', -2225.896),
 ('L-6.0 Norm Loss', 1.0685508),
 ('TV(2.0) Loss', 1149.5277)], overall loss: -1075.2996826171875
Iteration: 229, named\_losses: [('ActivationMax Loss', -2214.5032),
 ('L-6.0 Norm Loss', 1.0683707),
 ('TV(2.0) Loss', 1135.201)], overall loss: -1078.2337646484375
Iteration: 230, named\_losses: [('ActivationMax Loss', -2223.419),
 ('L-6.0 Norm Loss', 1.0703778),
 ('TV(2.0) Loss', 1146.5973)], overall loss: -1075.7513427734375
Iteration: 231, named\_losses: [('ActivationMax Loss', -2211.4917),
 ('L-6.0 Norm Loss', 1.0714633),
 ('TV(2.0) Loss', 1134.9369)], overall loss: -1075.4832763671875
Iteration: 232, named\_losses: [('ActivationMax Loss', -2228.9683),
 ('L-6.0 Norm Loss', 1.0653485),
 ('TV(2.0) Loss', 1148.677)], overall loss: -1079.225830078125
Iteration: 233, named\_losses: [('ActivationMax Loss', -2217.6619),
 ('L-6.0 Norm Loss', 1.0687191),
 ('TV(2.0) Loss', 1137.602)], overall loss: -1078.9912109375
Iteration: 234, named\_losses: [('ActivationMax Loss', -2226.139),
 ('L-6.0 Norm Loss', 1.0688251),
 ('TV(2.0) Loss', 1146.2546)], overall loss: -1078.8154296875
Iteration: 235, named\_losses: [('ActivationMax Loss', -2216.528),
 ('L-6.0 Norm Loss', 1.0711704),
 ('TV(2.0) Loss', 1139.328)], overall loss: -1076.1287841796875
Iteration: 236, named\_losses: [('ActivationMax Loss', -2230.649),
 ('L-6.0 Norm Loss', 1.068604),
 ('TV(2.0) Loss', 1147.9723)], overall loss: -1081.6080322265625
Iteration: 237, named\_losses: [('ActivationMax Loss', -2211.82),
 ('L-6.0 Norm Loss', 1.0677623),
 ('TV(2.0) Loss', 1137.2312)], overall loss: -1073.52099609375
Iteration: 238, named\_losses: [('ActivationMax Loss', -2229.812),
 ('L-6.0 Norm Loss', 1.0662059),
 ('TV(2.0) Loss', 1150.8646)], overall loss: -1077.8812255859375
Iteration: 239, named\_losses: [('ActivationMax Loss', -2206.5813),
 ('L-6.0 Norm Loss', 1.0692549),
 ('TV(2.0) Loss', 1131.6476)], overall loss: -1073.8643798828125
Iteration: 240, named\_losses: [('ActivationMax Loss', -2223.6477),
 ('L-6.0 Norm Loss', 1.0672681),
 ('TV(2.0) Loss', 1144.5911)], overall loss: -1077.9892578125
Iteration: 241, named\_losses: [('ActivationMax Loss', -2215.3113),
 ('L-6.0 Norm Loss', 1.0677105),
 ('TV(2.0) Loss', 1139.9346)], overall loss: -1074.30908203125
Iteration: 242, named\_losses: [('ActivationMax Loss', -2220.5632),
 ('L-6.0 Norm Loss', 1.0676848),
 ('TV(2.0) Loss', 1145.1549)], overall loss: -1074.3406982421875
Iteration: 243, named\_losses: [('ActivationMax Loss', -2217.0945),
 ('L-6.0 Norm Loss', 1.0688134),
 ('TV(2.0) Loss', 1140.4442)], overall loss: -1075.5814208984375
Iteration: 244, named\_losses: [('ActivationMax Loss', -2220.6733),
 ('L-6.0 Norm Loss', 1.0691892),
 ('TV(2.0) Loss', 1147.5433)], overall loss: -1072.0609130859375
Iteration: 245, named\_losses: [('ActivationMax Loss', -2220.6504),
 ('L-6.0 Norm Loss', 1.0697591),
 ('TV(2.0) Loss', 1142.453)], overall loss: -1077.1275634765625
Iteration: 246, named\_losses: [('ActivationMax Loss', -2222.5754),
 ('L-6.0 Norm Loss', 1.0672952),
 ('TV(2.0) Loss', 1143.3716)], overall loss: -1078.136474609375
Iteration: 247, named\_losses: [('ActivationMax Loss', -2212.7158),
 ('L-6.0 Norm Loss', 1.0702599),
 ('TV(2.0) Loss', 1132.9675)], overall loss: -1078.677978515625
Iteration: 248, named\_losses: [('ActivationMax Loss', -2225.923),
 ('L-6.0 Norm Loss', 1.0724453),
 ('TV(2.0) Loss', 1149.576)], overall loss: -1075.2745361328125
Iteration: 249, named\_losses: [('ActivationMax Loss', -2217.887),
 ('L-6.0 Norm Loss', 1.0700067),
 ('TV(2.0) Loss', 1137.5002)], overall loss: -1079.316650390625
Iteration: 250, named\_losses: [('ActivationMax Loss', -2230.1543),
 ('L-6.0 Norm Loss', 1.0688695),
 ('TV(2.0) Loss', 1150.6257)], overall loss: -1078.459716796875
Iteration: 251, named\_losses: [('ActivationMax Loss', -2217.2495),
 ('L-6.0 Norm Loss', 1.0677301),
 ('TV(2.0) Loss', 1139.1581)], overall loss: -1077.0238037109375
Iteration: 252, named\_losses: [('ActivationMax Loss', -2227.9253),
 ('L-6.0 Norm Loss', 1.069999),
 ('TV(2.0) Loss', 1147.8699)], overall loss: -1078.9853515625
Iteration: 253, named\_losses: [('ActivationMax Loss', -2211.5198),
 ('L-6.0 Norm Loss', 1.071441),
 ('TV(2.0) Loss', 1137.4838)], overall loss: -1072.9644775390625
Iteration: 254, named\_losses: [('ActivationMax Loss', -2229.9934),
 ('L-6.0 Norm Loss', 1.0694948),
 ('TV(2.0) Loss', 1149.432)], overall loss: -1079.4918212890625
Iteration: 255, named\_losses: [('ActivationMax Loss', -2219.9495),
 ('L-6.0 Norm Loss', 1.0669308),
 ('TV(2.0) Loss', 1139.8074)], overall loss: -1079.0751953125
Iteration: 256, named\_losses: [('ActivationMax Loss', -2233.3367),
 ('L-6.0 Norm Loss', 1.0692912),
 ('TV(2.0) Loss', 1153.2512)], overall loss: -1079.01611328125
Iteration: 257, named\_losses: [('ActivationMax Loss', -2212.4492),
 ('L-6.0 Norm Loss', 1.0704396),
 ('TV(2.0) Loss', 1138.4086)], overall loss: -1072.9700927734375
Iteration: 258, named\_losses: [('ActivationMax Loss', -2225.0754),
 ('L-6.0 Norm Loss', 1.0679861),
 ('TV(2.0) Loss', 1145.2653)], overall loss: -1078.7423095703125
Iteration: 259, named\_losses: [('ActivationMax Loss', -2208.3389),
 ('L-6.0 Norm Loss', 1.0670954),
 ('TV(2.0) Loss', 1133.6088)], overall loss: -1073.6629638671875
Iteration: 260, named\_losses: [('ActivationMax Loss', -2230.1663),
 ('L-6.0 Norm Loss', 1.0676808),
 ('TV(2.0) Loss', 1149.9393)], overall loss: -1079.1593017578125
Iteration: 261, named\_losses: [('ActivationMax Loss', -2214.2258),
 ('L-6.0 Norm Loss', 1.0681254),
 ('TV(2.0) Loss', 1134.4552)], overall loss: -1078.7025146484375
Iteration: 262, named\_losses: [('ActivationMax Loss', -2221.5789),
 ('L-6.0 Norm Loss', 1.0693963),
 ('TV(2.0) Loss', 1146.6598)], overall loss: -1073.8497314453125
Iteration: 263, named\_losses: [('ActivationMax Loss', -2213.1807),
 ('L-6.0 Norm Loss', 1.0678902),
 ('TV(2.0) Loss', 1135.8132)], overall loss: -1076.299560546875
Iteration: 264, named\_losses: [('ActivationMax Loss', -2219.4192),
 ('L-6.0 Norm Loss', 1.0695654),
 ('TV(2.0) Loss', 1142.2867)], overall loss: -1076.0628662109375
Iteration: 265, named\_losses: [('ActivationMax Loss', -2209.179),
 ('L-6.0 Norm Loss', 1.0698609),
 ('TV(2.0) Loss', 1133.1289)], overall loss: -1074.980224609375
Iteration: 266, named\_losses: [('ActivationMax Loss', -2214.636),
 ('L-6.0 Norm Loss', 1.0677804),
 ('TV(2.0) Loss', 1140.076)], overall loss: -1073.4920654296875
Iteration: 267, named\_losses: [('ActivationMax Loss', -2215.0923),
 ('L-6.0 Norm Loss', 1.0674344),
 ('TV(2.0) Loss', 1137.6188)], overall loss: -1076.4061279296875
Iteration: 268, named\_losses: [('ActivationMax Loss', -2221.2195),
 ('L-6.0 Norm Loss', 1.0688828),
 ('TV(2.0) Loss', 1139.9062)], overall loss: -1080.244384765625
Iteration: 269, named\_losses: [('ActivationMax Loss', -2211.6707),
 ('L-6.0 Norm Loss', 1.068558),
 ('TV(2.0) Loss', 1131.3278)], overall loss: -1079.2742919921875
Iteration: 270, named\_losses: [('ActivationMax Loss', -2220.8557),
 ('L-6.0 Norm Loss', 1.0690988),
 ('TV(2.0) Loss', 1140.5865)], overall loss: -1079.2000732421875
Iteration: 271, named\_losses: [('ActivationMax Loss', -2206.9885),
 ('L-6.0 Norm Loss', 1.068122),
 ('TV(2.0) Loss', 1130.6432)], overall loss: -1075.2772216796875
Iteration: 272, named\_losses: [('ActivationMax Loss', -2227.0945),
 ('L-6.0 Norm Loss', 1.070838),
 ('TV(2.0) Loss', 1148.6996)], overall loss: -1077.3240966796875
Iteration: 273, named\_losses: [('ActivationMax Loss', -2211.6238),
 ('L-6.0 Norm Loss', 1.0674763),
 ('TV(2.0) Loss', 1134.2351)], overall loss: -1076.3212890625
Iteration: 274, named\_losses: [('ActivationMax Loss', -2222.8696),
 ('L-6.0 Norm Loss', 1.0691396),
 ('TV(2.0) Loss', 1149.7334)], overall loss: -1072.067138671875
Iteration: 275, named\_losses: [('ActivationMax Loss', -2212.8862),
 ('L-6.0 Norm Loss', 1.0724727),
 ('TV(2.0) Loss', 1131.8481)], overall loss: -1079.965576171875
Iteration: 276, named\_losses: [('ActivationMax Loss', -2223.7163),
 ('L-6.0 Norm Loss', 1.064938),
 ('TV(2.0) Loss', 1143.54)], overall loss: -1079.111328125
Iteration: 277, named\_losses: [('ActivationMax Loss', -2212.4573),
 ('L-6.0 Norm Loss', 1.0682064),
 ('TV(2.0) Loss', 1133.7013)], overall loss: -1077.6878662109375
Iteration: 278, named\_losses: [('ActivationMax Loss', -2220.7915),
 ('L-6.0 Norm Loss', 1.0708511),
 ('TV(2.0) Loss', 1143.7697)], overall loss: -1075.9510498046875
Iteration: 279, named\_losses: [('ActivationMax Loss', -2221.67),
 ('L-6.0 Norm Loss', 1.0699381),
 ('TV(2.0) Loss', 1140.8949)], overall loss: -1079.7052001953125
Iteration: 280, named\_losses: [('ActivationMax Loss', -2223.8762),
 ('L-6.0 Norm Loss', 1.0652871),
 ('TV(2.0) Loss', 1149.472)], overall loss: -1073.3389892578125
Iteration: 281, named\_losses: [('ActivationMax Loss', -2213.7708),
 ('L-6.0 Norm Loss', 1.0684645),
 ('TV(2.0) Loss', 1137.807)], overall loss: -1074.8953857421875
Iteration: 282, named\_losses: [('ActivationMax Loss', -2220.9636),
 ('L-6.0 Norm Loss', 1.0666615),
 ('TV(2.0) Loss', 1142.0989)], overall loss: -1077.798095703125
Iteration: 283, named\_losses: [('ActivationMax Loss', -2206.082),
 ('L-6.0 Norm Loss', 1.0658021),
 ('TV(2.0) Loss', 1133.6039)], overall loss: -1071.4122314453125
Iteration: 284, named\_losses: [('ActivationMax Loss', -2228.5776),
 ('L-6.0 Norm Loss', 1.0707105),
 ('TV(2.0) Loss', 1145.4369)], overall loss: -1082.0699462890625
Iteration: 285, named\_losses: [('ActivationMax Loss', -2216.6272),
 ('L-6.0 Norm Loss', 1.0701088),
 ('TV(2.0) Loss', 1142.9927)], overall loss: -1072.564453125
Iteration: 286, named\_losses: [('ActivationMax Loss', -2230.0496),
 ('L-6.0 Norm Loss', 1.0703616),
 ('TV(2.0) Loss', 1148.4027)], overall loss: -1080.5765380859375
Iteration: 287, named\_losses: [('ActivationMax Loss', -2208.1921),
 ('L-6.0 Norm Loss', 1.0703225),
 ('TV(2.0) Loss', 1133.6848)], overall loss: -1073.43701171875
Iteration: 288, named\_losses: [('ActivationMax Loss', -2224.5662),
 ('L-6.0 Norm Loss', 1.0681554),
 ('TV(2.0) Loss', 1147.3411)], overall loss: -1076.156982421875
Iteration: 289, named\_losses: [('ActivationMax Loss', -2222.954),
 ('L-6.0 Norm Loss', 1.0711205),
 ('TV(2.0) Loss', 1145.6338)], overall loss: -1076.249267578125
Iteration: 290, named\_losses: [('ActivationMax Loss', -2227.9216),
 ('L-6.0 Norm Loss', 1.0721291),
 ('TV(2.0) Loss', 1148.5002)], overall loss: -1078.349365234375
Iteration: 291, named\_losses: [('ActivationMax Loss', -2217.4033),
 ('L-6.0 Norm Loss', 1.0699792),
 ('TV(2.0) Loss', 1139.8468)], overall loss: -1076.4864501953125
Iteration: 292, named\_losses: [('ActivationMax Loss', -2224.8308),
 ('L-6.0 Norm Loss', 1.0694923),
 ('TV(2.0) Loss', 1143.7191)], overall loss: -1080.0421142578125
Iteration: 293, named\_losses: [('ActivationMax Loss', -2214.736),
 ('L-6.0 Norm Loss', 1.0708653),
 ('TV(2.0) Loss', 1140.7404)], overall loss: -1072.9249267578125
Iteration: 294, named\_losses: [('ActivationMax Loss', -2223.6133),
 ('L-6.0 Norm Loss', 1.066608),
 ('TV(2.0) Loss', 1142.4924)], overall loss: -1080.05419921875
Iteration: 295, named\_losses: [('ActivationMax Loss', -2216.9062),
 ('L-6.0 Norm Loss', 1.0694431),
 ('TV(2.0) Loss', 1140.9476)], overall loss: -1074.8892822265625
Iteration: 296, named\_losses: [('ActivationMax Loss', -2223.7278),
 ('L-6.0 Norm Loss', 1.0716522),
 ('TV(2.0) Loss', 1141.497)], overall loss: -1081.1593017578125
Iteration: 297, named\_losses: [('ActivationMax Loss', -2213.0327),
 ('L-6.0 Norm Loss', 1.0695183),
 ('TV(2.0) Loss', 1137.0823)], overall loss: -1074.880859375
Iteration: 298, named\_losses: [('ActivationMax Loss', -2227.3071),
 ('L-6.0 Norm Loss', 1.0701641),
 ('TV(2.0) Loss', 1144.6115)], overall loss: -1081.6256103515625
Iteration: 299, named\_losses: [('ActivationMax Loss', -2210.8245),
 ('L-6.0 Norm Loss', 1.0703547),
 ('TV(2.0) Loss', 1132.1633)], overall loss: -1077.5908203125
Iteration: 300, named\_losses: [('ActivationMax Loss', -2221.8274),
 ('L-6.0 Norm Loss', 1.0688038),
 ('TV(2.0) Loss', 1143.5043)], overall loss: -1077.2542724609375
Iteration: 301, named\_losses: [('ActivationMax Loss', -2222.0999),
 ('L-6.0 Norm Loss', 1.0682659),
 ('TV(2.0) Loss', 1141.1902)], overall loss: -1079.84130859375
Iteration: 302, named\_losses: [('ActivationMax Loss', -2224.4514),
 ('L-6.0 Norm Loss', 1.0682715),
 ('TV(2.0) Loss', 1148.0886)], overall loss: -1075.29443359375
Iteration: 303, named\_losses: [('ActivationMax Loss', -2221.3018),
 ('L-6.0 Norm Loss', 1.0704014),
 ('TV(2.0) Loss', 1143.3011)], overall loss: -1076.9302978515625
Iteration: 304, named\_losses: [('ActivationMax Loss', -2219.4263),
 ('L-6.0 Norm Loss', 1.0670465),
 ('TV(2.0) Loss', 1138.176)], overall loss: -1080.18310546875
Iteration: 305, named\_losses: [('ActivationMax Loss', -2212.4133),
 ('L-6.0 Norm Loss', 1.071585),
 ('TV(2.0) Loss', 1135.3143)], overall loss: -1076.0274658203125
Iteration: 306, named\_losses: [('ActivationMax Loss', -2222.3098),
 ('L-6.0 Norm Loss', 1.0720104),
 ('TV(2.0) Loss', 1141.778)], overall loss: -1079.4598388671875
Iteration: 307, named\_losses: [('ActivationMax Loss', -2202.9133),
 ('L-6.0 Norm Loss', 1.0697917),
 ('TV(2.0) Loss', 1124.2941)], overall loss: -1077.5494384765625
Iteration: 308, named\_losses: [('ActivationMax Loss', -2221.359),
 ('L-6.0 Norm Loss', 1.0690776),
 ('TV(2.0) Loss', 1143.1271)], overall loss: -1077.1627197265625
Iteration: 309, named\_losses: [('ActivationMax Loss', -2211.5176),
 ('L-6.0 Norm Loss', 1.0708724),
 ('TV(2.0) Loss', 1134.1859)], overall loss: -1076.2608642578125
Iteration: 310, named\_losses: [('ActivationMax Loss', -2226.3918),
 ('L-6.0 Norm Loss', 1.0716937),
 ('TV(2.0) Loss', 1147.5471)], overall loss: -1077.77294921875
Iteration: 311, named\_losses: [('ActivationMax Loss', -2209.733),
 ('L-6.0 Norm Loss', 1.0702659),
 ('TV(2.0) Loss', 1135.932)], overall loss: -1072.7305908203125
Iteration: 312, named\_losses: [('ActivationMax Loss', -2228.451),
 ('L-6.0 Norm Loss', 1.0728015),
 ('TV(2.0) Loss', 1149.2614)], overall loss: -1078.1168212890625
Iteration: 313, named\_losses: [('ActivationMax Loss', -2223.7341),
 ('L-6.0 Norm Loss', 1.0698755),
 ('TV(2.0) Loss', 1140.1815)], overall loss: -1082.4827880859375
Iteration: 314, named\_losses: [('ActivationMax Loss', -2224.6562),
 ('L-6.0 Norm Loss', 1.071598),
 ('TV(2.0) Loss', 1146.765)], overall loss: -1076.8197021484375
Iteration: 315, named\_losses: [('ActivationMax Loss', -2220.7417),
 ('L-6.0 Norm Loss', 1.0705546),
 ('TV(2.0) Loss', 1137.8395)], overall loss: -1081.8316650390625
Iteration: 316, named\_losses: [('ActivationMax Loss', -2227.4844),
 ('L-6.0 Norm Loss', 1.0675604),
 ('TV(2.0) Loss', 1149.0657)], overall loss: -1077.35107421875
Iteration: 317, named\_losses: [('ActivationMax Loss', -2218.8113),
 ('L-6.0 Norm Loss', 1.070179),
 ('TV(2.0) Loss', 1137.74)], overall loss: -1080.001220703125
Iteration: 318, named\_losses: [('ActivationMax Loss', -2227.9639),
 ('L-6.0 Norm Loss', 1.0751518),
 ('TV(2.0) Loss', 1145.9714)], overall loss: -1080.917236328125
Iteration: 319, named\_losses: [('ActivationMax Loss', -2225.1877),
 ('L-6.0 Norm Loss', 1.0702394),
 ('TV(2.0) Loss', 1144.9056)], overall loss: -1079.2117919921875
Iteration: 320, named\_losses: [('ActivationMax Loss', -2227.8381),
 ('L-6.0 Norm Loss', 1.0688683),
 ('TV(2.0) Loss', 1146.0931)], overall loss: -1080.6761474609375
Iteration: 321, named\_losses: [('ActivationMax Loss', -2219.9182),
 ('L-6.0 Norm Loss', 1.0688087),
 ('TV(2.0) Loss', 1136.1196)], overall loss: -1082.729736328125
Iteration: 322, named\_losses: [('ActivationMax Loss', -2228.5332),
 ('L-6.0 Norm Loss', 1.0666585),
 ('TV(2.0) Loss', 1148.4867)], overall loss: -1078.9798583984375
Iteration: 323, named\_losses: [('ActivationMax Loss', -2232.2627),
 ('L-6.0 Norm Loss', 1.0726833),
 ('TV(2.0) Loss', 1148.5913)], overall loss: -1082.5986328125
Iteration: 324, named\_losses: [('ActivationMax Loss', -2227.736),
 ('L-6.0 Norm Loss', 1.0664213),
 ('TV(2.0) Loss', 1144.6366)], overall loss: -1082.0330810546875
Iteration: 325, named\_losses: [('ActivationMax Loss', -2226.3657),
 ('L-6.0 Norm Loss', 1.0693629),
 ('TV(2.0) Loss', 1142.6304)], overall loss: -1082.666015625
Iteration: 326, named\_losses: [('ActivationMax Loss', -2223.857),
 ('L-6.0 Norm Loss', 1.0709329),
 ('TV(2.0) Loss', 1139.8601)], overall loss: -1082.92578125
Iteration: 327, named\_losses: [('ActivationMax Loss', -2219.7146),
 ('L-6.0 Norm Loss', 1.067012),
 ('TV(2.0) Loss', 1140.2993)], overall loss: -1078.348388671875
Iteration: 328, named\_losses: [('ActivationMax Loss', -2226.3293),
 ('L-6.0 Norm Loss', 1.0698183),
 ('TV(2.0) Loss', 1145.8552)], overall loss: -1079.404296875
Iteration: 329, named\_losses: [('ActivationMax Loss', -2224.3499),
 ('L-6.0 Norm Loss', 1.0727707),
 ('TV(2.0) Loss', 1147.3169)], overall loss: -1075.960205078125
Iteration: 330, named\_losses: [('ActivationMax Loss', -2225.3699),
 ('L-6.0 Norm Loss', 1.0663509),
 ('TV(2.0) Loss', 1146.9702)], overall loss: -1077.333251953125
Iteration: 331, named\_losses: [('ActivationMax Loss', -2221.876),
 ('L-6.0 Norm Loss', 1.0714482),
 ('TV(2.0) Loss', 1141.362)], overall loss: -1079.4423828125
Iteration: 332, named\_losses: [('ActivationMax Loss', -2229.431),
 ('L-6.0 Norm Loss', 1.0707744),
 ('TV(2.0) Loss', 1151.2644)], overall loss: -1077.095703125
Iteration: 333, named\_losses: [('ActivationMax Loss', -2219.9878),
 ('L-6.0 Norm Loss', 1.0701796),
 ('TV(2.0) Loss', 1139.6129)], overall loss: -1079.3048095703125
Iteration: 334, named\_losses: [('ActivationMax Loss', -2226.7527),
 ('L-6.0 Norm Loss', 1.0708959),
 ('TV(2.0) Loss', 1146.3112)], overall loss: -1079.3707275390625
Iteration: 335, named\_losses: [('ActivationMax Loss', -2219.4336),
 ('L-6.0 Norm Loss', 1.0695702),
 ('TV(2.0) Loss', 1135.8428)], overall loss: -1082.521240234375
Iteration: 336, named\_losses: [('ActivationMax Loss', -2227.6257),
 ('L-6.0 Norm Loss', 1.0729705),
 ('TV(2.0) Loss', 1146.7219)], overall loss: -1079.830810546875
Iteration: 337, named\_losses: [('ActivationMax Loss', -2221.1816),
 ('L-6.0 Norm Loss', 1.0703337),
 ('TV(2.0) Loss', 1140.9939)], overall loss: -1079.117431640625
Iteration: 338, named\_losses: [('ActivationMax Loss', -2233.116),
 ('L-6.0 Norm Loss', 1.0699345),
 ('TV(2.0) Loss', 1150.3815)], overall loss: -1081.6646728515625
Iteration: 339, named\_losses: [('ActivationMax Loss', -2220.9666),
 ('L-6.0 Norm Loss', 1.0735099),
 ('TV(2.0) Loss', 1140.9263)], overall loss: -1078.966796875
Iteration: 340, named\_losses: [('ActivationMax Loss', -2229.897),
 ('L-6.0 Norm Loss', 1.0700167),
 ('TV(2.0) Loss', 1148.0337)], overall loss: -1080.793212890625
Iteration: 341, named\_losses: [('ActivationMax Loss', -2220.6191),
 ('L-6.0 Norm Loss', 1.0723099),
 ('TV(2.0) Loss', 1139.9281)], overall loss: -1079.6187744140625
Iteration: 342, named\_losses: [('ActivationMax Loss', -2230.4988),
 ('L-6.0 Norm Loss', 1.0704751),
 ('TV(2.0) Loss', 1151.1018)], overall loss: -1078.326416015625
Iteration: 343, named\_losses: [('ActivationMax Loss', -2222.7952),
 ('L-6.0 Norm Loss', 1.0725307),
 ('TV(2.0) Loss', 1137.5773)], overall loss: -1084.1453857421875
Iteration: 344, named\_losses: [('ActivationMax Loss', -2229.04),
 ('L-6.0 Norm Loss', 1.067829),
 ('TV(2.0) Loss', 1143.6827)], overall loss: -1084.2894287109375
Iteration: 345, named\_losses: [('ActivationMax Loss', -2219.2144),
 ('L-6.0 Norm Loss', 1.0716202),
 ('TV(2.0) Loss', 1142.678)], overall loss: -1075.46484375
Iteration: 346, named\_losses: [('ActivationMax Loss', -2236.3965),
 ('L-6.0 Norm Loss', 1.0695457),
 ('TV(2.0) Loss', 1149.3739)], overall loss: -1085.9530029296875
Iteration: 347, named\_losses: [('ActivationMax Loss', -2230.9614),
 ('L-6.0 Norm Loss', 1.0738435),
 ('TV(2.0) Loss', 1146.1727)], overall loss: -1083.7149658203125
Iteration: 348, named\_losses: [('ActivationMax Loss', -2228.7131),
 ('L-6.0 Norm Loss', 1.0691221),
 ('TV(2.0) Loss', 1150.4694)], overall loss: -1077.1746826171875
Iteration: 349, named\_losses: [('ActivationMax Loss', -2226.3777),
 ('L-6.0 Norm Loss', 1.0733091),
 ('TV(2.0) Loss', 1144.3142)], overall loss: -1080.990234375
Iteration: 350, named\_losses: [('ActivationMax Loss', -2225.7866),
 ('L-6.0 Norm Loss', 1.0697532),
 ('TV(2.0) Loss', 1146.506)], overall loss: -1078.2108154296875
Iteration: 351, named\_losses: [('ActivationMax Loss', -2220.2664),
 ('L-6.0 Norm Loss', 1.0720801),
 ('TV(2.0) Loss', 1141.6135)], overall loss: -1077.580810546875
Iteration: 352, named\_losses: [('ActivationMax Loss', -2227.5005),
 ('L-6.0 Norm Loss', 1.0711288),
 ('TV(2.0) Loss', 1145.4753)], overall loss: -1080.9541015625
Iteration: 353, named\_losses: [('ActivationMax Loss', -2225.9426),
 ('L-6.0 Norm Loss', 1.0710819),
 ('TV(2.0) Loss', 1144.4523)], overall loss: -1080.4193115234375
Iteration: 354, named\_losses: [('ActivationMax Loss', -2230.9924),
 ('L-6.0 Norm Loss', 1.0698309),
 ('TV(2.0) Loss', 1147.4733)], overall loss: -1082.4493408203125
Iteration: 355, named\_losses: [('ActivationMax Loss', -2224.3992),
 ('L-6.0 Norm Loss', 1.0715375),
 ('TV(2.0) Loss', 1143.159)], overall loss: -1080.1685791015625
Iteration: 356, named\_losses: [('ActivationMax Loss', -2226.7693),
 ('L-6.0 Norm Loss', 1.0710906),
 ('TV(2.0) Loss', 1143.3412)], overall loss: -1082.3570556640625
Iteration: 357, named\_losses: [('ActivationMax Loss', -2226.5378),
 ('L-6.0 Norm Loss', 1.0714155),
 ('TV(2.0) Loss', 1146.6354)], overall loss: -1078.8309326171875
Iteration: 358, named\_losses: [('ActivationMax Loss', -2226.8762),
 ('L-6.0 Norm Loss', 1.0681161),
 ('TV(2.0) Loss', 1139.6971)], overall loss: -1086.1109619140625
Iteration: 359, named\_losses: [('ActivationMax Loss', -2216.7556),
 ('L-6.0 Norm Loss', 1.0704328),
 ('TV(2.0) Loss', 1138.6799)], overall loss: -1077.00537109375
Iteration: 360, named\_losses: [('ActivationMax Loss', -2232.6318),
 ('L-6.0 Norm Loss', 1.0690626),
 ('TV(2.0) Loss', 1142.6586)], overall loss: -1088.9041748046875
Iteration: 361, named\_losses: [('ActivationMax Loss', -2223.7039),
 ('L-6.0 Norm Loss', 1.0676056),
 ('TV(2.0) Loss', 1141.8916)], overall loss: -1080.74462890625
Iteration: 362, named\_losses: [('ActivationMax Loss', -2234.7979),
 ('L-6.0 Norm Loss', 1.0695409),
 ('TV(2.0) Loss', 1150.3846)], overall loss: -1083.3436279296875
Iteration: 363, named\_losses: [('ActivationMax Loss', -2223.9338),
 ('L-6.0 Norm Loss', 1.0709136),
 ('TV(2.0) Loss', 1143.7117)], overall loss: -1079.1513671875
Iteration: 364, named\_losses: [('ActivationMax Loss', -2234.178),
 ('L-6.0 Norm Loss', 1.0678378),
 ('TV(2.0) Loss', 1151.3926)], overall loss: -1081.717529296875
Iteration: 365, named\_losses: [('ActivationMax Loss', -2216.663),
 ('L-6.0 Norm Loss', 1.0686363),
 ('TV(2.0) Loss', 1141.4055)], overall loss: -1074.18896484375
Iteration: 366, named\_losses: [('ActivationMax Loss', -2238.6445),
 ('L-6.0 Norm Loss', 1.070758),
 ('TV(2.0) Loss', 1153.1681)], overall loss: -1084.4056396484375
Iteration: 367, named\_losses: [('ActivationMax Loss', -2216.262),
 ('L-6.0 Norm Loss', 1.0738297),
 ('TV(2.0) Loss', 1141.0922)], overall loss: -1074.0960693359375
Iteration: 368, named\_losses: [('ActivationMax Loss', -2229.2214),
 ('L-6.0 Norm Loss', 1.0693816),
 ('TV(2.0) Loss', 1145.6708)], overall loss: -1082.4813232421875
Iteration: 369, named\_losses: [('ActivationMax Loss', -2214.8218),
 ('L-6.0 Norm Loss', 1.0713011),
 ('TV(2.0) Loss', 1139.0315)], overall loss: -1074.718994140625
Iteration: 370, named\_losses: [('ActivationMax Loss', -2227.4788),
 ('L-6.0 Norm Loss', 1.067562),
 ('TV(2.0) Loss', 1143.4246)], overall loss: -1082.986572265625
Iteration: 371, named\_losses: [('ActivationMax Loss', -2224.6423),
 ('L-6.0 Norm Loss', 1.069884),
 ('TV(2.0) Loss', 1142.9504)], overall loss: -1080.6220703125
Iteration: 372, named\_losses: [('ActivationMax Loss', -2230.3606),
 ('L-6.0 Norm Loss', 1.0652119),
 ('TV(2.0) Loss', 1150.662)], overall loss: -1078.6334228515625
Iteration: 373, named\_losses: [('ActivationMax Loss', -2221.9988),
 ('L-6.0 Norm Loss', 1.0695977),
 ('TV(2.0) Loss', 1141.1798)], overall loss: -1079.7493896484375
Iteration: 374, named\_losses: [('ActivationMax Loss', -2225.0547),
 ('L-6.0 Norm Loss', 1.0697302),
 ('TV(2.0) Loss', 1143.8384)], overall loss: -1080.146484375
Iteration: 375, named\_losses: [('ActivationMax Loss', -2218.275),
 ('L-6.0 Norm Loss', 1.0720671),
 ('TV(2.0) Loss', 1139.4432)], overall loss: -1077.7596435546875
Iteration: 376, named\_losses: [('ActivationMax Loss', -2229.9019),
 ('L-6.0 Norm Loss', 1.0705365),
 ('TV(2.0) Loss', 1146.9243)], overall loss: -1081.906982421875
Iteration: 377, named\_losses: [('ActivationMax Loss', -2215.8328),
 ('L-6.0 Norm Loss', 1.0699829),
 ('TV(2.0) Loss', 1137.3103)], overall loss: -1077.452392578125
Iteration: 378, named\_losses: [('ActivationMax Loss', -2230.629),
 ('L-6.0 Norm Loss', 1.0688547),
 ('TV(2.0) Loss', 1147.0281)], overall loss: -1082.531982421875
Iteration: 379, named\_losses: [('ActivationMax Loss', -2215.0261),
 ('L-6.0 Norm Loss', 1.0713302),
 ('TV(2.0) Loss', 1138.4177)], overall loss: -1075.537109375
Iteration: 380, named\_losses: [('ActivationMax Loss', -2226.799),
 ('L-6.0 Norm Loss', 1.0730578),
 ('TV(2.0) Loss', 1142.3435)], overall loss: -1083.382568359375
Iteration: 381, named\_losses: [('ActivationMax Loss', -2220.3704),
 ('L-6.0 Norm Loss', 1.071222),
 ('TV(2.0) Loss', 1144.3064)], overall loss: -1074.99267578125
Iteration: 382, named\_losses: [('ActivationMax Loss', -2232.6748),
 ('L-6.0 Norm Loss', 1.0714543),
 ('TV(2.0) Loss', 1147.2384)], overall loss: -1084.3648681640625
Iteration: 383, named\_losses: [('ActivationMax Loss', -2225.7),
 ('L-6.0 Norm Loss', 1.0715263),
 ('TV(2.0) Loss', 1146.424)], overall loss: -1078.2044677734375
Iteration: 384, named\_losses: [('ActivationMax Loss', -2227.7478),
 ('L-6.0 Norm Loss', 1.0719185),
 ('TV(2.0) Loss', 1142.1824)], overall loss: -1084.493408203125
Iteration: 385, named\_losses: [('ActivationMax Loss', -2222.5847),
 ('L-6.0 Norm Loss', 1.0697095),
 ('TV(2.0) Loss', 1141.2343)], overall loss: -1080.2806396484375
Iteration: 386, named\_losses: [('ActivationMax Loss', -2231.133),
 ('L-6.0 Norm Loss', 1.0676821),
 ('TV(2.0) Loss', 1146.3713)], overall loss: -1083.694091796875
Iteration: 387, named\_losses: [('ActivationMax Loss', -2215.0413),
 ('L-6.0 Norm Loss', 1.0706545),
 ('TV(2.0) Loss', 1138.8911)], overall loss: -1075.07958984375
Iteration: 388, named\_losses: [('ActivationMax Loss', -2235.2876),
 ('L-6.0 Norm Loss', 1.0693521),
 ('TV(2.0) Loss', 1153.2754)], overall loss: -1080.94287109375
Iteration: 389, named\_losses: [('ActivationMax Loss', -2218.0366),
 ('L-6.0 Norm Loss', 1.0691152),
 ('TV(2.0) Loss', 1137.7687)], overall loss: -1079.1988525390625
Iteration: 390, named\_losses: [('ActivationMax Loss', -2233.1611),
 ('L-6.0 Norm Loss', 1.0729673),
 ('TV(2.0) Loss', 1150.7334)], overall loss: -1081.354736328125
Iteration: 391, named\_losses: [('ActivationMax Loss', -2225.4263),
 ('L-6.0 Norm Loss', 1.0699403),
 ('TV(2.0) Loss', 1142.9293)], overall loss: -1081.4271240234375
Iteration: 392, named\_losses: [('ActivationMax Loss', -2227.0852),
 ('L-6.0 Norm Loss', 1.0705498),
 ('TV(2.0) Loss', 1143.3339)], overall loss: -1082.6807861328125
Iteration: 393, named\_losses: [('ActivationMax Loss', -2223.8787),
 ('L-6.0 Norm Loss', 1.0691488),
 ('TV(2.0) Loss', 1141.612)], overall loss: -1081.197509765625
Iteration: 394, named\_losses: [('ActivationMax Loss', -2230.4753),
 ('L-6.0 Norm Loss', 1.0685493),
 ('TV(2.0) Loss', 1147.8757)], overall loss: -1081.531005859375
Iteration: 395, named\_losses: [('ActivationMax Loss', -2224.4736),
 ('L-6.0 Norm Loss', 1.0708581),
 ('TV(2.0) Loss', 1144.3949)], overall loss: -1079.0079345703125
Iteration: 396, named\_losses: [('ActivationMax Loss', -2223.1401),
 ('L-6.0 Norm Loss', 1.071313),
 ('TV(2.0) Loss', 1141.5625)], overall loss: -1080.50634765625
Iteration: 397, named\_losses: [('ActivationMax Loss', -2217.77),
 ('L-6.0 Norm Loss', 1.0696902),
 ('TV(2.0) Loss', 1144.5137)], overall loss: -1072.186767578125
Iteration: 398, named\_losses: [('ActivationMax Loss', -2223.7236),
 ('L-6.0 Norm Loss', 1.0697925),
 ('TV(2.0) Loss', 1142.3191)], overall loss: -1080.334716796875
Iteration: 399, named\_losses: [('ActivationMax Loss', -2225.159),
 ('L-6.0 Norm Loss', 1.072106),
 ('TV(2.0) Loss', 1147.6283)], overall loss: -1076.4586181640625
Iteration: 400, named\_losses: [('ActivationMax Loss', -2228.5422),
 ('L-6.0 Norm Loss', 1.0737258),
 ('TV(2.0) Loss', 1145.0844)], overall loss: -1082.3841552734375
Iteration: 401, named\_losses: [('ActivationMax Loss', -2226.547),
 ('L-6.0 Norm Loss', 1.0705583),
 ('TV(2.0) Loss', 1144.1335)], overall loss: -1081.343017578125
Iteration: 402, named\_losses: [('ActivationMax Loss', -2229.9783),
 ('L-6.0 Norm Loss', 1.0740055),
 ('TV(2.0) Loss', 1147.5825)], overall loss: -1081.32177734375
Iteration: 403, named\_losses: [('ActivationMax Loss', -2223.1008),
 ('L-6.0 Norm Loss', 1.072607),
 ('TV(2.0) Loss', 1148.1754)], overall loss: -1073.8529052734375
Iteration: 404, named\_losses: [('ActivationMax Loss', -2230.245),
 ('L-6.0 Norm Loss', 1.0713866),
 ('TV(2.0) Loss', 1148.4429)], overall loss: -1080.73095703125
Iteration: 405, named\_losses: [('ActivationMax Loss', -2216.5745),
 ('L-6.0 Norm Loss', 1.0707264),
 ('TV(2.0) Loss', 1143.1947)], overall loss: -1072.3089599609375
Iteration: 406, named\_losses: [('ActivationMax Loss', -2231.807),
 ('L-6.0 Norm Loss', 1.0709524),
 ('TV(2.0) Loss', 1146.6237)], overall loss: -1084.1121826171875
Iteration: 407, named\_losses: [('ActivationMax Loss', -2220.6824),
 ('L-6.0 Norm Loss', 1.0722017),
 ('TV(2.0) Loss', 1144.631)], overall loss: -1074.9791259765625
Iteration: 408, named\_losses: [('ActivationMax Loss', -2231.5671),
 ('L-6.0 Norm Loss', 1.0693734),
 ('TV(2.0) Loss', 1144.5005)], overall loss: -1085.997314453125
Iteration: 409, named\_losses: [('ActivationMax Loss', -2224.9531),
 ('L-6.0 Norm Loss', 1.0737127),
 ('TV(2.0) Loss', 1149.9489)], overall loss: -1073.9305419921875
Iteration: 410, named\_losses: [('ActivationMax Loss', -2230.2998),
 ('L-6.0 Norm Loss', 1.0710733),
 ('TV(2.0) Loss', 1150.0835)], overall loss: -1079.145263671875
Iteration: 411, named\_losses: [('ActivationMax Loss', -2227.7393),
 ('L-6.0 Norm Loss', 1.0727637),
 ('TV(2.0) Loss', 1144.7102)], overall loss: -1081.956298828125
Iteration: 412, named\_losses: [('ActivationMax Loss', -2233.292),
 ('L-6.0 Norm Loss', 1.0699636),
 ('TV(2.0) Loss', 1153.757)], overall loss: -1078.4649658203125
Iteration: 413, named\_losses: [('ActivationMax Loss', -2221.7725),
 ('L-6.0 Norm Loss', 1.0706537),
 ('TV(2.0) Loss', 1138.322)], overall loss: -1082.3798828125
Iteration: 414, named\_losses: [('ActivationMax Loss', -2232.7456),
 ('L-6.0 Norm Loss', 1.0678574),
 ('TV(2.0) Loss', 1152.4458)], overall loss: -1079.23193359375
Iteration: 415, named\_losses: [('ActivationMax Loss', -2222.95),
 ('L-6.0 Norm Loss', 1.0685068),
 ('TV(2.0) Loss', 1141.3204)], overall loss: -1080.5609130859375
Iteration: 416, named\_losses: [('ActivationMax Loss', -2226.7583),
 ('L-6.0 Norm Loss', 1.0678746),
 ('TV(2.0) Loss', 1147.6268)], overall loss: -1078.0635986328125
Iteration: 417, named\_losses: [('ActivationMax Loss', -2227.7075),
 ('L-6.0 Norm Loss', 1.0699638),
 ('TV(2.0) Loss', 1143.338)], overall loss: -1083.2994384765625
Iteration: 418, named\_losses: [('ActivationMax Loss', -2224.2588),
 ('L-6.0 Norm Loss', 1.0649029),
 ('TV(2.0) Loss', 1146.4739)], overall loss: -1076.719970703125
Iteration: 419, named\_losses: [('ActivationMax Loss', -2229.0664),
 ('L-6.0 Norm Loss', 1.0699455),
 ('TV(2.0) Loss', 1141.1417)], overall loss: -1086.8548583984375
Iteration: 420, named\_losses: [('ActivationMax Loss', -2229.2246),
 ('L-6.0 Norm Loss', 1.0683689),
 ('TV(2.0) Loss', 1146.8583)], overall loss: -1081.2979736328125
Iteration: 421, named\_losses: [('ActivationMax Loss', -2216.3225),
 ('L-6.0 Norm Loss', 1.0689929),
 ('TV(2.0) Loss', 1135.1234)], overall loss: -1080.1300048828125
Iteration: 422, named\_losses: [('ActivationMax Loss', -2229.262),
 ('L-6.0 Norm Loss', 1.0658777),
 ('TV(2.0) Loss', 1150.0658)], overall loss: -1078.1302490234375
Iteration: 423, named\_losses: [('ActivationMax Loss', -2221.0757),
 ('L-6.0 Norm Loss', 1.0678734),
 ('TV(2.0) Loss', 1137.7482)], overall loss: -1082.2596435546875
Iteration: 424, named\_losses: [('ActivationMax Loss', -2238.7007),
 ('L-6.0 Norm Loss', 1.070707),
 ('TV(2.0) Loss', 1154.0432)], overall loss: -1083.586669921875
Iteration: 425, named\_losses: [('ActivationMax Loss', -2221.7844),
 ('L-6.0 Norm Loss', 1.0722615),
 ('TV(2.0) Loss', 1138.1919)], overall loss: -1082.520263671875
Iteration: 426, named\_losses: [('ActivationMax Loss', -2233.8367),
 ('L-6.0 Norm Loss', 1.0669957),
 ('TV(2.0) Loss', 1152.9006)], overall loss: -1079.869140625
Iteration: 427, named\_losses: [('ActivationMax Loss', -2215.0605),
 ('L-6.0 Norm Loss', 1.0701615),
 ('TV(2.0) Loss', 1134.8005)], overall loss: -1079.18994140625
Iteration: 428, named\_losses: [('ActivationMax Loss', -2233.825),
 ('L-6.0 Norm Loss', 1.0654416),
 ('TV(2.0) Loss', 1149.6217)], overall loss: -1083.1378173828125
Iteration: 429, named\_losses: [('ActivationMax Loss', -2211.9734),
 ('L-6.0 Norm Loss', 1.0704193),
 ('TV(2.0) Loss', 1137.3286)], overall loss: -1073.574462890625
Iteration: 430, named\_losses: [('ActivationMax Loss', -2225.3757),
 ('L-6.0 Norm Loss', 1.0705053),
 ('TV(2.0) Loss', 1142.1453)], overall loss: -1082.159912109375
Iteration: 431, named\_losses: [('ActivationMax Loss', -2224.7458),
 ('L-6.0 Norm Loss', 1.0713704),
 ('TV(2.0) Loss', 1147.5778)], overall loss: -1076.0968017578125
Iteration: 432, named\_losses: [('ActivationMax Loss', -2230.112),
 ('L-6.0 Norm Loss', 1.0714102),
 ('TV(2.0) Loss', 1146.0317)], overall loss: -1083.009033203125
Iteration: 433, named\_losses: [('ActivationMax Loss', -2222.344),
 ('L-6.0 Norm Loss', 1.0716697),
 ('TV(2.0) Loss', 1144.5842)], overall loss: -1076.68798828125
Iteration: 434, named\_losses: [('ActivationMax Loss', -2230.568),
 ('L-6.0 Norm Loss', 1.0688405),
 ('TV(2.0) Loss', 1142.8342)], overall loss: -1086.6650390625
Iteration: 435, named\_losses: [('ActivationMax Loss', -2221.9006),
 ('L-6.0 Norm Loss', 1.0709251),
 ('TV(2.0) Loss', 1141.6351)], overall loss: -1079.1944580078125
Iteration: 436, named\_losses: [('ActivationMax Loss', -2237.269),
 ('L-6.0 Norm Loss', 1.0733347),
 ('TV(2.0) Loss', 1154.1597)], overall loss: -1082.0361328125
Iteration: 437, named\_losses: [('ActivationMax Loss', -2219.4856),
 ('L-6.0 Norm Loss', 1.0701149),
 ('TV(2.0) Loss', 1141.3864)], overall loss: -1077.0291748046875
Iteration: 438, named\_losses: [('ActivationMax Loss', -2234.249),
 ('L-6.0 Norm Loss', 1.0699177),
 ('TV(2.0) Loss', 1153.5388)], overall loss: -1079.640380859375
Iteration: 439, named\_losses: [('ActivationMax Loss', -2219.858),
 ('L-6.0 Norm Loss', 1.0723609),
 ('TV(2.0) Loss', 1140.2451)], overall loss: -1078.54052734375
Iteration: 440, named\_losses: [('ActivationMax Loss', -2231.7876),
 ('L-6.0 Norm Loss', 1.069224),
 ('TV(2.0) Loss', 1148.4382)], overall loss: -1082.280029296875
Iteration: 441, named\_losses: [('ActivationMax Loss', -2213.9893),
 ('L-6.0 Norm Loss', 1.0726874),
 ('TV(2.0) Loss', 1138.477)], overall loss: -1074.439453125
Iteration: 442, named\_losses: [('ActivationMax Loss', -2221.3904),
 ('L-6.0 Norm Loss', 1.0727704),
 ('TV(2.0) Loss', 1140.0186)], overall loss: -1080.299072265625
Iteration: 443, named\_losses: [('ActivationMax Loss', -2224.495),
 ('L-6.0 Norm Loss', 1.0702499),
 ('TV(2.0) Loss', 1141.1671)], overall loss: -1082.2576904296875
Iteration: 444, named\_losses: [('ActivationMax Loss', -2230.7944),
 ('L-6.0 Norm Loss', 1.0726025),
 ('TV(2.0) Loss', 1146.7769)], overall loss: -1082.945068359375
Iteration: 445, named\_losses: [('ActivationMax Loss', -2219.5059),
 ('L-6.0 Norm Loss', 1.072545),
 ('TV(2.0) Loss', 1138.2617)], overall loss: -1080.171630859375
Iteration: 446, named\_losses: [('ActivationMax Loss', -2232.4297),
 ('L-6.0 Norm Loss', 1.0700046),
 ('TV(2.0) Loss', 1149.0029)], overall loss: -1082.356689453125
Iteration: 447, named\_losses: [('ActivationMax Loss', -2220.4377),
 ('L-6.0 Norm Loss', 1.0695211),
 ('TV(2.0) Loss', 1137.7637)], overall loss: -1081.6044921875
Iteration: 448, named\_losses: [('ActivationMax Loss', -2237.4038),
 ('L-6.0 Norm Loss', 1.0735687),
 ('TV(2.0) Loss', 1153.3444)], overall loss: -1082.9859619140625
Iteration: 449, named\_losses: [('ActivationMax Loss', -2220.743),
 ('L-6.0 Norm Loss', 1.070093),
 ('TV(2.0) Loss', 1140.8254)], overall loss: -1078.847412109375
Iteration: 450, named\_losses: [('ActivationMax Loss', -2230.0942),
 ('L-6.0 Norm Loss', 1.0716071),
 ('TV(2.0) Loss', 1146.0052)], overall loss: -1083.0174560546875
Iteration: 451, named\_losses: [('ActivationMax Loss', -2218.4758),
 ('L-6.0 Norm Loss', 1.0723805),
 ('TV(2.0) Loss', 1142.831)], overall loss: -1074.572509765625
Iteration: 452, named\_losses: [('ActivationMax Loss', -2230.8774),
 ('L-6.0 Norm Loss', 1.0710627),
 ('TV(2.0) Loss', 1148.0167)], overall loss: -1081.7896728515625
Iteration: 453, named\_losses: [('ActivationMax Loss', -2222.141),
 ('L-6.0 Norm Loss', 1.0703552),
 ('TV(2.0) Loss', 1143.1088)], overall loss: -1077.9620361328125
Iteration: 454, named\_losses: [('ActivationMax Loss', -2231.192),
 ('L-6.0 Norm Loss', 1.0732466),
 ('TV(2.0) Loss', 1147.2195)], overall loss: -1082.899169921875
Iteration: 455, named\_losses: [('ActivationMax Loss', -2226.111),
 ('L-6.0 Norm Loss', 1.0706222),
 ('TV(2.0) Loss', 1142.268)], overall loss: -1082.7725830078125
Iteration: 456, named\_losses: [('ActivationMax Loss', -2228.4146),
 ('L-6.0 Norm Loss', 1.0703714),
 ('TV(2.0) Loss', 1144.3096)], overall loss: -1083.03466796875
Iteration: 457, named\_losses: [('ActivationMax Loss', -2223.4543),
 ('L-6.0 Norm Loss', 1.0721749),
 ('TV(2.0) Loss', 1145.1545)], overall loss: -1077.2275390625
Iteration: 458, named\_losses: [('ActivationMax Loss', -2227.1543),
 ('L-6.0 Norm Loss', 1.0690999),
 ('TV(2.0) Loss', 1143.021)], overall loss: -1083.064208984375
Iteration: 459, named\_losses: [('ActivationMax Loss', -2224.1882),
 ('L-6.0 Norm Loss', 1.0717015),
 ('TV(2.0) Loss', 1141.4758)], overall loss: -1081.640625
Iteration: 460, named\_losses: [('ActivationMax Loss', -2228.023),
 ('L-6.0 Norm Loss', 1.0736313),
 ('TV(2.0) Loss', 1148.0359)], overall loss: -1078.913330078125
Iteration: 461, named\_losses: [('ActivationMax Loss', -2228.4001),
 ('L-6.0 Norm Loss', 1.0705146),
 ('TV(2.0) Loss', 1147.9042)], overall loss: -1079.4254150390625
Iteration: 462, named\_losses: [('ActivationMax Loss', -2227.129),
 ('L-6.0 Norm Loss', 1.0687574),
 ('TV(2.0) Loss', 1147.487)], overall loss: -1078.572998046875
Iteration: 463, named\_losses: [('ActivationMax Loss', -2216.605),
 ('L-6.0 Norm Loss', 1.0725129),
 ('TV(2.0) Loss', 1135.441)], overall loss: -1080.0914306640625
Iteration: 464, named\_losses: [('ActivationMax Loss', -2229.3752),
 ('L-6.0 Norm Loss', 1.0683669),
 ('TV(2.0) Loss', 1150.0005)], overall loss: -1078.306396484375
Iteration: 465, named\_losses: [('ActivationMax Loss', -2224.535),
 ('L-6.0 Norm Loss', 1.0693744),
 ('TV(2.0) Loss', 1145.0037)], overall loss: -1078.4619140625
Iteration: 466, named\_losses: [('ActivationMax Loss', -2233.8247),
 ('L-6.0 Norm Loss', 1.0718662),
 ('TV(2.0) Loss', 1152.9137)], overall loss: -1079.8392333984375
Iteration: 467, named\_losses: [('ActivationMax Loss', -2214.4873),
 ('L-6.0 Norm Loss', 1.0719575),
 ('TV(2.0) Loss', 1136.1111)], overall loss: -1077.30419921875
Iteration: 468, named\_losses: [('ActivationMax Loss', -2228.7188),
 ('L-6.0 Norm Loss', 1.0681515),
 ('TV(2.0) Loss', 1142.078)], overall loss: -1085.5726318359375
Iteration: 469, named\_losses: [('ActivationMax Loss', -2223.9487),
 ('L-6.0 Norm Loss', 1.072851),
 ('TV(2.0) Loss', 1145.56)], overall loss: -1077.31591796875
Iteration: 470, named\_losses: [('ActivationMax Loss', -2230.3696),
 ('L-6.0 Norm Loss', 1.0707906),
 ('TV(2.0) Loss', 1147.121)], overall loss: -1082.1778564453125
Iteration: 471, named\_losses: [('ActivationMax Loss', -2223.0447),
 ('L-6.0 Norm Loss', 1.0700123),
 ('TV(2.0) Loss', 1144.4316)], overall loss: -1077.54296875
Iteration: 472, named\_losses: [('ActivationMax Loss', -2232.399),
 ('L-6.0 Norm Loss', 1.0689219),
 ('TV(2.0) Loss', 1149.7034)], overall loss: -1081.626708984375
Iteration: 473, named\_losses: [('ActivationMax Loss', -2223.134),
 ('L-6.0 Norm Loss', 1.0692208),
 ('TV(2.0) Loss', 1145.0181)], overall loss: -1077.046630859375
Iteration: 474, named\_losses: [('ActivationMax Loss', -2231.3284),
 ('L-6.0 Norm Loss', 1.06748),
 ('TV(2.0) Loss', 1152.3674)], overall loss: -1077.8935546875
Iteration: 475, named\_losses: [('ActivationMax Loss', -2223.5344),
 ('L-6.0 Norm Loss', 1.0698806),
 ('TV(2.0) Loss', 1144.4901)], overall loss: -1077.9744873046875
Iteration: 476, named\_losses: [('ActivationMax Loss', -2228.3538),
 ('L-6.0 Norm Loss', 1.0714864),
 ('TV(2.0) Loss', 1148.5061)], overall loss: -1078.776123046875
Iteration: 477, named\_losses: [('ActivationMax Loss', -2228.9739),
 ('L-6.0 Norm Loss', 1.0726657),
 ('TV(2.0) Loss', 1147.7345)], overall loss: -1080.1666259765625
Iteration: 478, named\_losses: [('ActivationMax Loss', -2228.928),
 ('L-6.0 Norm Loss', 1.0655029),
 ('TV(2.0) Loss', 1143.227)], overall loss: -1084.635498046875
Iteration: 479, named\_losses: [('ActivationMax Loss', -2232.9392),
 ('L-6.0 Norm Loss', 1.0710982),
 ('TV(2.0) Loss', 1150.0795)], overall loss: -1081.7886962890625
Iteration: 480, named\_losses: [('ActivationMax Loss', -2229.3645),
 ('L-6.0 Norm Loss', 1.068321),
 ('TV(2.0) Loss', 1144.949)], overall loss: -1083.34716796875
Iteration: 481, named\_losses: [('ActivationMax Loss', -2217.921),
 ('L-6.0 Norm Loss', 1.0718664),
 ('TV(2.0) Loss', 1143.386)], overall loss: -1073.463134765625
Iteration: 482, named\_losses: [('ActivationMax Loss', -2231.4702),
 ('L-6.0 Norm Loss', 1.0674037),
 ('TV(2.0) Loss', 1147.815)], overall loss: -1082.587890625
Iteration: 483, named\_losses: [('ActivationMax Loss', -2222.2532),
 ('L-6.0 Norm Loss', 1.071775),
 ('TV(2.0) Loss', 1143.4012)], overall loss: -1077.7801513671875
Iteration: 484, named\_losses: [('ActivationMax Loss', -2230.4807),
 ('L-6.0 Norm Loss', 1.069676),
 ('TV(2.0) Loss', 1143.9038)], overall loss: -1085.50732421875
Iteration: 485, named\_losses: [('ActivationMax Loss', -2219.4316),
 ('L-6.0 Norm Loss', 1.0677434),
 ('TV(2.0) Loss', 1136.4949)], overall loss: -1081.869140625
Iteration: 486, named\_losses: [('ActivationMax Loss', -2229.451),
 ('L-6.0 Norm Loss', 1.0672336),
 ('TV(2.0) Loss', 1147.7144)], overall loss: -1080.66943359375
Iteration: 487, named\_losses: [('ActivationMax Loss', -2234.1558),
 ('L-6.0 Norm Loss', 1.070165),
 ('TV(2.0) Loss', 1148.8245)], overall loss: -1084.26123046875
Iteration: 488, named\_losses: [('ActivationMax Loss', -2230.0042),
 ('L-6.0 Norm Loss', 1.0705265),
 ('TV(2.0) Loss', 1152.9694)], overall loss: -1075.9642333984375
Iteration: 489, named\_losses: [('ActivationMax Loss', -2227.2625),
 ('L-6.0 Norm Loss', 1.0698992),
 ('TV(2.0) Loss', 1142.744)], overall loss: -1083.4486083984375
Iteration: 490, named\_losses: [('ActivationMax Loss', -2226.2751),
 ('L-6.0 Norm Loss', 1.0682589),
 ('TV(2.0) Loss', 1144.8949)], overall loss: -1080.3118896484375
Iteration: 491, named\_losses: [('ActivationMax Loss', -2225.426),
 ('L-6.0 Norm Loss', 1.072447),
 ('TV(2.0) Loss', 1143.7976)], overall loss: -1080.555908203125
Iteration: 492, named\_losses: [('ActivationMax Loss', -2229.898),
 ('L-6.0 Norm Loss', 1.0669627),
 ('TV(2.0) Loss', 1144.5812)], overall loss: -1084.2498779296875
Iteration: 493, named\_losses: [('ActivationMax Loss', -2214.0002),
 ('L-6.0 Norm Loss', 1.0698755),
 ('TV(2.0) Loss', 1138.2119)], overall loss: -1074.718505859375
Iteration: 494, named\_losses: [('ActivationMax Loss', -2225.5288),
 ('L-6.0 Norm Loss', 1.0714601),
 ('TV(2.0) Loss', 1141.8153)], overall loss: -1082.6419677734375
Iteration: 495, named\_losses: [('ActivationMax Loss', -2233.5022),
 ('L-6.0 Norm Loss', 1.0714294),
 ('TV(2.0) Loss', 1154.4662)], overall loss: -1077.9644775390625
Iteration: 496, named\_losses: [('ActivationMax Loss', -2227.815),
 ('L-6.0 Norm Loss', 1.0662185),
 ('TV(2.0) Loss', 1144.3068)], overall loss: -1082.4420166015625
Iteration: 497, named\_losses: [('ActivationMax Loss', -2224.345),
 ('L-6.0 Norm Loss', 1.0701692),
 ('TV(2.0) Loss', 1140.3068)], overall loss: -1082.9681396484375
Iteration: 498, named\_losses: [('ActivationMax Loss', -2230.5537),
 ('L-6.0 Norm Loss', 1.066704),
 ('TV(2.0) Loss', 1147.97)], overall loss: -1081.51708984375
Iteration: 499, named\_losses: [('ActivationMax Loss', -2220.3862),
 ('L-6.0 Norm Loss', 1.0670965),
 ('TV(2.0) Loss', 1139.96)], overall loss: -1079.359130859375
Iteration: 500, named\_losses: [('ActivationMax Loss', -2229.584),
 ('L-6.0 Norm Loss', 1.0692662),
 ('TV(2.0) Loss', 1152.8817)], overall loss: -1075.6329345703125
Iteration: 501, named\_losses: [('ActivationMax Loss', -2216.9763),
 ('L-6.0 Norm Loss', 1.066907),
 ('TV(2.0) Loss', 1138.6875)], overall loss: -1077.221923828125
Iteration: 502, named\_losses: [('ActivationMax Loss', -2230.1672),
 ('L-6.0 Norm Loss', 1.0688972),
 ('TV(2.0) Loss', 1147.8553)], overall loss: -1081.2430419921875
Iteration: 503, named\_losses: [('ActivationMax Loss', -2224.3853),
 ('L-6.0 Norm Loss', 1.0653846),
 ('TV(2.0) Loss', 1139.4017)], overall loss: -1083.9180908203125
Iteration: 504, named\_losses: [('ActivationMax Loss', -2231.7893),
 ('L-6.0 Norm Loss', 1.0660093),
 ('TV(2.0) Loss', 1147.1617)], overall loss: -1083.5616455078125
Iteration: 505, named\_losses: [('ActivationMax Loss', -2224.8994),
 ('L-6.0 Norm Loss', 1.0703353),
 ('TV(2.0) Loss', 1143.7495)], overall loss: -1080.07958984375
Iteration: 506, named\_losses: [('ActivationMax Loss', -2226.218),
 ('L-6.0 Norm Loss', 1.0658567),
 ('TV(2.0) Loss', 1144.9065)], overall loss: -1080.24560546875
Iteration: 507, named\_losses: [('ActivationMax Loss', -2221.7336),
 ('L-6.0 Norm Loss', 1.0681243),
 ('TV(2.0) Loss', 1143.3494)], overall loss: -1077.316162109375
Iteration: 508, named\_losses: [('ActivationMax Loss', -2223.965),
 ('L-6.0 Norm Loss', 1.0637747),
 ('TV(2.0) Loss', 1143.5695)], overall loss: -1079.3319091796875
Iteration: 509, named\_losses: [('ActivationMax Loss', -2226.1008),
 ('L-6.0 Norm Loss', 1.0687068),
 ('TV(2.0) Loss', 1146.375)], overall loss: -1078.6572265625
Iteration: 510, named\_losses: [('ActivationMax Loss', -2230.2214),
 ('L-6.0 Norm Loss', 1.0655411),
 ('TV(2.0) Loss', 1149.3068)], overall loss: -1079.8492431640625
Iteration: 511, named\_losses: [('ActivationMax Loss', -2216.7712),
 ('L-6.0 Norm Loss', 1.0676404),
 ('TV(2.0) Loss', 1139.6475)], overall loss: -1076.05615234375
Iteration: 512, named\_losses: [('ActivationMax Loss', -2232.1995),
 ('L-6.0 Norm Loss', 1.0686098),
 ('TV(2.0) Loss', 1146.7734)], overall loss: -1084.357421875
Iteration: 513, named\_losses: [('ActivationMax Loss', -2215.9944),
 ('L-6.0 Norm Loss', 1.0671048),
 ('TV(2.0) Loss', 1133.4918)], overall loss: -1081.4354248046875
Iteration: 514, named\_losses: [('ActivationMax Loss', -2234.1272),
 ('L-6.0 Norm Loss', 1.0663133),
 ('TV(2.0) Loss', 1152.2094)], overall loss: -1080.8514404296875
Iteration: 515, named\_losses: [('ActivationMax Loss', -2227.2869),
 ('L-6.0 Norm Loss', 1.0698186),
 ('TV(2.0) Loss', 1148.012)], overall loss: -1078.205078125
Iteration: 516, named\_losses: [('ActivationMax Loss', -2234.1646),
 ('L-6.0 Norm Loss', 1.0670316),
 ('TV(2.0) Loss', 1155.1016)], overall loss: -1077.995849609375
Iteration: 517, named\_losses: [('ActivationMax Loss', -2222.0876),
 ('L-6.0 Norm Loss', 1.0667115),
 ('TV(2.0) Loss', 1136.1544)], overall loss: -1084.8665771484375
Iteration: 518, named\_losses: [('ActivationMax Loss', -2234.129),
 ('L-6.0 Norm Loss', 1.0684185),
 ('TV(2.0) Loss', 1153.895)], overall loss: -1079.16552734375
Iteration: 519, named\_losses: [('ActivationMax Loss', -2221.1128),
 ('L-6.0 Norm Loss', 1.0685667),
 ('TV(2.0) Loss', 1137.439)], overall loss: -1082.605224609375
Iteration: 520, named\_losses: [('ActivationMax Loss', -2231.3918),
 ('L-6.0 Norm Loss', 1.0671538),
 ('TV(2.0) Loss', 1151.8282)], overall loss: -1078.4964599609375
Iteration: 521, named\_losses: [('ActivationMax Loss', -2220.522),
 ('L-6.0 Norm Loss', 1.0702944),
 ('TV(2.0) Loss', 1139.1354)], overall loss: -1080.3162841796875
Iteration: 522, named\_losses: [('ActivationMax Loss', -2229.215),
 ('L-6.0 Norm Loss', 1.0678453),
 ('TV(2.0) Loss', 1147.9133)], overall loss: -1080.23388671875
Iteration: 523, named\_losses: [('ActivationMax Loss', -2220.4678),
 ('L-6.0 Norm Loss', 1.0698446),
 ('TV(2.0) Loss', 1137.4739)], overall loss: -1081.924072265625
Iteration: 524, named\_losses: [('ActivationMax Loss', -2227.592),
 ('L-6.0 Norm Loss', 1.0652387),
 ('TV(2.0) Loss', 1145.9268)], overall loss: -1080.60009765625
Iteration: 525, named\_losses: [('ActivationMax Loss', -2221.2356),
 ('L-6.0 Norm Loss', 1.0672328),
 ('TV(2.0) Loss', 1141.1223)], overall loss: -1079.046142578125
Iteration: 526, named\_losses: [('ActivationMax Loss', -2235.5632),
 ('L-6.0 Norm Loss', 1.0699029),
 ('TV(2.0) Loss', 1151.3871)], overall loss: -1083.1063232421875
Iteration: 527, named\_losses: [('ActivationMax Loss', -2219.8977),
 ('L-6.0 Norm Loss', 1.0706869),
 ('TV(2.0) Loss', 1140.5591)], overall loss: -1078.267822265625
Iteration: 528, named\_losses: [('ActivationMax Loss', -2229.039),
 ('L-6.0 Norm Loss', 1.0667068),
 ('TV(2.0) Loss', 1147.4341)], overall loss: -1080.538330078125
Iteration: 529, named\_losses: [('ActivationMax Loss', -2220.9275),
 ('L-6.0 Norm Loss', 1.0689731),
 ('TV(2.0) Loss', 1138.8811)], overall loss: -1080.977294921875
Iteration: 530, named\_losses: [('ActivationMax Loss', -2233.092),
 ('L-6.0 Norm Loss', 1.0668122),
 ('TV(2.0) Loss', 1148.0624)], overall loss: -1083.9627685546875
Iteration: 531, named\_losses: [('ActivationMax Loss', -2218.6526),
 ('L-6.0 Norm Loss', 1.0685657),
 ('TV(2.0) Loss', 1142.1538)], overall loss: -1075.43017578125
Iteration: 532, named\_losses: [('ActivationMax Loss', -2233.4827),
 ('L-6.0 Norm Loss', 1.0702218),
 ('TV(2.0) Loss', 1152.7)], overall loss: -1079.71240234375
Iteration: 533, named\_losses: [('ActivationMax Loss', -2219.8245),
 ('L-6.0 Norm Loss', 1.0654396),
 ('TV(2.0) Loss', 1140.9451)], overall loss: -1077.81396484375
Iteration: 534, named\_losses: [('ActivationMax Loss', -2231.5415),
 ('L-6.0 Norm Loss', 1.0676515),
 ('TV(2.0) Loss', 1150.6581)], overall loss: -1079.8157958984375
Iteration: 535, named\_losses: [('ActivationMax Loss', -2228.715),
 ('L-6.0 Norm Loss', 1.0676837),
 ('TV(2.0) Loss', 1143.8866)], overall loss: -1083.7608642578125
Iteration: 536, named\_losses: [('ActivationMax Loss', -2231.26),
 ('L-6.0 Norm Loss', 1.0685205),
 ('TV(2.0) Loss', 1151.1006)], overall loss: -1079.0908203125
Iteration: 537, named\_losses: [('ActivationMax Loss', -2222.817),
 ('L-6.0 Norm Loss', 1.0695084),
 ('TV(2.0) Loss', 1139.2938)], overall loss: -1082.4534912109375
Iteration: 538, named\_losses: [('ActivationMax Loss', -2231.1238),
 ('L-6.0 Norm Loss', 1.068561),
 ('TV(2.0) Loss', 1149.4928)], overall loss: -1080.5623779296875
Iteration: 539, named\_losses: [('ActivationMax Loss', -2224.866),
 ('L-6.0 Norm Loss', 1.0679299),
 ('TV(2.0) Loss', 1142.884)], overall loss: -1080.9140625
Iteration: 540, named\_losses: [('ActivationMax Loss', -2219.4902),
 ('L-6.0 Norm Loss', 1.065889),
 ('TV(2.0) Loss', 1143.7601)], overall loss: -1074.6641845703125
Iteration: 541, named\_losses: [('ActivationMax Loss', -2224.4006),
 ('L-6.0 Norm Loss', 1.066124),
 ('TV(2.0) Loss', 1139.5714)], overall loss: -1083.7630615234375
Iteration: 542, named\_losses: [('ActivationMax Loss', -2229.228),
 ('L-6.0 Norm Loss', 1.0699592),
 ('TV(2.0) Loss', 1152.0101)], overall loss: -1076.1478271484375
Iteration: 543, named\_losses: [('ActivationMax Loss', -2225.6418),
 ('L-6.0 Norm Loss', 1.0683542),
 ('TV(2.0) Loss', 1143.8116)], overall loss: -1080.7618408203125
Iteration: 544, named\_losses: [('ActivationMax Loss', -2222.7979),
 ('L-6.0 Norm Loss', 1.0691837),
 ('TV(2.0) Loss', 1141.0116)], overall loss: -1080.7171630859375
Iteration: 545, named\_losses: [('ActivationMax Loss', -2221.0452),
 ('L-6.0 Norm Loss', 1.0683714),
 ('TV(2.0) Loss', 1141.0862)], overall loss: -1078.890625
Iteration: 546, named\_losses: [('ActivationMax Loss', -2220.6177),
 ('L-6.0 Norm Loss', 1.066187),
 ('TV(2.0) Loss', 1138.6532)], overall loss: -1080.8983154296875
Iteration: 547, named\_losses: [('ActivationMax Loss', -2223.4124),
 ('L-6.0 Norm Loss', 1.0705248),
 ('TV(2.0) Loss', 1143.3955)], overall loss: -1078.9462890625
Iteration: 548, named\_losses: [('ActivationMax Loss', -2225.1853),
 ('L-6.0 Norm Loss', 1.0684382),
 ('TV(2.0) Loss', 1145.5895)], overall loss: -1078.5274658203125
Iteration: 549, named\_losses: [('ActivationMax Loss', -2220.4768),
 ('L-6.0 Norm Loss', 1.0658686),
 ('TV(2.0) Loss', 1137.2563)], overall loss: -1082.154541015625
Iteration: 550, named\_losses: [('ActivationMax Loss', -2228.9026),
 ('L-6.0 Norm Loss', 1.0659795),
 ('TV(2.0) Loss', 1145.2198)], overall loss: -1082.6168212890625
Iteration: 551, named\_losses: [('ActivationMax Loss', -2218.1814),
 ('L-6.0 Norm Loss', 1.070605),
 ('TV(2.0) Loss', 1137.5112)], overall loss: -1079.599609375
Iteration: 552, named\_losses: [('ActivationMax Loss', -2228.1594),
 ('L-6.0 Norm Loss', 1.0686034),
 ('TV(2.0) Loss', 1146.0736)], overall loss: -1081.0172119140625
Iteration: 553, named\_losses: [('ActivationMax Loss', -2225.76),
 ('L-6.0 Norm Loss', 1.0705109),
 ('TV(2.0) Loss', 1140.7949)], overall loss: -1083.89453125
Iteration: 554, named\_losses: [('ActivationMax Loss', -2233.5364),
 ('L-6.0 Norm Loss', 1.0665786),
 ('TV(2.0) Loss', 1151.5253)], overall loss: -1080.9444580078125
Iteration: 555, named\_losses: [('ActivationMax Loss', -2223.623),
 ('L-6.0 Norm Loss', 1.0715394),
 ('TV(2.0) Loss', 1141.9325)], overall loss: -1080.6190185546875
Iteration: 556, named\_losses: [('ActivationMax Loss', -2237.382),
 ('L-6.0 Norm Loss', 1.0649248),
 ('TV(2.0) Loss', 1153.7)], overall loss: -1082.6171875
Iteration: 557, named\_losses: [('ActivationMax Loss', -2229.3481),
 ('L-6.0 Norm Loss', 1.0698102),
 ('TV(2.0) Loss', 1146.134)], overall loss: -1082.144287109375
Iteration: 558, named\_losses: [('ActivationMax Loss', -2232.6108),
 ('L-6.0 Norm Loss', 1.0681181),
 ('TV(2.0) Loss', 1152.6553)], overall loss: -1078.887451171875
Iteration: 559, named\_losses: [('ActivationMax Loss', -2222.8394),
 ('L-6.0 Norm Loss', 1.0708578),
 ('TV(2.0) Loss', 1141.0071)], overall loss: -1080.761474609375
Iteration: 560, named\_losses: [('ActivationMax Loss', -2231.3916),
 ('L-6.0 Norm Loss', 1.0668985),
 ('TV(2.0) Loss', 1145.964)], overall loss: -1084.3607177734375
Iteration: 561, named\_losses: [('ActivationMax Loss', -2229.0342),
 ('L-6.0 Norm Loss', 1.0680336),
 ('TV(2.0) Loss', 1145.9945)], overall loss: -1081.9715576171875
Iteration: 562, named\_losses: [('ActivationMax Loss', -2230.0847),
 ('L-6.0 Norm Loss', 1.0656134),
 ('TV(2.0) Loss', 1149.1802)], overall loss: -1079.8388671875
Iteration: 563, named\_losses: [('ActivationMax Loss', -2227.066),
 ('L-6.0 Norm Loss', 1.0677927),
 ('TV(2.0) Loss', 1144.6819)], overall loss: -1081.316162109375
Iteration: 564, named\_losses: [('ActivationMax Loss', -2226.6257),
 ('L-6.0 Norm Loss', 1.0639999),
 ('TV(2.0) Loss', 1143.2556)], overall loss: -1082.30615234375
Iteration: 565, named\_losses: [('ActivationMax Loss', -2234.416),
 ('L-6.0 Norm Loss', 1.069486),
 ('TV(2.0) Loss', 1148.2295)], overall loss: -1085.116943359375
Iteration: 566, named\_losses: [('ActivationMax Loss', -2228.1865),
 ('L-6.0 Norm Loss', 1.06628),
 ('TV(2.0) Loss', 1149.4824)], overall loss: -1077.637939453125
Iteration: 567, named\_losses: [('ActivationMax Loss', -2232.9333),
 ('L-6.0 Norm Loss', 1.0697469),
 ('TV(2.0) Loss', 1145.6073)], overall loss: -1086.2562255859375
Iteration: 568, named\_losses: [('ActivationMax Loss', -2236.5576),
 ('L-6.0 Norm Loss', 1.0688103),
 ('TV(2.0) Loss', 1158.365)], overall loss: -1077.123779296875
Iteration: 569, named\_losses: [('ActivationMax Loss', -2229.429),
 ('L-6.0 Norm Loss', 1.0669957),
 ('TV(2.0) Loss', 1141.9017)], overall loss: -1086.4603271484375
Iteration: 570, named\_losses: [('ActivationMax Loss', -2230.377),
 ('L-6.0 Norm Loss', 1.0684013),
 ('TV(2.0) Loss', 1146.8687)], overall loss: -1082.43994140625
Iteration: 571, named\_losses: [('ActivationMax Loss', -2224.569),
 ('L-6.0 Norm Loss', 1.0675428),
 ('TV(2.0) Loss', 1139.2716)], overall loss: -1084.2298583984375
Iteration: 572, named\_losses: [('ActivationMax Loss', -2231.3374),
 ('L-6.0 Norm Loss', 1.0664675),
 ('TV(2.0) Loss', 1149.6632)], overall loss: -1080.6077880859375
Iteration: 573, named\_losses: [('ActivationMax Loss', -2236.447),
 ('L-6.0 Norm Loss', 1.0688437),
 ('TV(2.0) Loss', 1149.7573)], overall loss: -1085.620849609375
Iteration: 574, named\_losses: [('ActivationMax Loss', -2221.7197),
 ('L-6.0 Norm Loss', 1.0684955),
 ('TV(2.0) Loss', 1144.3247)], overall loss: -1076.326416015625
Iteration: 575, named\_losses: [('ActivationMax Loss', -2229.1133),
 ('L-6.0 Norm Loss', 1.0687358),
 ('TV(2.0) Loss', 1144.7041)], overall loss: -1083.34033203125
Iteration: 576, named\_losses: [('ActivationMax Loss', -2226.6907),
 ('L-6.0 Norm Loss', 1.0666872),
 ('TV(2.0) Loss', 1147.5304)], overall loss: -1078.0936279296875
Iteration: 577, named\_losses: [('ActivationMax Loss', -2233.9297),
 ('L-6.0 Norm Loss', 1.0668862),
 ('TV(2.0) Loss', 1147.4478)], overall loss: -1085.4150390625
Iteration: 578, named\_losses: [('ActivationMax Loss', -2225.6138),
 ('L-6.0 Norm Loss', 1.0682557),
 ('TV(2.0) Loss', 1143.833)], overall loss: -1080.71240234375
Iteration: 579, named\_losses: [('ActivationMax Loss', -2230.2417),
 ('L-6.0 Norm Loss', 1.0668026),
 ('TV(2.0) Loss', 1144.223)], overall loss: -1084.9517822265625
Iteration: 580, named\_losses: [('ActivationMax Loss', -2226.24),
 ('L-6.0 Norm Loss', 1.0674615),
 ('TV(2.0) Loss', 1147.8093)], overall loss: -1077.36328125
Iteration: 581, named\_losses: [('ActivationMax Loss', -2227.682),
 ('L-6.0 Norm Loss', 1.0666034),
 ('TV(2.0) Loss', 1140.2186)], overall loss: -1086.3966064453125
Iteration: 582, named\_losses: [('ActivationMax Loss', -2226.777),
 ('L-6.0 Norm Loss', 1.0654298),
 ('TV(2.0) Loss', 1146.7983)], overall loss: -1078.913330078125
Iteration: 583, named\_losses: [('ActivationMax Loss', -2235.3132),
 ('L-6.0 Norm Loss', 1.0666312),
 ('TV(2.0) Loss', 1150.6254)], overall loss: -1083.6212158203125
Iteration: 584, named\_losses: [('ActivationMax Loss', -2235.8552),
 ('L-6.0 Norm Loss', 1.0675818),
 ('TV(2.0) Loss', 1154.8541)], overall loss: -1079.9334716796875
Iteration: 585, named\_losses: [('ActivationMax Loss', -2234.9534),
 ('L-6.0 Norm Loss', 1.0677546),
 ('TV(2.0) Loss', 1150.6671)], overall loss: -1083.2183837890625
Iteration: 586, named\_losses: [('ActivationMax Loss', -2236.8213),
 ('L-6.0 Norm Loss', 1.0681354),
 ('TV(2.0) Loss', 1151.7455)], overall loss: -1084.0076904296875
Iteration: 587, named\_losses: [('ActivationMax Loss', -2223.3171),
 ('L-6.0 Norm Loss', 1.0667957),
 ('TV(2.0) Loss', 1140.6614)], overall loss: -1081.5888671875
Iteration: 588, named\_losses: [('ActivationMax Loss', -2228.5085),
 ('L-6.0 Norm Loss', 1.0665164),
 ('TV(2.0) Loss', 1151.0625)], overall loss: -1076.379638671875
Iteration: 589, named\_losses: [('ActivationMax Loss', -2237.241),
 ('L-6.0 Norm Loss', 1.0674058),
 ('TV(2.0) Loss', 1146.8658)], overall loss: -1089.3077392578125
Iteration: 590, named\_losses: [('ActivationMax Loss', -2226.03),
 ('L-6.0 Norm Loss', 1.0680863),
 ('TV(2.0) Loss', 1149.7203)], overall loss: -1075.2415771484375
Iteration: 591, named\_losses: [('ActivationMax Loss', -2228.5732),
 ('L-6.0 Norm Loss', 1.0651536),
 ('TV(2.0) Loss', 1144.1708)], overall loss: -1083.3372802734375
Iteration: 592, named\_losses: [('ActivationMax Loss', -2231.5603),
 ('L-6.0 Norm Loss', 1.065727),
 ('TV(2.0) Loss', 1147.3547)], overall loss: -1083.139892578125
Iteration: 593, named\_losses: [('ActivationMax Loss', -2227.854),
 ('L-6.0 Norm Loss', 1.067447),
 ('TV(2.0) Loss', 1142.5552)], overall loss: -1084.2314453125
Iteration: 594, named\_losses: [('ActivationMax Loss', -2231.3955),
 ('L-6.0 Norm Loss', 1.0657629),
 ('TV(2.0) Loss', 1151.8043)], overall loss: -1078.5255126953125
Iteration: 595, named\_losses: [('ActivationMax Loss', -2233.1262),
 ('L-6.0 Norm Loss', 1.0690931),
 ('TV(2.0) Loss', 1149.4343)], overall loss: -1082.622802734375
Iteration: 596, named\_losses: [('ActivationMax Loss', -2226.451),
 ('L-6.0 Norm Loss', 1.0653068),
 ('TV(2.0) Loss', 1145.5299)], overall loss: -1079.8558349609375
Iteration: 597, named\_losses: [('ActivationMax Loss', -2231.3372),
 ('L-6.0 Norm Loss', 1.0684847),
 ('TV(2.0) Loss', 1143.8156)], overall loss: -1086.4530029296875
Iteration: 598, named\_losses: [('ActivationMax Loss', -2225.092),
 ('L-6.0 Norm Loss', 1.0650562),
 ('TV(2.0) Loss', 1144.6112)], overall loss: -1079.4158935546875
Iteration: 599, named\_losses: [('ActivationMax Loss', -2229.6592),
 ('L-6.0 Norm Loss', 1.0661237),
 ('TV(2.0) Loss', 1145.5321)], overall loss: -1083.0609130859375
Iteration: 600, named\_losses: [('ActivationMax Loss', -2231.3296),
 ('L-6.0 Norm Loss', 1.0623866),
 ('TV(2.0) Loss', 1149.4095)], overall loss: -1080.8575439453125
Iteration: 601, named\_losses: [('ActivationMax Loss', -2218.4702),
 ('L-6.0 Norm Loss', 1.0663632),
 ('TV(2.0) Loss', 1133.6609)], overall loss: -1083.742919921875
Iteration: 602, named\_losses: [('ActivationMax Loss', -2233.927),
 ('L-6.0 Norm Loss', 1.0651356),
 ('TV(2.0) Loss', 1147.0077)], overall loss: -1085.8541259765625
Iteration: 603, named\_losses: [('ActivationMax Loss', -2219.9653),
 ('L-6.0 Norm Loss', 1.0678991),
 ('TV(2.0) Loss', 1137.3942)], overall loss: -1081.5032958984375
Iteration: 604, named\_losses: [('ActivationMax Loss', -2232.0498),
 ('L-6.0 Norm Loss', 1.065186),
 ('TV(2.0) Loss', 1147.4674)], overall loss: -1083.5172119140625
Iteration: 605, named\_losses: [('ActivationMax Loss', -2227.6702),
 ('L-6.0 Norm Loss', 1.0689768),
 ('TV(2.0) Loss', 1147.9415)], overall loss: -1078.6595458984375
Iteration: 606, named\_losses: [('ActivationMax Loss', -2239.0386),
 ('L-6.0 Norm Loss', 1.065612),
 ('TV(2.0) Loss', 1153.1324)], overall loss: -1084.8404541015625
Iteration: 607, named\_losses: [('ActivationMax Loss', -2228.108),
 ('L-6.0 Norm Loss', 1.0662048),
 ('TV(2.0) Loss', 1147.4585)], overall loss: -1079.583251953125
Iteration: 608, named\_losses: [('ActivationMax Loss', -2225.3606),
 ('L-6.0 Norm Loss', 1.0662516),
 ('TV(2.0) Loss', 1148.202)], overall loss: -1076.0924072265625
Iteration: 609, named\_losses: [('ActivationMax Loss', -2232.2742),
 ('L-6.0 Norm Loss', 1.0662491),
 ('TV(2.0) Loss', 1150.503)], overall loss: -1080.7049560546875
Iteration: 610, named\_losses: [('ActivationMax Loss', -2227.2676),
 ('L-6.0 Norm Loss', 1.0677301),
 ('TV(2.0) Loss', 1147.1494)], overall loss: -1079.050537109375
Iteration: 611, named\_losses: [('ActivationMax Loss', -2230.4187),
 ('L-6.0 Norm Loss', 1.0632632),
 ('TV(2.0) Loss', 1147.1407)], overall loss: -1082.2147216796875
Iteration: 612, named\_losses: [('ActivationMax Loss', -2228.1387),
 ('L-6.0 Norm Loss', 1.0660143),
 ('TV(2.0) Loss', 1146.7131)], overall loss: -1080.359619140625
Iteration: 613, named\_losses: [('ActivationMax Loss', -2233.8196),
 ('L-6.0 Norm Loss', 1.0666777),
 ('TV(2.0) Loss', 1148.0942)], overall loss: -1084.65869140625
Iteration: 614, named\_losses: [('ActivationMax Loss', -2221.2307),
 ('L-6.0 Norm Loss', 1.0667884),
 ('TV(2.0) Loss', 1140.1436)], overall loss: -1080.020263671875
Iteration: 615, named\_losses: [('ActivationMax Loss', -2239.9314),
 ('L-6.0 Norm Loss', 1.0684633),
 ('TV(2.0) Loss', 1152.3417)], overall loss: -1086.5213623046875
Iteration: 616, named\_losses: [('ActivationMax Loss', -2219.3794),
 ('L-6.0 Norm Loss', 1.0665175),
 ('TV(2.0) Loss', 1139.3925)], overall loss: -1078.9205322265625
Iteration: 617, named\_losses: [('ActivationMax Loss', -2237.4006),
 ('L-6.0 Norm Loss', 1.0671893),
 ('TV(2.0) Loss', 1153.9314)], overall loss: -1082.402099609375
Iteration: 618, named\_losses: [('ActivationMax Loss', -2220.6362),
 ('L-6.0 Norm Loss', 1.0687544),
 ('TV(2.0) Loss', 1140.0398)], overall loss: -1079.527587890625
Iteration: 619, named\_losses: [('ActivationMax Loss', -2235.88),
 ('L-6.0 Norm Loss', 1.067035),
 ('TV(2.0) Loss', 1149.9432)], overall loss: -1084.8695068359375
Iteration: 620, named\_losses: [('ActivationMax Loss', -2227.414),
 ('L-6.0 Norm Loss', 1.0670034),
 ('TV(2.0) Loss', 1141.2125)], overall loss: -1085.1346435546875
Iteration: 621, named\_losses: [('ActivationMax Loss', -2232.8562),
 ('L-6.0 Norm Loss', 1.0657831),
 ('TV(2.0) Loss', 1155.7157)], overall loss: -1076.0748291015625
Iteration: 622, named\_losses: [('ActivationMax Loss', -2226.4463),
 ('L-6.0 Norm Loss', 1.066058),
 ('TV(2.0) Loss', 1141.5476)], overall loss: -1083.83251953125
Iteration: 623, named\_losses: [('ActivationMax Loss', -2230.6272),
 ('L-6.0 Norm Loss', 1.0676755),
 ('TV(2.0) Loss', 1149.061)], overall loss: -1080.49853515625
Iteration: 624, named\_losses: [('ActivationMax Loss', -2222.4622),
 ('L-6.0 Norm Loss', 1.0683669),
 ('TV(2.0) Loss', 1140.5896)], overall loss: -1080.80419921875
Iteration: 625, named\_losses: [('ActivationMax Loss', -2233.4705),
 ('L-6.0 Norm Loss', 1.0655886),
 ('TV(2.0) Loss', 1151.8132)], overall loss: -1080.591552734375
Iteration: 626, named\_losses: [('ActivationMax Loss', -2217.7202),
 ('L-6.0 Norm Loss', 1.0678185),
 ('TV(2.0) Loss', 1134.7588)], overall loss: -1081.8935546875
Iteration: 627, named\_losses: [('ActivationMax Loss', -2221.5222),
 ('L-6.0 Norm Loss', 1.0647138),
 ('TV(2.0) Loss', 1136.6011)], overall loss: -1083.8564453125
Iteration: 628, named\_losses: [('ActivationMax Loss', -2217.628),
 ('L-6.0 Norm Loss', 1.0692698),
 ('TV(2.0) Loss', 1138.048)], overall loss: -1078.5106201171875
Iteration: 629, named\_losses: [('ActivationMax Loss', -2227.755),
 ('L-6.0 Norm Loss', 1.0666184),
 ('TV(2.0) Loss', 1147.512)], overall loss: -1079.17626953125
Iteration: 630, named\_losses: [('ActivationMax Loss', -2225.1213),
 ('L-6.0 Norm Loss', 1.0693913),
 ('TV(2.0) Loss', 1144.128)], overall loss: -1079.9239501953125
Iteration: 631, named\_losses: [('ActivationMax Loss', -2232.9749),
 ('L-6.0 Norm Loss', 1.0659862),
 ('TV(2.0) Loss', 1144.8485)], overall loss: -1087.0604248046875
Iteration: 632, named\_losses: [('ActivationMax Loss', -2230.7844),
 ('L-6.0 Norm Loss', 1.0686398),
 ('TV(2.0) Loss', 1143.8025)], overall loss: -1085.913330078125
Iteration: 633, named\_losses: [('ActivationMax Loss', -2232.414),
 ('L-6.0 Norm Loss', 1.0657895),
 ('TV(2.0) Loss', 1147.4457)], overall loss: -1083.9027099609375
Iteration: 634, named\_losses: [('ActivationMax Loss', -2222.691),
 ('L-6.0 Norm Loss', 1.0687941),
 ('TV(2.0) Loss', 1139.572)], overall loss: -1082.050048828125
Iteration: 635, named\_losses: [('ActivationMax Loss', -2229.4807),
 ('L-6.0 Norm Loss', 1.064761),
 ('TV(2.0) Loss', 1148.3617)], overall loss: -1080.0543212890625
Iteration: 636, named\_losses: [('ActivationMax Loss', -2229.5583),
 ('L-6.0 Norm Loss', 1.0702816),
 ('TV(2.0) Loss', 1147.5901)], overall loss: -1080.89794921875
Iteration: 637, named\_losses: [('ActivationMax Loss', -2231.2737),
 ('L-6.0 Norm Loss', 1.0668744),
 ('TV(2.0) Loss', 1147.0594)], overall loss: -1083.1473388671875
Iteration: 638, named\_losses: [('ActivationMax Loss', -2236.1345),
 ('L-6.0 Norm Loss', 1.0691532),
 ('TV(2.0) Loss', 1147.931)], overall loss: -1087.1343994140625
Iteration: 639, named\_losses: [('ActivationMax Loss', -2233.0771),
 ('L-6.0 Norm Loss', 1.0677965),
 ('TV(2.0) Loss', 1148.5925)], overall loss: -1083.416748046875
Iteration: 640, named\_losses: [('ActivationMax Loss', -2225.1086),
 ('L-6.0 Norm Loss', 1.0679775),
 ('TV(2.0) Loss', 1144.3955)], overall loss: -1079.645263671875
Iteration: 641, named\_losses: [('ActivationMax Loss', -2230.115),
 ('L-6.0 Norm Loss', 1.0663863),
 ('TV(2.0) Loss', 1145.4218)], overall loss: -1083.6268310546875
Iteration: 642, named\_losses: [('ActivationMax Loss', -2226.9749),
 ('L-6.0 Norm Loss', 1.0686841),
 ('TV(2.0) Loss', 1146.6163)], overall loss: -1079.2899169921875
Iteration: 643, named\_losses: [('ActivationMax Loss', -2227.3875),
 ('L-6.0 Norm Loss', 1.0681725),
 ('TV(2.0) Loss', 1143.1411)], overall loss: -1083.17822265625
Iteration: 644, named\_losses: [('ActivationMax Loss', -2228.2046),
 ('L-6.0 Norm Loss', 1.0687366),
 ('TV(2.0) Loss', 1141.5941)], overall loss: -1085.5416259765625
Iteration: 645, named\_losses: [('ActivationMax Loss', -2229.0706),
 ('L-6.0 Norm Loss', 1.066819),
 ('TV(2.0) Loss', 1147.3802)], overall loss: -1080.6234130859375
Iteration: 646, named\_losses: [('ActivationMax Loss', -2235.0354),
 ('L-6.0 Norm Loss', 1.0664304),
 ('TV(2.0) Loss', 1156.2572)], overall loss: -1077.7117919921875
Iteration: 647, named\_losses: [('ActivationMax Loss', -2222.0603),
 ('L-6.0 Norm Loss', 1.0668541),
 ('TV(2.0) Loss', 1141.9194)], overall loss: -1079.073974609375
Iteration: 648, named\_losses: [('ActivationMax Loss', -2228.5461),
 ('L-6.0 Norm Loss', 1.0668153),
 ('TV(2.0) Loss', 1149.7471)], overall loss: -1077.732177734375
Iteration: 649, named\_losses: [('ActivationMax Loss', -2228.7585),
 ('L-6.0 Norm Loss', 1.0661463),
 ('TV(2.0) Loss', 1144.4235)], overall loss: -1083.2689208984375
Iteration: 650, named\_losses: [('ActivationMax Loss', -2233.3843),
 ('L-6.0 Norm Loss', 1.0668056),
 ('TV(2.0) Loss', 1149.7689)], overall loss: -1082.5484619140625
Iteration: 651, named\_losses: [('ActivationMax Loss', -2230.8025),
 ('L-6.0 Norm Loss', 1.0656761),
 ('TV(2.0) Loss', 1144.7539)], overall loss: -1084.98291015625
Iteration: 652, named\_losses: [('ActivationMax Loss', -2238.7437),
 ('L-6.0 Norm Loss', 1.0710953),
 ('TV(2.0) Loss', 1154.2009)], overall loss: -1083.4716796875
Iteration: 653, named\_losses: [('ActivationMax Loss', -2227.509),
 ('L-6.0 Norm Loss', 1.0690466),
 ('TV(2.0) Loss', 1143.4669)], overall loss: -1082.9730224609375
Iteration: 654, named\_losses: [('ActivationMax Loss', -2229.1572),
 ('L-6.0 Norm Loss', 1.0681146),
 ('TV(2.0) Loss', 1150.6804)], overall loss: -1077.40869140625
Iteration: 655, named\_losses: [('ActivationMax Loss', -2233.8718),
 ('L-6.0 Norm Loss', 1.0693142),
 ('TV(2.0) Loss', 1149.3783)], overall loss: -1083.4241943359375
Iteration: 656, named\_losses: [('ActivationMax Loss', -2230.5405),
 ('L-6.0 Norm Loss', 1.0670402),
 ('TV(2.0) Loss', 1145.9476)], overall loss: -1083.5257568359375
Iteration: 657, named\_losses: [('ActivationMax Loss', -2226.6685),
 ('L-6.0 Norm Loss', 1.0673825),
 ('TV(2.0) Loss', 1146.2411)], overall loss: -1079.3599853515625
Iteration: 658, named\_losses: [('ActivationMax Loss', -2234.4636),
 ('L-6.0 Norm Loss', 1.0688252),
 ('TV(2.0) Loss', 1153.8356)], overall loss: -1079.5592041015625
Iteration: 659, named\_losses: [('ActivationMax Loss', -2220.532),
 ('L-6.0 Norm Loss', 1.0668458),
 ('TV(2.0) Loss', 1139.7511)], overall loss: -1079.7139892578125
Iteration: 660, named\_losses: [('ActivationMax Loss', -2231.3723),
 ('L-6.0 Norm Loss', 1.0676583),
 ('TV(2.0) Loss', 1145.7782)], overall loss: -1084.5264892578125
Iteration: 661, named\_losses: [('ActivationMax Loss', -2224.673),
 ('L-6.0 Norm Loss', 1.0686722),
 ('TV(2.0) Loss', 1139.8807)], overall loss: -1083.7237548828125
Iteration: 662, named\_losses: [('ActivationMax Loss', -2231.8901),
 ('L-6.0 Norm Loss', 1.0650501),
 ('TV(2.0) Loss', 1148.8141)], overall loss: -1082.0111083984375
Iteration: 663, named\_losses: [('ActivationMax Loss', -2221.6584),
 ('L-6.0 Norm Loss', 1.070016),
 ('TV(2.0) Loss', 1140.1838)], overall loss: -1080.404541015625
Iteration: 664, named\_losses: [('ActivationMax Loss', -2230.8743),
 ('L-6.0 Norm Loss', 1.0679919),
 ('TV(2.0) Loss', 1147.8181)], overall loss: -1081.98828125
Iteration: 665, named\_losses: [('ActivationMax Loss', -2224.134),
 ('L-6.0 Norm Loss', 1.069488),
 ('TV(2.0) Loss', 1143.4579)], overall loss: -1079.6065673828125
Iteration: 666, named\_losses: [('ActivationMax Loss', -2230.4436),
 ('L-6.0 Norm Loss', 1.0668902),
 ('TV(2.0) Loss', 1148.6564)], overall loss: -1080.7203369140625
Iteration: 667, named\_losses: [('ActivationMax Loss', -2220.708),
 ('L-6.0 Norm Loss', 1.0667562),
 ('TV(2.0) Loss', 1140.7272)], overall loss: -1078.9141845703125
Iteration: 668, named\_losses: [('ActivationMax Loss', -2237.2883),
 ('L-6.0 Norm Loss', 1.0696068),
 ('TV(2.0) Loss', 1152.3406)], overall loss: -1083.878173828125
Iteration: 669, named\_losses: [('ActivationMax Loss', -2228.1553),
 ('L-6.0 Norm Loss', 1.0681568),
 ('TV(2.0) Loss', 1143.9979)], overall loss: -1083.0892333984375
Iteration: 670, named\_losses: [('ActivationMax Loss', -2241.3918),
 ('L-6.0 Norm Loss', 1.069451),
 ('TV(2.0) Loss', 1152.0773)], overall loss: -1088.2452392578125
Iteration: 671, named\_losses: [('ActivationMax Loss', -2227.2734),
 ('L-6.0 Norm Loss', 1.0694953),
 ('TV(2.0) Loss', 1146.1124)], overall loss: -1080.0914306640625
Iteration: 672, named\_losses: [('ActivationMax Loss', -2236.3015),
 ('L-6.0 Norm Loss', 1.068065),
 ('TV(2.0) Loss', 1152.7792)], overall loss: -1082.4542236328125
Iteration: 673, named\_losses: [('ActivationMax Loss', -2223.7043),
 ('L-6.0 Norm Loss', 1.0659128),
 ('TV(2.0) Loss', 1143.8718)], overall loss: -1078.7666015625
Iteration: 674, named\_losses: [('ActivationMax Loss', -2242.1565),
 ('L-6.0 Norm Loss', 1.0677198),
 ('TV(2.0) Loss', 1156.2767)], overall loss: -1084.8121337890625
Iteration: 675, named\_losses: [('ActivationMax Loss', -2216.6897),
 ('L-6.0 Norm Loss', 1.0700045),
 ('TV(2.0) Loss', 1138.4366)], overall loss: -1077.1829833984375
Iteration: 676, named\_losses: [('ActivationMax Loss', -2237.7764),
 ('L-6.0 Norm Loss', 1.069284),
 ('TV(2.0) Loss', 1153.9026)], overall loss: -1082.804443359375
Iteration: 677, named\_losses: [('ActivationMax Loss', -2227.0254),
 ('L-6.0 Norm Loss', 1.0674665),
 ('TV(2.0) Loss', 1142.6106)], overall loss: -1083.347412109375
Iteration: 678, named\_losses: [('ActivationMax Loss', -2237.4358),
 ('L-6.0 Norm Loss', 1.0654917),
 ('TV(2.0) Loss', 1150.0381)], overall loss: -1086.332275390625
Iteration: 679, named\_losses: [('ActivationMax Loss', -2219.7063),
 ('L-6.0 Norm Loss', 1.0709726),
 ('TV(2.0) Loss', 1141.9562)], overall loss: -1076.6790771484375
Iteration: 680, named\_losses: [('ActivationMax Loss', -2239.5417),
 ('L-6.0 Norm Loss', 1.066105),
 ('TV(2.0) Loss', 1151.9324)], overall loss: -1086.543212890625
Iteration: 681, named\_losses: [('ActivationMax Loss', -2227.3357),
 ('L-6.0 Norm Loss', 1.0685959),
 ('TV(2.0) Loss', 1145.0304)], overall loss: -1081.2366943359375
Iteration: 682, named\_losses: [('ActivationMax Loss', -2233.5369),
 ('L-6.0 Norm Loss', 1.0663487),
 ('TV(2.0) Loss', 1147.8567)], overall loss: -1084.61376953125
Iteration: 683, named\_losses: [('ActivationMax Loss', -2223.055),
 ('L-6.0 Norm Loss', 1.0688856),
 ('TV(2.0) Loss', 1143.8663)], overall loss: -1078.1197509765625
Iteration: 684, named\_losses: [('ActivationMax Loss', -2228.2747),
 ('L-6.0 Norm Loss', 1.0660489),
 ('TV(2.0) Loss', 1145.8861)], overall loss: -1081.3223876953125
Iteration: 685, named\_losses: [('ActivationMax Loss', -2225.0417),
 ('L-6.0 Norm Loss', 1.0695176),
 ('TV(2.0) Loss', 1140.6134)], overall loss: -1083.3587646484375
Iteration: 686, named\_losses: [('ActivationMax Loss', -2231.2168),
 ('L-6.0 Norm Loss', 1.0680795),
 ('TV(2.0) Loss', 1145.0125)], overall loss: -1085.13623046875
Iteration: 687, named\_losses: [('ActivationMax Loss', -2224.462),
 ('L-6.0 Norm Loss', 1.0675666),
 ('TV(2.0) Loss', 1141.4203)], overall loss: -1081.9739990234375
Iteration: 688, named\_losses: [('ActivationMax Loss', -2234.547),
 ('L-6.0 Norm Loss', 1.0684767),
 ('TV(2.0) Loss', 1154.3391)], overall loss: -1079.1396484375
Iteration: 689, named\_losses: [('ActivationMax Loss', -2229.1172),
 ('L-6.0 Norm Loss', 1.0687788),
 ('TV(2.0) Loss', 1148.0485)], overall loss: -1079.9998779296875
Iteration: 690, named\_losses: [('ActivationMax Loss', -2224.939),
 ('L-6.0 Norm Loss', 1.0659385),
 ('TV(2.0) Loss', 1146.8583)], overall loss: -1077.0147705078125
Iteration: 691, named\_losses: [('ActivationMax Loss', -2223.2053),
 ('L-6.0 Norm Loss', 1.0678643),
 ('TV(2.0) Loss', 1141.6631)], overall loss: -1080.474365234375
Iteration: 692, named\_losses: [('ActivationMax Loss', -2230.0208),
 ('L-6.0 Norm Loss', 1.0684068),
 ('TV(2.0) Loss', 1151.6198)], overall loss: -1077.3326416015625
Iteration: 693, named\_losses: [('ActivationMax Loss', -2223.0261),
 ('L-6.0 Norm Loss', 1.0666997),
 ('TV(2.0) Loss', 1142.2744)], overall loss: -1079.68505859375
Iteration: 694, named\_losses: [('ActivationMax Loss', -2226.7385),
 ('L-6.0 Norm Loss', 1.0684805),
 ('TV(2.0) Loss', 1140.2552)], overall loss: -1085.4149169921875
Iteration: 695, named\_losses: [('ActivationMax Loss', -2226.1748),
 ('L-6.0 Norm Loss', 1.0678205),
 ('TV(2.0) Loss', 1137.393)], overall loss: -1087.7139892578125
Iteration: 696, named\_losses: [('ActivationMax Loss', -2230.3398),
 ('L-6.0 Norm Loss', 1.0701189),
 ('TV(2.0) Loss', 1146.286)], overall loss: -1082.9837646484375
Iteration: 697, named\_losses: [('ActivationMax Loss', -2222.9937),
 ('L-6.0 Norm Loss', 1.0684204),
 ('TV(2.0) Loss', 1142.1195)], overall loss: -1079.8057861328125
Iteration: 698, named\_losses: [('ActivationMax Loss', -2231.5415),
 ('L-6.0 Norm Loss', 1.0692065),
 ('TV(2.0) Loss', 1150.5887)], overall loss: -1079.8836669921875
Iteration: 699, named\_losses: [('ActivationMax Loss', -2223.9946),
 ('L-6.0 Norm Loss', 1.0671091),
 ('TV(2.0) Loss', 1140.9403)], overall loss: -1081.9871826171875
Iteration: 700, named\_losses: [('ActivationMax Loss', -2229.9473),
 ('L-6.0 Norm Loss', 1.0698369),
 ('TV(2.0) Loss', 1149.3147)], overall loss: -1079.562744140625
Iteration: 701, named\_losses: [('ActivationMax Loss', -2230.9602),
 ('L-6.0 Norm Loss', 1.0710351),
 ('TV(2.0) Loss', 1144.8416)], overall loss: -1085.047607421875
Iteration: 702, named\_losses: [('ActivationMax Loss', -2233.7605),
 ('L-6.0 Norm Loss', 1.0685601),
 ('TV(2.0) Loss', 1154.2788)], overall loss: -1078.4130859375
Iteration: 703, named\_losses: [('ActivationMax Loss', -2226.3806),
 ('L-6.0 Norm Loss', 1.0696946),
 ('TV(2.0) Loss', 1141.8567)], overall loss: -1083.454345703125
Iteration: 704, named\_losses: [('ActivationMax Loss', -2234.8955),
 ('L-6.0 Norm Loss', 1.0669795),
 ('TV(2.0) Loss', 1150.2965)], overall loss: -1083.5321044921875
Iteration: 705, named\_losses: [('ActivationMax Loss', -2222.8381),
 ('L-6.0 Norm Loss', 1.0696578),
 ('TV(2.0) Loss', 1137.5265)], overall loss: -1084.2420654296875
Iteration: 706, named\_losses: [('ActivationMax Loss', -2237.8228),
 ('L-6.0 Norm Loss', 1.0678157),
 ('TV(2.0) Loss', 1150.0662)], overall loss: -1086.688720703125
Iteration: 707, named\_losses: [('ActivationMax Loss', -2227.0283),
 ('L-6.0 Norm Loss', 1.0684843),
 ('TV(2.0) Loss', 1144.3097)], overall loss: -1081.6500244140625
Iteration: 708, named\_losses: [('ActivationMax Loss', -2235.9607),
 ('L-6.0 Norm Loss', 1.0640314),
 ('TV(2.0) Loss', 1150.9534)], overall loss: -1083.943359375
Iteration: 709, named\_losses: [('ActivationMax Loss', -2225.5076),
 ('L-6.0 Norm Loss', 1.0674021),
 ('TV(2.0) Loss', 1143.8234)], overall loss: -1080.6168212890625
Iteration: 710, named\_losses: [('ActivationMax Loss', -2246.007),
 ('L-6.0 Norm Loss', 1.0711031),
 ('TV(2.0) Loss', 1159.7137)], overall loss: -1085.2222900390625
Iteration: 711, named\_losses: [('ActivationMax Loss', -2224.7725),
 ('L-6.0 Norm Loss', 1.0654924),
 ('TV(2.0) Loss', 1148.6665)], overall loss: -1075.04052734375
Iteration: 712, named\_losses: [('ActivationMax Loss', -2243.0002),
 ('L-6.0 Norm Loss', 1.069275),
 ('TV(2.0) Loss', 1159.3486)], overall loss: -1082.582275390625
Iteration: 713, named\_losses: [('ActivationMax Loss', -2236.5825),
 ('L-6.0 Norm Loss', 1.0702024),
 ('TV(2.0) Loss', 1150.836)], overall loss: -1084.6761474609375
Iteration: 714, named\_losses: [('ActivationMax Loss', -2234.9473),
 ('L-6.0 Norm Loss', 1.0685974),
 ('TV(2.0) Loss', 1153.4242)], overall loss: -1080.4544677734375
Iteration: 715, named\_losses: [('ActivationMax Loss', -2227.5955),
 ('L-6.0 Norm Loss', 1.065048),
 ('TV(2.0) Loss', 1147.0948)], overall loss: -1079.4356689453125
Iteration: 716, named\_losses: [('ActivationMax Loss', -2234.748),
 ('L-6.0 Norm Loss', 1.0694696),
 ('TV(2.0) Loss', 1150.4796)], overall loss: -1083.1988525390625
Iteration: 717, named\_losses: [('ActivationMax Loss', -2230.8423),
 ('L-6.0 Norm Loss', 1.0708867),
 ('TV(2.0) Loss', 1148.7804)], overall loss: -1080.9910888671875
Iteration: 718, named\_losses: [('ActivationMax Loss', -2233.7134),
 ('L-6.0 Norm Loss', 1.0666182),
 ('TV(2.0) Loss', 1152.8048)], overall loss: -1079.8419189453125
Iteration: 719, named\_losses: [('ActivationMax Loss', -2228.027),
 ('L-6.0 Norm Loss', 1.068296),
 ('TV(2.0) Loss', 1146.9948)], overall loss: -1079.9639892578125
Iteration: 720, named\_losses: [('ActivationMax Loss', -2229.961),
 ('L-6.0 Norm Loss', 1.0658439),
 ('TV(2.0) Loss', 1143.7997)], overall loss: -1085.0953369140625
Iteration: 721, named\_losses: [('ActivationMax Loss', -2228.9185),
 ('L-6.0 Norm Loss', 1.0660866),
 ('TV(2.0) Loss', 1144.5648)], overall loss: -1083.2874755859375
Iteration: 722, named\_losses: [('ActivationMax Loss', -2239.501),
 ('L-6.0 Norm Loss', 1.0680856),
 ('TV(2.0) Loss', 1154.1124)], overall loss: -1084.3204345703125
Iteration: 723, named\_losses: [('ActivationMax Loss', -2235.7483),
 ('L-6.0 Norm Loss', 1.067373),
 ('TV(2.0) Loss', 1155.9916)], overall loss: -1078.6893310546875
Iteration: 724, named\_losses: [('ActivationMax Loss', -2234.6992),
 ('L-6.0 Norm Loss', 1.0666788),
 ('TV(2.0) Loss', 1153.0707)], overall loss: -1080.5618896484375
Iteration: 725, named\_losses: [('ActivationMax Loss', -2227.171),
 ('L-6.0 Norm Loss', 1.0685022),
 ('TV(2.0) Loss', 1147.5616)], overall loss: -1078.5406494140625
Iteration: 726, named\_losses: [('ActivationMax Loss', -2231.6772),
 ('L-6.0 Norm Loss', 1.0688063),
 ('TV(2.0) Loss', 1144.6432)], overall loss: -1085.9652099609375
Iteration: 727, named\_losses: [('ActivationMax Loss', -2225.4502),
 ('L-6.0 Norm Loss', 1.0675564),
 ('TV(2.0) Loss', 1145.3231)], overall loss: -1079.0594482421875
Iteration: 728, named\_losses: [('ActivationMax Loss', -2230.6184),
 ('L-6.0 Norm Loss', 1.0651428),
 ('TV(2.0) Loss', 1145.8275)], overall loss: -1083.7257080078125
Iteration: 729, named\_losses: [('ActivationMax Loss', -2228.1184),
 ('L-6.0 Norm Loss', 1.0676545),
 ('TV(2.0) Loss', 1146.4963)], overall loss: -1080.554443359375
Iteration: 730, named\_losses: [('ActivationMax Loss', -2228.9631),
 ('L-6.0 Norm Loss', 1.0668197),
 ('TV(2.0) Loss', 1144.0531)], overall loss: -1083.8431396484375
Iteration: 731, named\_losses: [('ActivationMax Loss', -2226.9207),
 ('L-6.0 Norm Loss', 1.0671586),
 ('TV(2.0) Loss', 1143.1426)], overall loss: -1082.7109375
Iteration: 732, named\_losses: [('ActivationMax Loss', -2235.2778),
 ('L-6.0 Norm Loss', 1.0666182),
 ('TV(2.0) Loss', 1149.588)], overall loss: -1084.6231689453125
Iteration: 733, named\_losses: [('ActivationMax Loss', -2230.189),
 ('L-6.0 Norm Loss', 1.0674244),
 ('TV(2.0) Loss', 1148.6343)], overall loss: -1080.4873046875
Iteration: 734, named\_losses: [('ActivationMax Loss', -2231.1912),
 ('L-6.0 Norm Loss', 1.068253),
 ('TV(2.0) Loss', 1145.9515)], overall loss: -1084.1712646484375
Iteration: 735, named\_losses: [('ActivationMax Loss', -2224.8154),
 ('L-6.0 Norm Loss', 1.064336),
 ('TV(2.0) Loss', 1138.4714)], overall loss: -1085.279541015625
Iteration: 736, named\_losses: [('ActivationMax Loss', -2237.8682),
 ('L-6.0 Norm Loss', 1.068275),
 ('TV(2.0) Loss', 1152.2036)], overall loss: -1084.59619140625
Iteration: 737, named\_losses: [('ActivationMax Loss', -2232.1497),
 ('L-6.0 Norm Loss', 1.0696967),
 ('TV(2.0) Loss', 1144.7252)], overall loss: -1086.3548583984375
Iteration: 738, named\_losses: [('ActivationMax Loss', -2240.6865),
 ('L-6.0 Norm Loss', 1.0689288),
 ('TV(2.0) Loss', 1156.805)], overall loss: -1082.8126220703125
Iteration: 739, named\_losses: [('ActivationMax Loss', -2222.452),
 ('L-6.0 Norm Loss', 1.0682001),
 ('TV(2.0) Loss', 1140.3475)], overall loss: -1081.0362548828125
Iteration: 740, named\_losses: [('ActivationMax Loss', -2227.4133),
 ('L-6.0 Norm Loss', 1.0661114),
 ('TV(2.0) Loss', 1149.0757)], overall loss: -1077.271484375
Iteration: 741, named\_losses: [('ActivationMax Loss', -2222.2383),
 ('L-6.0 Norm Loss', 1.0674262),
 ('TV(2.0) Loss', 1137.6346)], overall loss: -1083.5362548828125
Iteration: 742, named\_losses: [('ActivationMax Loss', -2233.5044),
 ('L-6.0 Norm Loss', 1.0666994),
 ('TV(2.0) Loss', 1148.597)], overall loss: -1083.8406982421875
Iteration: 743, named\_losses: [('ActivationMax Loss', -2222.4163),
 ('L-6.0 Norm Loss', 1.0672231),
 ('TV(2.0) Loss', 1139.478)], overall loss: -1081.87109375
Iteration: 744, named\_losses: [('ActivationMax Loss', -2229.3096),
 ('L-6.0 Norm Loss', 1.064767),
 ('TV(2.0) Loss', 1146.0109)], overall loss: -1082.2340087890625
Iteration: 745, named\_losses: [('ActivationMax Loss', -2218.0347),
 ('L-6.0 Norm Loss', 1.0671473),
 ('TV(2.0) Loss', 1141.2418)], overall loss: -1075.7257080078125
Iteration: 746, named\_losses: [('ActivationMax Loss', -2232.4963),
 ('L-6.0 Norm Loss', 1.0688149),
 ('TV(2.0) Loss', 1148.6267)], overall loss: -1082.80078125
Iteration: 747, named\_losses: [('ActivationMax Loss', -2227.5823),
 ('L-6.0 Norm Loss', 1.0684482),
 ('TV(2.0) Loss', 1145.5322)], overall loss: -1080.981689453125
Iteration: 748, named\_losses: [('ActivationMax Loss', -2236.4644),
 ('L-6.0 Norm Loss', 1.0666323),
 ('TV(2.0) Loss', 1150.2335)], overall loss: -1085.1641845703125
Iteration: 749, named\_losses: [('ActivationMax Loss', -2228.9165),
 ('L-6.0 Norm Loss', 1.0668494),
 ('TV(2.0) Loss', 1147.5122)], overall loss: -1080.33740234375
Iteration: 750, named\_losses: [('ActivationMax Loss', -2233.2053),
 ('L-6.0 Norm Loss', 1.0677869),
 ('TV(2.0) Loss', 1143.7786)], overall loss: -1088.35888671875
Iteration: 751, named\_losses: [('ActivationMax Loss', -2218.216),
 ('L-6.0 Norm Loss', 1.0686083),
 ('TV(2.0) Loss', 1141.047)], overall loss: -1076.1004638671875
Iteration: 752, named\_losses: [('ActivationMax Loss', -2237.9148),
 ('L-6.0 Norm Loss', 1.0675144),
 ('TV(2.0) Loss', 1149.0752)], overall loss: -1087.77197265625
Iteration: 753, named\_losses: [('ActivationMax Loss', -2226.5571),
 ('L-6.0 Norm Loss', 1.0661042),
 ('TV(2.0) Loss', 1146.3296)], overall loss: -1079.161376953125
Iteration: 754, named\_losses: [('ActivationMax Loss', -2243.3245),
 ('L-6.0 Norm Loss', 1.065746),
 ('TV(2.0) Loss', 1156.8455)], overall loss: -1085.413330078125
Iteration: 755, named\_losses: [('ActivationMax Loss', -2224.923),
 ('L-6.0 Norm Loss', 1.0679203),
 ('TV(2.0) Loss', 1141.281)], overall loss: -1082.57421875
Iteration: 756, named\_losses: [('ActivationMax Loss', -2235.0264),
 ('L-6.0 Norm Loss', 1.066339),
 ('TV(2.0) Loss', 1148.545)], overall loss: -1085.4149169921875
Iteration: 757, named\_losses: [('ActivationMax Loss', -2226.8909),
 ('L-6.0 Norm Loss', 1.0682077),
 ('TV(2.0) Loss', 1145.4598)], overall loss: -1080.3629150390625
Iteration: 758, named\_losses: [('ActivationMax Loss', -2233.7205),
 ('L-6.0 Norm Loss', 1.0668026),
 ('TV(2.0) Loss', 1153.2234)], overall loss: -1079.43017578125
Iteration: 759, named\_losses: [('ActivationMax Loss', -2232.1494),
 ('L-6.0 Norm Loss', 1.0682577),
 ('TV(2.0) Loss', 1146.2039)], overall loss: -1084.877197265625
Iteration: 760, named\_losses: [('ActivationMax Loss', -2223.1843),
 ('L-6.0 Norm Loss', 1.0661443),
 ('TV(2.0) Loss', 1141.2678)], overall loss: -1080.850341796875
Iteration: 761, named\_losses: [('ActivationMax Loss', -2231.293),
 ('L-6.0 Norm Loss', 1.0698038),
 ('TV(2.0) Loss', 1146.8885)], overall loss: -1083.3345947265625
Iteration: 762, named\_losses: [('ActivationMax Loss', -2234.0923),
 ('L-6.0 Norm Loss', 1.0662273),
 ('TV(2.0) Loss', 1151.4031)], overall loss: -1081.623046875
Iteration: 763, named\_losses: [('ActivationMax Loss', -2226.4956),
 ('L-6.0 Norm Loss', 1.0670402),
 ('TV(2.0) Loss', 1143.5474)], overall loss: -1081.881103515625
Iteration: 764, named\_losses: [('ActivationMax Loss', -2225.102),
 ('L-6.0 Norm Loss', 1.0695351),
 ('TV(2.0) Loss', 1141.3557)], overall loss: -1082.6767578125
Iteration: 765, named\_losses: [('ActivationMax Loss', -2228.26),
 ('L-6.0 Norm Loss', 1.0675468),
 ('TV(2.0) Loss', 1148.7992)], overall loss: -1078.3931884765625
Iteration: 766, named\_losses: [('ActivationMax Loss', -2228.4634),
 ('L-6.0 Norm Loss', 1.0665553),
 ('TV(2.0) Loss', 1145.5598)], overall loss: -1081.8369140625
Iteration: 767, named\_losses: [('ActivationMax Loss', -2232.5486),
 ('L-6.0 Norm Loss', 1.0706563),
 ('TV(2.0) Loss', 1152.6819)], overall loss: -1078.796142578125
Iteration: 768, named\_losses: [('ActivationMax Loss', -2230.163),
 ('L-6.0 Norm Loss', 1.0706096),
 ('TV(2.0) Loss', 1146.1262)], overall loss: -1082.96630859375
Iteration: 769, named\_losses: [('ActivationMax Loss', -2229.9946),
 ('L-6.0 Norm Loss', 1.0650873),
 ('TV(2.0) Loss', 1150.2897)], overall loss: -1078.6397705078125
Iteration: 770, named\_losses: [('ActivationMax Loss', -2225.1826),
 ('L-6.0 Norm Loss', 1.0678643),
 ('TV(2.0) Loss', 1138.8323)], overall loss: -1085.282470703125
Iteration: 771, named\_losses: [('ActivationMax Loss', -2235.6643),
 ('L-6.0 Norm Loss', 1.0682275),
 ('TV(2.0) Loss', 1147.5895)], overall loss: -1087.0067138671875
Iteration: 772, named\_losses: [('ActivationMax Loss', -2218.9187),
 ('L-6.0 Norm Loss', 1.068989),
 ('TV(2.0) Loss', 1138.8344)], overall loss: -1079.0152587890625
Iteration: 773, named\_losses: [('ActivationMax Loss', -2233.9126),
 ('L-6.0 Norm Loss', 1.0684824),
 ('TV(2.0) Loss', 1154.0194)], overall loss: -1078.8245849609375
Iteration: 774, named\_losses: [('ActivationMax Loss', -2225.4355),
 ('L-6.0 Norm Loss', 1.0689907),
 ('TV(2.0) Loss', 1144.638)], overall loss: -1079.728515625
Iteration: 775, named\_losses: [('ActivationMax Loss', -2233.7637),
 ('L-6.0 Norm Loss', 1.0645591),
 ('TV(2.0) Loss', 1149.437)], overall loss: -1083.26220703125
Iteration: 776, named\_losses: [('ActivationMax Loss', -2222.315),
 ('L-6.0 Norm Loss', 1.0695311),
 ('TV(2.0) Loss', 1139.2369)], overall loss: -1082.0084228515625
Iteration: 777, named\_losses: [('ActivationMax Loss', -2238.5996),
 ('L-6.0 Norm Loss', 1.069352),
 ('TV(2.0) Loss', 1159.5754)], overall loss: -1077.954833984375
Iteration: 778, named\_losses: [('ActivationMax Loss', -2225.2903),
 ('L-6.0 Norm Loss', 1.0704451),
 ('TV(2.0) Loss', 1145.7631)], overall loss: -1078.4566650390625
Iteration: 779, named\_losses: [('ActivationMax Loss', -2234.7441),
 ('L-6.0 Norm Loss', 1.0634305),
 ('TV(2.0) Loss', 1149.2361)], overall loss: -1084.444580078125
Iteration: 780, named\_losses: [('ActivationMax Loss', -2235.0295),
 ('L-6.0 Norm Loss', 1.0674276),
 ('TV(2.0) Loss', 1147.3242)], overall loss: -1086.637939453125
Iteration: 781, named\_losses: [('ActivationMax Loss', -2232.7007),
 ('L-6.0 Norm Loss', 1.0647395),
 ('TV(2.0) Loss', 1149.0673)], overall loss: -1082.5687255859375
Iteration: 782, named\_losses: [('ActivationMax Loss', -2224.2136),
 ('L-6.0 Norm Loss', 1.0695978),
 ('TV(2.0) Loss', 1143.4531)], overall loss: -1079.69091796875
Iteration: 783, named\_losses: [('ActivationMax Loss', -2227.989),
 ('L-6.0 Norm Loss', 1.0656084),
 ('TV(2.0) Loss', 1142.6757)], overall loss: -1084.2476806640625
Iteration: 784, named\_losses: [('ActivationMax Loss', -2220.9324),
 ('L-6.0 Norm Loss', 1.06838),
 ('TV(2.0) Loss', 1136.5918)], overall loss: -1083.272216796875
Iteration: 785, named\_losses: [('ActivationMax Loss', -2232.0796),
 ('L-6.0 Norm Loss', 1.0627637),
 ('TV(2.0) Loss', 1147.4562)], overall loss: -1083.5606689453125
Iteration: 786, named\_losses: [('ActivationMax Loss', -2234.113),
 ('L-6.0 Norm Loss', 1.0691891),
 ('TV(2.0) Loss', 1146.9469)], overall loss: -1086.0970458984375
Iteration: 787, named\_losses: [('ActivationMax Loss', -2237.7686),
 ('L-6.0 Norm Loss', 1.065327),
 ('TV(2.0) Loss', 1155.5364)], overall loss: -1081.166748046875
Iteration: 788, named\_losses: [('ActivationMax Loss', -2231.0693),
 ('L-6.0 Norm Loss', 1.0691952),
 ('TV(2.0) Loss', 1147.1458)], overall loss: -1082.8544921875
Iteration: 789, named\_losses: [('ActivationMax Loss', -2235.4338),
 ('L-6.0 Norm Loss', 1.068464),
 ('TV(2.0) Loss', 1153.8157)], overall loss: -1080.5498046875
Iteration: 790, named\_losses: [('ActivationMax Loss', -2226.25),
 ('L-6.0 Norm Loss', 1.0714239),
 ('TV(2.0) Loss', 1144.7947)], overall loss: -1080.3837890625
Iteration: 791, named\_losses: [('ActivationMax Loss', -2231.5369),
 ('L-6.0 Norm Loss', 1.0666901),
 ('TV(2.0) Loss', 1145.0948)], overall loss: -1085.3753662109375
Iteration: 792, named\_losses: [('ActivationMax Loss', -2225.4258),
 ('L-6.0 Norm Loss', 1.0690708),
 ('TV(2.0) Loss', 1142.2864)], overall loss: -1082.0703125
Iteration: 793, named\_losses: [('ActivationMax Loss', -2235.3503),
 ('L-6.0 Norm Loss', 1.0664077),
 ('TV(2.0) Loss', 1149.7411)], overall loss: -1084.5428466796875
Iteration: 794, named\_losses: [('ActivationMax Loss', -2224.1768),
 ('L-6.0 Norm Loss', 1.068387),
 ('TV(2.0) Loss', 1144.431)], overall loss: -1078.6773681640625
Iteration: 795, named\_losses: [('ActivationMax Loss', -2234.1047),
 ('L-6.0 Norm Loss', 1.0658602),
 ('TV(2.0) Loss', 1150.9935)], overall loss: -1082.0452880859375
Iteration: 796, named\_losses: [('ActivationMax Loss', -2217.9363),
 ('L-6.0 Norm Loss', 1.0670063),
 ('TV(2.0) Loss', 1144.392)], overall loss: -1072.4774169921875
Iteration: 797, named\_losses: [('ActivationMax Loss', -2237.2722),
 ('L-6.0 Norm Loss', 1.0679514),
 ('TV(2.0) Loss', 1149.4396)], overall loss: -1086.7647705078125
Iteration: 798, named\_losses: [('ActivationMax Loss', -2219.7217),
 ('L-6.0 Norm Loss', 1.0661272),
 ('TV(2.0) Loss', 1144.4819)], overall loss: -1074.173583984375
Iteration: 799, named\_losses: [('ActivationMax Loss', -2234.6511),
 ('L-6.0 Norm Loss', 1.0669497),
 ('TV(2.0) Loss', 1148.0913)], overall loss: -1085.492919921875
Iteration: 800, named\_losses: [('ActivationMax Loss', -2228.788),
 ('L-6.0 Norm Loss', 1.0680646),
 ('TV(2.0) Loss', 1148.376)], overall loss: -1079.343994140625
Iteration: 801, named\_losses: [('ActivationMax Loss', -2239.2222),
 ('L-6.0 Norm Loss', 1.0696988),
 ('TV(2.0) Loss', 1151.5809)], overall loss: -1086.5716552734375
Iteration: 802, named\_losses: [('ActivationMax Loss', -2226.268),
 ('L-6.0 Norm Loss', 1.0692943),
 ('TV(2.0) Loss', 1149.3353)], overall loss: -1075.8634033203125
Iteration: 803, named\_losses: [('ActivationMax Loss', -2235.7039),
 ('L-6.0 Norm Loss', 1.0655077),
 ('TV(2.0) Loss', 1149.7861)], overall loss: -1084.852294921875
Iteration: 804, named\_losses: [('ActivationMax Loss', -2223.3452),
 ('L-6.0 Norm Loss', 1.0698794),
 ('TV(2.0) Loss', 1141.4429)], overall loss: -1080.83251953125
Iteration: 805, named\_losses: [('ActivationMax Loss', -2236.176),
 ('L-6.0 Norm Loss', 1.0674192),
 ('TV(2.0) Loss', 1147.0818)], overall loss: -1088.02685546875
Iteration: 806, named\_losses: [('ActivationMax Loss', -2228.2502),
 ('L-6.0 Norm Loss', 1.0697346),
 ('TV(2.0) Loss', 1148.0317)], overall loss: -1079.148681640625
Iteration: 807, named\_losses: [('ActivationMax Loss', -2233.1475),
 ('L-6.0 Norm Loss', 1.0627944),
 ('TV(2.0) Loss', 1146.2767)], overall loss: -1085.8079833984375
Iteration: 808, named\_losses: [('ActivationMax Loss', -2231.2107),
 ('L-6.0 Norm Loss', 1.0680022),
 ('TV(2.0) Loss', 1152.1034)], overall loss: -1078.0391845703125
Iteration: 809, named\_losses: [('ActivationMax Loss', -2235.194),
 ('L-6.0 Norm Loss', 1.0680661),
 ('TV(2.0) Loss', 1146.3136)], overall loss: -1087.8123779296875
Iteration: 810, named\_losses: [('ActivationMax Loss', -2225.983),
 ('L-6.0 Norm Loss', 1.069212),
 ('TV(2.0) Loss', 1144.253)], overall loss: -1080.6607666015625
Iteration: 811, named\_losses: [('ActivationMax Loss', -2231.4),
 ('L-6.0 Norm Loss', 1.0635867),
 ('TV(2.0) Loss', 1141.7926)], overall loss: -1088.5438232421875
Iteration: 812, named\_losses: [('ActivationMax Loss', -2227.154),
 ('L-6.0 Norm Loss', 1.0692478),
 ('TV(2.0) Loss', 1148.8052)], overall loss: -1077.279541015625
Iteration: 813, named\_losses: [('ActivationMax Loss', -2233.1655),
 ('L-6.0 Norm Loss', 1.0667528),
 ('TV(2.0) Loss', 1144.14)], overall loss: -1087.9588623046875
Iteration: 814, named\_losses: [('ActivationMax Loss', -2230.5923),
 ('L-6.0 Norm Loss', 1.0705434),
 ('TV(2.0) Loss', 1149.3542)], overall loss: -1080.16748046875
Iteration: 815, named\_losses: [('ActivationMax Loss', -2232.0388),
 ('L-6.0 Norm Loss', 1.0651287),
 ('TV(2.0) Loss', 1148.0242)], overall loss: -1082.949462890625
Iteration: 816, named\_losses: [('ActivationMax Loss', -2227.8354),
 ('L-6.0 Norm Loss', 1.0676285),
 ('TV(2.0) Loss', 1147.3375)], overall loss: -1079.4302978515625
Iteration: 817, named\_losses: [('ActivationMax Loss', -2226.4),
 ('L-6.0 Norm Loss', 1.0687797),
 ('TV(2.0) Loss', 1141.4688)], overall loss: -1083.8623046875
Iteration: 818, named\_losses: [('ActivationMax Loss', -2233.5134),
 ('L-6.0 Norm Loss', 1.0701587),
 ('TV(2.0) Loss', 1146.858)], overall loss: -1085.5853271484375
Iteration: 819, named\_losses: [('ActivationMax Loss', -2223.7366),
 ('L-6.0 Norm Loss', 1.0694104),
 ('TV(2.0) Loss', 1137.791)], overall loss: -1084.876220703125
Iteration: 820, named\_losses: [('ActivationMax Loss', -2235.8115),
 ('L-6.0 Norm Loss', 1.069132),
 ('TV(2.0) Loss', 1152.394)], overall loss: -1082.348388671875
Iteration: 821, named\_losses: [('ActivationMax Loss', -2221.8794),
 ('L-6.0 Norm Loss', 1.0669614),
 ('TV(2.0) Loss', 1136.9153)], overall loss: -1083.897216796875
Iteration: 822, named\_losses: [('ActivationMax Loss', -2237.129),
 ('L-6.0 Norm Loss', 1.0700495),
 ('TV(2.0) Loss', 1154.1315)], overall loss: -1081.9273681640625
Iteration: 823, named\_losses: [('ActivationMax Loss', -2228.1477),
 ('L-6.0 Norm Loss', 1.0682061),
 ('TV(2.0) Loss', 1147.1764)], overall loss: -1079.9031982421875
Iteration: 824, named\_losses: [('ActivationMax Loss', -2235.2087),
 ('L-6.0 Norm Loss', 1.0689933),
 ('TV(2.0) Loss', 1149.9836)], overall loss: -1084.156005859375
Iteration: 825, named\_losses: [('ActivationMax Loss', -2217.589),
 ('L-6.0 Norm Loss', 1.0726092),
 ('TV(2.0) Loss', 1136.1997)], overall loss: -1080.31689453125
Iteration: 826, named\_losses: [('ActivationMax Loss', -2235.987),
 ('L-6.0 Norm Loss', 1.0659852),
 ('TV(2.0) Loss', 1148.8386)], overall loss: -1086.08251953125
Iteration: 827, named\_losses: [('ActivationMax Loss', -2215.2058),
 ('L-6.0 Norm Loss', 1.0703299),
 ('TV(2.0) Loss', 1134.3331)], overall loss: -1079.8023681640625
Iteration: 828, named\_losses: [('ActivationMax Loss', -2231.8098),
 ('L-6.0 Norm Loss', 1.0687865),
 ('TV(2.0) Loss', 1147.4844)], overall loss: -1083.256591796875
Iteration: 829, named\_losses: [('ActivationMax Loss', -2219.9705),
 ('L-6.0 Norm Loss', 1.0683511),
 ('TV(2.0) Loss', 1141.3047)], overall loss: -1077.597412109375
Iteration: 830, named\_losses: [('ActivationMax Loss', -2241.0696),
 ('L-6.0 Norm Loss', 1.0697472),
 ('TV(2.0) Loss', 1152.9862)], overall loss: -1087.0135498046875
Iteration: 831, named\_losses: [('ActivationMax Loss', -2226.5781),
 ('L-6.0 Norm Loss', 1.0715871),
 ('TV(2.0) Loss', 1145.5494)], overall loss: -1079.9571533203125
Iteration: 832, named\_losses: [('ActivationMax Loss', -2244.1301),
 ('L-6.0 Norm Loss', 1.0719538),
 ('TV(2.0) Loss', 1157.6523)], overall loss: -1085.40576171875
Iteration: 833, named\_losses: [('ActivationMax Loss', -2213.1116),
 ('L-6.0 Norm Loss', 1.0683323),
 ('TV(2.0) Loss', 1137.3821)], overall loss: -1074.6611328125
Iteration: 834, named\_losses: [('ActivationMax Loss', -2239.5186),
 ('L-6.0 Norm Loss', 1.0687222),
 ('TV(2.0) Loss', 1155.7607)], overall loss: -1082.689208984375
Iteration: 835, named\_losses: [('ActivationMax Loss', -2230.198),
 ('L-6.0 Norm Loss', 1.0680444),
 ('TV(2.0) Loss', 1144.2397)], overall loss: -1084.89013671875
Iteration: 836, named\_losses: [('ActivationMax Loss', -2235.525),
 ('L-6.0 Norm Loss', 1.0669328),
 ('TV(2.0) Loss', 1149.9086)], overall loss: -1084.5494384765625
Iteration: 837, named\_losses: [('ActivationMax Loss', -2222.3508),
 ('L-6.0 Norm Loss', 1.0693057),
 ('TV(2.0) Loss', 1139.8608)], overall loss: -1081.420654296875
Iteration: 838, named\_losses: [('ActivationMax Loss', -2232.7747),
 ('L-6.0 Norm Loss', 1.069145),
 ('TV(2.0) Loss', 1149.5117)], overall loss: -1082.19384765625
Iteration: 839, named\_losses: [('ActivationMax Loss', -2216.6135),
 ('L-6.0 Norm Loss', 1.0702727),
 ('TV(2.0) Loss', 1135.4249)], overall loss: -1080.1182861328125
Iteration: 840, named\_losses: [('ActivationMax Loss', -2236.9077),
 ('L-6.0 Norm Loss', 1.068568),
 ('TV(2.0) Loss', 1152.8256)], overall loss: -1083.0135498046875
Iteration: 841, named\_losses: [('ActivationMax Loss', -2225.929),
 ('L-6.0 Norm Loss', 1.0666193),
 ('TV(2.0) Loss', 1143.3751)], overall loss: -1081.4871826171875
Iteration: 842, named\_losses: [('ActivationMax Loss', -2234.7043),
 ('L-6.0 Norm Loss', 1.0670033),
 ('TV(2.0) Loss', 1151.4211)], overall loss: -1082.21630859375
Iteration: 843, named\_losses: [('ActivationMax Loss', -2219.0688),
 ('L-6.0 Norm Loss', 1.0701901),
 ('TV(2.0) Loss', 1137.9713)], overall loss: -1080.0274658203125
Iteration: 844, named\_losses: [('ActivationMax Loss', -2234.531),
 ('L-6.0 Norm Loss', 1.0696788),
 ('TV(2.0) Loss', 1150.4648)], overall loss: -1082.99658203125
Iteration: 845, named\_losses: [('ActivationMax Loss', -2231.6074),
 ('L-6.0 Norm Loss', 1.0671982),
 ('TV(2.0) Loss', 1150.0851)], overall loss: -1080.4552001953125
Iteration: 846, named\_losses: [('ActivationMax Loss', -2228.0474),
 ('L-6.0 Norm Loss', 1.0682598),
 ('TV(2.0) Loss', 1145.4435)], overall loss: -1081.5355224609375
Iteration: 847, named\_losses: [('ActivationMax Loss', -2233.3472),
 ('L-6.0 Norm Loss', 1.0689571),
 ('TV(2.0) Loss', 1151.6892)], overall loss: -1080.589111328125
Iteration: 848, named\_losses: [('ActivationMax Loss', -2227.1692),
 ('L-6.0 Norm Loss', 1.0670037),
 ('TV(2.0) Loss', 1145.8204)], overall loss: -1080.2818603515625
Iteration: 849, named\_losses: [('ActivationMax Loss', -2228.349),
 ('L-6.0 Norm Loss', 1.0677241),
 ('TV(2.0) Loss', 1145.5966)], overall loss: -1081.6849365234375
Iteration: 850, named\_losses: [('ActivationMax Loss', -2231.3047),
 ('L-6.0 Norm Loss', 1.0683874),
 ('TV(2.0) Loss', 1148.5314)], overall loss: -1081.7049560546875
Iteration: 851, named\_losses: [('ActivationMax Loss', -2232.0593),
 ('L-6.0 Norm Loss', 1.0686116),
 ('TV(2.0) Loss', 1147.2162)], overall loss: -1083.7745361328125
Iteration: 852, named\_losses: [('ActivationMax Loss', -2226.7546),
 ('L-6.0 Norm Loss', 1.0689353),
 ('TV(2.0) Loss', 1145.2388)], overall loss: -1080.447021484375
Iteration: 853, named\_losses: [('ActivationMax Loss', -2234.5024),
 ('L-6.0 Norm Loss', 1.0659621),
 ('TV(2.0) Loss', 1149.7678)], overall loss: -1083.668701171875
Iteration: 854, named\_losses: [('ActivationMax Loss', -2223.6438),
 ('L-6.0 Norm Loss', 1.0679975),
 ('TV(2.0) Loss', 1140.2612)], overall loss: -1082.314453125
Iteration: 855, named\_losses: [('ActivationMax Loss', -2223.1543),
 ('L-6.0 Norm Loss', 1.0673987),
 ('TV(2.0) Loss', 1140.2596)], overall loss: -1081.8272705078125
Iteration: 856, named\_losses: [('ActivationMax Loss', -2228.4373),
 ('L-6.0 Norm Loss', 1.0697643),
 ('TV(2.0) Loss', 1148.4059)], overall loss: -1078.9615478515625
Iteration: 857, named\_losses: [('ActivationMax Loss', -2228.8494),
 ('L-6.0 Norm Loss', 1.064014),
 ('TV(2.0) Loss', 1145.5925)], overall loss: -1082.19287109375
Iteration: 858, named\_losses: [('ActivationMax Loss', -2226.0269),
 ('L-6.0 Norm Loss', 1.0690646),
 ('TV(2.0) Loss', 1141.4773)], overall loss: -1083.48046875
Iteration: 859, named\_losses: [('ActivationMax Loss', -2236.675),
 ('L-6.0 Norm Loss', 1.0680094),
 ('TV(2.0) Loss', 1150.4995)], overall loss: -1085.107421875
Iteration: 860, named\_losses: [('ActivationMax Loss', -2227.0771),
 ('L-6.0 Norm Loss', 1.0701537),
 ('TV(2.0) Loss', 1145.127)], overall loss: -1080.880126953125
Iteration: 861, named\_losses: [('ActivationMax Loss', -2233.863),
 ('L-6.0 Norm Loss', 1.066078),
 ('TV(2.0) Loss', 1148.484)], overall loss: -1084.3128662109375
Iteration: 862, named\_losses: [('ActivationMax Loss', -2229.0618),
 ('L-6.0 Norm Loss', 1.0693955),
 ('TV(2.0) Loss', 1145.0359)], overall loss: -1082.95654296875
Iteration: 863, named\_losses: [('ActivationMax Loss', -2229.0618),
 ('L-6.0 Norm Loss', 1.0662361),
 ('TV(2.0) Loss', 1149.898)], overall loss: -1078.09765625
Iteration: 864, named\_losses: [('ActivationMax Loss', -2231.2886),
 ('L-6.0 Norm Loss', 1.0681221),
 ('TV(2.0) Loss', 1147.135)], overall loss: -1083.08544921875
Iteration: 865, named\_losses: [('ActivationMax Loss', -2233.3735),
 ('L-6.0 Norm Loss', 1.0678368),
 ('TV(2.0) Loss', 1147.887)], overall loss: -1084.418701171875
Iteration: 866, named\_losses: [('ActivationMax Loss', -2222.0466),
 ('L-6.0 Norm Loss', 1.0682906),
 ('TV(2.0) Loss', 1140.3726)], overall loss: -1080.605712890625
Iteration: 867, named\_losses: [('ActivationMax Loss', -2233.5264),
 ('L-6.0 Norm Loss', 1.0647984),
 ('TV(2.0) Loss', 1150.682)], overall loss: -1081.7796630859375
Iteration: 868, named\_losses: [('ActivationMax Loss', -2239.075),
 ('L-6.0 Norm Loss', 1.0687423),
 ('TV(2.0) Loss', 1154.4692)], overall loss: -1083.536865234375
Iteration: 869, named\_losses: [('ActivationMax Loss', -2239.699),
 ('L-6.0 Norm Loss', 1.0671505),
 ('TV(2.0) Loss', 1155.8203)], overall loss: -1082.8115234375
Iteration: 870, named\_losses: [('ActivationMax Loss', -2229.1243),
 ('L-6.0 Norm Loss', 1.0686469),
 ('TV(2.0) Loss', 1146.6519)], overall loss: -1081.40380859375
Iteration: 871, named\_losses: [('ActivationMax Loss', -2231.55),
 ('L-6.0 Norm Loss', 1.0681436),
 ('TV(2.0) Loss', 1147.4244)], overall loss: -1083.0574951171875
Iteration: 872, named\_losses: [('ActivationMax Loss', -2230.4153),
 ('L-6.0 Norm Loss', 1.069451),
 ('TV(2.0) Loss', 1148.128)], overall loss: -1081.2178955078125
Iteration: 873, named\_losses: [('ActivationMax Loss', -2227.891),
 ('L-6.0 Norm Loss', 1.0674726),
 ('TV(2.0) Loss', 1145.0454)], overall loss: -1081.7783203125
Iteration: 874, named\_losses: [('ActivationMax Loss', -2226.2773),
 ('L-6.0 Norm Loss', 1.0680261),
 ('TV(2.0) Loss', 1140.7382)], overall loss: -1084.4710693359375
Iteration: 875, named\_losses: [('ActivationMax Loss', -2232.2327),
 ('L-6.0 Norm Loss', 1.064738),
 ('TV(2.0) Loss', 1148.807)], overall loss: -1082.3609619140625
Iteration: 876, named\_losses: [('ActivationMax Loss', -2226.3745),
 ('L-6.0 Norm Loss', 1.0715768),
 ('TV(2.0) Loss', 1146.114)], overall loss: -1079.18896484375
Iteration: 877, named\_losses: [('ActivationMax Loss', -2237.2383),
 ('L-6.0 Norm Loss', 1.0668942),
 ('TV(2.0) Loss', 1153.2886)], overall loss: -1082.8828125
Iteration: 878, named\_losses: [('ActivationMax Loss', -2226.422),
 ('L-6.0 Norm Loss', 1.069834),
 ('TV(2.0) Loss', 1141.6442)], overall loss: -1083.7081298828125
Iteration: 879, named\_losses: [('ActivationMax Loss', -2237.5566),
 ('L-6.0 Norm Loss', 1.0687569),
 ('TV(2.0) Loss', 1156.8312)], overall loss: -1079.6566162109375
Iteration: 880, named\_losses: [('ActivationMax Loss', -2228.726),
 ('L-6.0 Norm Loss', 1.071007),
 ('TV(2.0) Loss', 1146.0322)], overall loss: -1081.622802734375
Iteration: 881, named\_losses: [('ActivationMax Loss', -2237.8284),
 ('L-6.0 Norm Loss', 1.0676978),
 ('TV(2.0) Loss', 1155.4713)], overall loss: -1081.2894287109375
Iteration: 882, named\_losses: [('ActivationMax Loss', -2225.9663),
 ('L-6.0 Norm Loss', 1.0683135),
 ('TV(2.0) Loss', 1146.9666)], overall loss: -1077.931396484375
Iteration: 883, named\_losses: [('ActivationMax Loss', -2231.7742),
 ('L-6.0 Norm Loss', 1.0680882),
 ('TV(2.0) Loss', 1147.5497)], overall loss: -1083.1563720703125
Iteration: 884, named\_losses: [('ActivationMax Loss', -2221.6465),
 ('L-6.0 Norm Loss', 1.0691801),
 ('TV(2.0) Loss', 1137.6141)], overall loss: -1082.9632568359375
Iteration: 885, named\_losses: [('ActivationMax Loss', -2232.381),
 ('L-6.0 Norm Loss', 1.0692497),
 ('TV(2.0) Loss', 1147.9995)], overall loss: -1083.312255859375
Iteration: 886, named\_losses: [('ActivationMax Loss', -2222.704),
 ('L-6.0 Norm Loss', 1.0672512),
 ('TV(2.0) Loss', 1140.3893)], overall loss: -1081.2476806640625
Iteration: 887, named\_losses: [('ActivationMax Loss', -2240.4578),
 ('L-6.0 Norm Loss', 1.067917),
 ('TV(2.0) Loss', 1154.7634)], overall loss: -1084.62646484375
Iteration: 888, named\_losses: [('ActivationMax Loss', -2228.6018),
 ('L-6.0 Norm Loss', 1.0674239),
 ('TV(2.0) Loss', 1147.292)], overall loss: -1080.242431640625
Iteration: 889, named\_losses: [('ActivationMax Loss', -2240.193),
 ('L-6.0 Norm Loss', 1.0667456),
 ('TV(2.0) Loss', 1152.9747)], overall loss: -1086.1517333984375
Iteration: 890, named\_losses: [('ActivationMax Loss', -2219.8254),
 ('L-6.0 Norm Loss', 1.0661571),
 ('TV(2.0) Loss', 1136.5273)], overall loss: -1082.23193359375
Iteration: 891, named\_losses: [('ActivationMax Loss', -2238.077),
 ('L-6.0 Norm Loss', 1.068856),
 ('TV(2.0) Loss', 1154.938)], overall loss: -1082.070068359375
Iteration: 892, named\_losses: [('ActivationMax Loss', -2223.8108),
 ('L-6.0 Norm Loss', 1.0657182),
 ('TV(2.0) Loss', 1141.6989)], overall loss: -1081.0462646484375
Iteration: 893, named\_losses: [('ActivationMax Loss', -2234.401),
 ('L-6.0 Norm Loss', 1.0684208),
 ('TV(2.0) Loss', 1154.8112)], overall loss: -1078.5213623046875
Iteration: 894, named\_losses: [('ActivationMax Loss', -2226.1965),
 ('L-6.0 Norm Loss', 1.0651007),
 ('TV(2.0) Loss', 1140.8197)], overall loss: -1084.3116455078125
Iteration: 895, named\_losses: [('ActivationMax Loss', -2228.7527),
 ('L-6.0 Norm Loss', 1.0667614),
 ('TV(2.0) Loss', 1145.6422)], overall loss: -1082.0438232421875
Iteration: 896, named\_losses: [('ActivationMax Loss', -2231.176),
 ('L-6.0 Norm Loss', 1.0689791),
 ('TV(2.0) Loss', 1145.8235)], overall loss: -1084.283447265625
Iteration: 897, named\_losses: [('ActivationMax Loss', -2235.026),
 ('L-6.0 Norm Loss', 1.0703338),
 ('TV(2.0) Loss', 1155.1393)], overall loss: -1078.8162841796875
Iteration: 898, named\_losses: [('ActivationMax Loss', -2228.7556),
 ('L-6.0 Norm Loss', 1.0698069),
 ('TV(2.0) Loss', 1146.4003)], overall loss: -1081.2855224609375
Iteration: 899, named\_losses: [('ActivationMax Loss', -2233.492),
 ('L-6.0 Norm Loss', 1.0688305),
 ('TV(2.0) Loss', 1149.3265)], overall loss: -1083.0965576171875
Iteration: 900, named\_losses: [('ActivationMax Loss', -2222.9556),
 ('L-6.0 Norm Loss', 1.0669297),
 ('TV(2.0) Loss', 1138.0968)], overall loss: -1083.7918701171875
Iteration: 901, named\_losses: [('ActivationMax Loss', -2231.7808),
 ('L-6.0 Norm Loss', 1.0682997),
 ('TV(2.0) Loss', 1145.0708)], overall loss: -1085.6416015625
Iteration: 902, named\_losses: [('ActivationMax Loss', -2226.7097),
 ('L-6.0 Norm Loss', 1.067131),
 ('TV(2.0) Loss', 1142.8252)], overall loss: -1082.8173828125
Iteration: 903, named\_losses: [('ActivationMax Loss', -2242.9775),
 ('L-6.0 Norm Loss', 1.0706735),
 ('TV(2.0) Loss', 1157.8611)], overall loss: -1084.0458984375
Iteration: 904, named\_losses: [('ActivationMax Loss', -2229.1257),
 ('L-6.0 Norm Loss', 1.0666059),
 ('TV(2.0) Loss', 1149.5192)], overall loss: -1078.5399169921875
Iteration: 905, named\_losses: [('ActivationMax Loss', -2235.697),
 ('L-6.0 Norm Loss', 1.0676835),
 ('TV(2.0) Loss', 1151.3059)], overall loss: -1083.323486328125
Iteration: 906, named\_losses: [('ActivationMax Loss', -2228.4602),
 ('L-6.0 Norm Loss', 1.0674556),
 ('TV(2.0) Loss', 1147.4509)], overall loss: -1079.94189453125
Iteration: 907, named\_losses: [('ActivationMax Loss', -2233.356),
 ('L-6.0 Norm Loss', 1.0694636),
 ('TV(2.0) Loss', 1147.4038)], overall loss: -1084.882568359375
Iteration: 908, named\_losses: [('ActivationMax Loss', -2232.8972),
 ('L-6.0 Norm Loss', 1.0627185),
 ('TV(2.0) Loss', 1150.682)], overall loss: -1081.1524658203125
Iteration: 909, named\_losses: [('ActivationMax Loss', -2224.6968),
 ('L-6.0 Norm Loss', 1.0691643),
 ('TV(2.0) Loss', 1143.4358)], overall loss: -1080.19189453125
Iteration: 910, named\_losses: [('ActivationMax Loss', -2232.8674),
 ('L-6.0 Norm Loss', 1.063056),
 ('TV(2.0) Loss', 1150.524)], overall loss: -1081.2803955078125
Iteration: 911, named\_losses: [('ActivationMax Loss', -2224.1067),
 ('L-6.0 Norm Loss', 1.0680616),
 ('TV(2.0) Loss', 1143.2202)], overall loss: -1079.818359375
Iteration: 912, named\_losses: [('ActivationMax Loss', -2237.7805),
 ('L-6.0 Norm Loss', 1.0647177),
 ('TV(2.0) Loss', 1150.8646)], overall loss: -1085.8511962890625
Iteration: 913, named\_losses: [('ActivationMax Loss', -2224.5828),
 ('L-6.0 Norm Loss', 1.0694456),
 ('TV(2.0) Loss', 1142.8148)], overall loss: -1080.6986083984375
Iteration: 914, named\_losses: [('ActivationMax Loss', -2229.6304),
 ('L-6.0 Norm Loss', 1.0644099),
 ('TV(2.0) Loss', 1143.0005)], overall loss: -1085.5654296875
Iteration: 915, named\_losses: [('ActivationMax Loss', -2225.8806),
 ('L-6.0 Norm Loss', 1.0694654),
 ('TV(2.0) Loss', 1141.6533)], overall loss: -1083.15771484375
Iteration: 916, named\_losses: [('ActivationMax Loss', -2234.9219),
 ('L-6.0 Norm Loss', 1.066031),
 ('TV(2.0) Loss', 1153.3083)], overall loss: -1080.547607421875
Iteration: 917, named\_losses: [('ActivationMax Loss', -2231.0288),
 ('L-6.0 Norm Loss', 1.0697662),
 ('TV(2.0) Loss', 1146.5293)], overall loss: -1083.4296875
Iteration: 918, named\_losses: [('ActivationMax Loss', -2234.7908),
 ('L-6.0 Norm Loss', 1.0640373),
 ('TV(2.0) Loss', 1152.5754)], overall loss: -1081.1513671875
Iteration: 919, named\_losses: [('ActivationMax Loss', -2232.0164),
 ('L-6.0 Norm Loss', 1.0691357),
 ('TV(2.0) Loss', 1145.4949)], overall loss: -1085.452392578125
Iteration: 920, named\_losses: [('ActivationMax Loss', -2237.1328),
 ('L-6.0 Norm Loss', 1.0665873),
 ('TV(2.0) Loss', 1151.8109)], overall loss: -1084.2552490234375
Iteration: 921, named\_losses: [('ActivationMax Loss', -2227.4932),
 ('L-6.0 Norm Loss', 1.069452),
 ('TV(2.0) Loss', 1146.1036)], overall loss: -1080.3201904296875
Iteration: 922, named\_losses: [('ActivationMax Loss', -2239.724),
 ('L-6.0 Norm Loss', 1.0653833),
 ('TV(2.0) Loss', 1155.6625)], overall loss: -1082.9962158203125
Iteration: 923, named\_losses: [('ActivationMax Loss', -2226.231),
 ('L-6.0 Norm Loss', 1.0684736),
 ('TV(2.0) Loss', 1143.954)], overall loss: -1081.2086181640625
Iteration: 924, named\_losses: [('ActivationMax Loss', -2231.1042),
 ('L-6.0 Norm Loss', 1.0679021),
 ('TV(2.0) Loss', 1148.5664)], overall loss: -1081.469970703125
Iteration: 925, named\_losses: [('ActivationMax Loss', -2225.647),
 ('L-6.0 Norm Loss', 1.0671751),
 ('TV(2.0) Loss', 1140.399)], overall loss: -1084.1807861328125
Iteration: 926, named\_losses: [('ActivationMax Loss', -2236.274),
 ('L-6.0 Norm Loss', 1.0643444),
 ('TV(2.0) Loss', 1150.5842)], overall loss: -1084.625244140625
Iteration: 927, named\_losses: [('ActivationMax Loss', -2224.8164),
 ('L-6.0 Norm Loss', 1.0674547),
 ('TV(2.0) Loss', 1142.2072)], overall loss: -1081.5418701171875
Iteration: 928, named\_losses: [('ActivationMax Loss', -2232.9397),
 ('L-6.0 Norm Loss', 1.0689133),
 ('TV(2.0) Loss', 1153.5927)], overall loss: -1078.2781982421875
Iteration: 929, named\_losses: [('ActivationMax Loss', -2227.7258),
 ('L-6.0 Norm Loss', 1.0671),
 ('TV(2.0) Loss', 1145.5823)], overall loss: -1081.076416015625
Iteration: 930, named\_losses: [('ActivationMax Loss', -2236.645),
 ('L-6.0 Norm Loss', 1.0659841),
 ('TV(2.0) Loss', 1154.745)], overall loss: -1080.8341064453125
Iteration: 931, named\_losses: [('ActivationMax Loss', -2221.5684),
 ('L-6.0 Norm Loss', 1.0683563),
 ('TV(2.0) Loss', 1139.9426)], overall loss: -1080.557373046875
Iteration: 932, named\_losses: [('ActivationMax Loss', -2228.6504),
 ('L-6.0 Norm Loss', 1.066176),
 ('TV(2.0) Loss', 1145.6838)], overall loss: -1081.900390625
Iteration: 933, named\_losses: [('ActivationMax Loss', -2219.7466),
 ('L-6.0 Norm Loss', 1.0699114),
 ('TV(2.0) Loss', 1139.276)], overall loss: -1079.4007568359375
Iteration: 934, named\_losses: [('ActivationMax Loss', -2235.8828),
 ('L-6.0 Norm Loss', 1.0668951),
 ('TV(2.0) Loss', 1145.5304)], overall loss: -1089.2855224609375
Iteration: 935, named\_losses: [('ActivationMax Loss', -2218.8782),
 ('L-6.0 Norm Loss', 1.0688274),
 ('TV(2.0) Loss', 1137.4128)], overall loss: -1080.396484375
Iteration: 936, named\_losses: [('ActivationMax Loss', -2232.6755),
 ('L-6.0 Norm Loss', 1.0676911),
 ('TV(2.0) Loss', 1152.1483)], overall loss: -1079.4595947265625
Iteration: 937, named\_losses: [('ActivationMax Loss', -2228.3455),
 ('L-6.0 Norm Loss', 1.0690502),
 ('TV(2.0) Loss', 1142.2344)], overall loss: -1085.0419921875
Iteration: 938, named\_losses: [('ActivationMax Loss', -2235.5845),
 ('L-6.0 Norm Loss', 1.0720361),
 ('TV(2.0) Loss', 1146.7179)], overall loss: -1087.7945556640625
Iteration: 939, named\_losses: [('ActivationMax Loss', -2223.3208),
 ('L-6.0 Norm Loss', 1.066966),
 ('TV(2.0) Loss', 1141.2673)], overall loss: -1080.986572265625
Iteration: 940, named\_losses: [('ActivationMax Loss', -2234.8296),
 ('L-6.0 Norm Loss', 1.0688423),
 ('TV(2.0) Loss', 1153.7626)], overall loss: -1079.9981689453125
Iteration: 941, named\_losses: [('ActivationMax Loss', -2225.577),
 ('L-6.0 Norm Loss', 1.0715392),
 ('TV(2.0) Loss', 1146.5962)], overall loss: -1077.9091796875
Iteration: 942, named\_losses: [('ActivationMax Loss', -2237.55),
 ('L-6.0 Norm Loss', 1.069941),
 ('TV(2.0) Loss', 1152.3651)], overall loss: -1084.1151123046875
Iteration: 943, named\_losses: [('ActivationMax Loss', -2223.2783),
 ('L-6.0 Norm Loss', 1.0694855),
 ('TV(2.0) Loss', 1141.0226)], overall loss: -1081.1861572265625
Iteration: 944, named\_losses: [('ActivationMax Loss', -2233.3716),
 ('L-6.0 Norm Loss', 1.0667413),
 ('TV(2.0) Loss', 1147.8394)], overall loss: -1084.465576171875
Iteration: 945, named\_losses: [('ActivationMax Loss', -2216.98),
 ('L-6.0 Norm Loss', 1.0680826),
 ('TV(2.0) Loss', 1138.9814)], overall loss: -1076.930419921875
Iteration: 946, named\_losses: [('ActivationMax Loss', -2239.9946),
 ('L-6.0 Norm Loss', 1.0700516),
 ('TV(2.0) Loss', 1155.4344)], overall loss: -1083.4901123046875
Iteration: 947, named\_losses: [('ActivationMax Loss', -2227.271),
 ('L-6.0 Norm Loss', 1.0675184),
 ('TV(2.0) Loss', 1149.5375)], overall loss: -1076.6658935546875
Iteration: 948, named\_losses: [('ActivationMax Loss', -2231.4414),
 ('L-6.0 Norm Loss', 1.0676064),
 ('TV(2.0) Loss', 1146.881)], overall loss: -1083.4927978515625
Iteration: 949, named\_losses: [('ActivationMax Loss', -2225.7463),
 ('L-6.0 Norm Loss', 1.0692934),
 ('TV(2.0) Loss', 1142.4475)], overall loss: -1082.2294921875
Iteration: 950, named\_losses: [('ActivationMax Loss', -2230.7605),
 ('L-6.0 Norm Loss', 1.0645165),
 ('TV(2.0) Loss', 1143.215)], overall loss: -1086.4810791015625
Iteration: 951, named\_losses: [('ActivationMax Loss', -2228.3723),
 ('L-6.0 Norm Loss', 1.0676554),
 ('TV(2.0) Loss', 1144.3534)], overall loss: -1082.9512939453125
Iteration: 952, named\_losses: [('ActivationMax Loss', -2231.9192),
 ('L-6.0 Norm Loss', 1.0705867),
 ('TV(2.0) Loss', 1150.3854)], overall loss: -1080.4632568359375
Iteration: 953, named\_losses: [('ActivationMax Loss', -2226.4998),
 ('L-6.0 Norm Loss', 1.0698317),
 ('TV(2.0) Loss', 1143.1757)], overall loss: -1082.2542724609375
Iteration: 954, named\_losses: [('ActivationMax Loss', -2233.4668),
 ('L-6.0 Norm Loss', 1.0672082),
 ('TV(2.0) Loss', 1147.7947)], overall loss: -1084.60498046875
Iteration: 955, named\_losses: [('ActivationMax Loss', -2223.7478),
 ('L-6.0 Norm Loss', 1.0699472),
 ('TV(2.0) Loss', 1140.423)], overall loss: -1082.2547607421875
Iteration: 956, named\_losses: [('ActivationMax Loss', -2231.8313),
 ('L-6.0 Norm Loss', 1.0660847),
 ('TV(2.0) Loss', 1146.6533)], overall loss: -1084.11181640625
Iteration: 957, named\_losses: [('ActivationMax Loss', -2228.141),
 ('L-6.0 Norm Loss', 1.0677005),
 ('TV(2.0) Loss', 1145.3424)], overall loss: -1081.7310791015625
Iteration: 958, named\_losses: [('ActivationMax Loss', -2231.3586),
 ('L-6.0 Norm Loss', 1.0684074),
 ('TV(2.0) Loss', 1152.47)], overall loss: -1077.8203125
Iteration: 959, named\_losses: [('ActivationMax Loss', -2224.1367),
 ('L-6.0 Norm Loss', 1.0689354),
 ('TV(2.0) Loss', 1146.2255)], overall loss: -1076.8424072265625
Iteration: 960, named\_losses: [('ActivationMax Loss', -2236.224),
 ('L-6.0 Norm Loss', 1.0694252),
 ('TV(2.0) Loss', 1149.7932)], overall loss: -1085.361572265625
Iteration: 961, named\_losses: [('ActivationMax Loss', -2217.542),
 ('L-6.0 Norm Loss', 1.0658479),
 ('TV(2.0) Loss', 1138.7214)], overall loss: -1077.754638671875
Iteration: 962, named\_losses: [('ActivationMax Loss', -2233.738),
 ('L-6.0 Norm Loss', 1.068227),
 ('TV(2.0) Loss', 1149.5685)], overall loss: -1083.1014404296875
Iteration: 963, named\_losses: [('ActivationMax Loss', -2227.643),
 ('L-6.0 Norm Loss', 1.0712966),
 ('TV(2.0) Loss', 1147.1631)], overall loss: -1079.40869140625
Iteration: 964, named\_losses: [('ActivationMax Loss', -2233.456),
 ('L-6.0 Norm Loss', 1.069702),
 ('TV(2.0) Loss', 1149.9202)], overall loss: -1082.46630859375
Iteration: 965, named\_losses: [('ActivationMax Loss', -2220.7622),
 ('L-6.0 Norm Loss', 1.0665014),
 ('TV(2.0) Loss', 1139.8033)], overall loss: -1079.8924560546875
Iteration: 966, named\_losses: [('ActivationMax Loss', -2235.798),
 ('L-6.0 Norm Loss', 1.0698459),
 ('TV(2.0) Loss', 1157.749)], overall loss: -1076.979248046875
Iteration: 967, named\_losses: [('ActivationMax Loss', -2227.3765),
 ('L-6.0 Norm Loss', 1.0719994),
 ('TV(2.0) Loss', 1146.6133)], overall loss: -1079.691162109375
Iteration: 968, named\_losses: [('ActivationMax Loss', -2229.7668),
 ('L-6.0 Norm Loss', 1.0697063),
 ('TV(2.0) Loss', 1145.1028)], overall loss: -1083.59423828125
Iteration: 969, named\_losses: [('ActivationMax Loss', -2219.8184),
 ('L-6.0 Norm Loss', 1.0663475),
 ('TV(2.0) Loss', 1139.3656)], overall loss: -1079.3863525390625
Iteration: 970, named\_losses: [('ActivationMax Loss', -2233.0098),
 ('L-6.0 Norm Loss', 1.0685372),
 ('TV(2.0) Loss', 1147.8192)], overall loss: -1084.1219482421875
Iteration: 971, named\_losses: [('ActivationMax Loss', -2230.9639),
 ('L-6.0 Norm Loss', 1.0679394),
 ('TV(2.0) Loss', 1147.16)], overall loss: -1082.7359619140625
Iteration: 972, named\_losses: [('ActivationMax Loss', -2230.8555),
 ('L-6.0 Norm Loss', 1.0659008),
 ('TV(2.0) Loss', 1149.1737)], overall loss: -1080.6158447265625
Iteration: 973, named\_losses: [('ActivationMax Loss', -2230.8696),
 ('L-6.0 Norm Loss', 1.0693691),
 ('TV(2.0) Loss', 1145.2715)], overall loss: -1084.52880859375
Iteration: 974, named\_losses: [('ActivationMax Loss', -2229.146),
 ('L-6.0 Norm Loss', 1.0665613),
 ('TV(2.0) Loss', 1143.447)], overall loss: -1084.63232421875
Iteration: 975, named\_losses: [('ActivationMax Loss', -2230.8528),
 ('L-6.0 Norm Loss', 1.0663096),
 ('TV(2.0) Loss', 1147.9491)], overall loss: -1081.8372802734375
Iteration: 976, named\_losses: [('ActivationMax Loss', -2227.3176),
 ('L-6.0 Norm Loss', 1.067955),
 ('TV(2.0) Loss', 1145.4828)], overall loss: -1080.7669677734375
Iteration: 977, named\_losses: [('ActivationMax Loss', -2226.6196),
 ('L-6.0 Norm Loss', 1.0668914),
 ('TV(2.0) Loss', 1146.4882)], overall loss: -1079.0645751953125
Iteration: 978, named\_losses: [('ActivationMax Loss', -2233.4402),
 ('L-6.0 Norm Loss', 1.0712702),
 ('TV(2.0) Loss', 1148.5267)], overall loss: -1083.8421630859375
Iteration: 979, named\_losses: [('ActivationMax Loss', -2226.9175),
 ('L-6.0 Norm Loss', 1.065139),
 ('TV(2.0) Loss', 1144.6532)], overall loss: -1081.1990966796875
Iteration: 980, named\_losses: [('ActivationMax Loss', -2236.3591),
 ('L-6.0 Norm Loss', 1.0660396),
 ('TV(2.0) Loss', 1146.3585)], overall loss: -1088.9346923828125
Iteration: 981, named\_losses: [('ActivationMax Loss', -2236.338),
 ('L-6.0 Norm Loss', 1.0691357),
 ('TV(2.0) Loss', 1149.9973)], overall loss: -1085.271484375
Iteration: 982, named\_losses: [('ActivationMax Loss', -2222.8914),
 ('L-6.0 Norm Loss', 1.069218),
 ('TV(2.0) Loss', 1144.1964)], overall loss: -1077.6256103515625
Iteration: 983, named\_losses: [('ActivationMax Loss', -2234.9436),
 ('L-6.0 Norm Loss', 1.0679382),
 ('TV(2.0) Loss', 1152.4026)], overall loss: -1081.47314453125
Iteration: 984, named\_losses: [('ActivationMax Loss', -2223.411),
 ('L-6.0 Norm Loss', 1.0697668),
 ('TV(2.0) Loss', 1140.6182)], overall loss: -1081.722900390625
Iteration: 985, named\_losses: [('ActivationMax Loss', -2227.3362),
 ('L-6.0 Norm Loss', 1.0668495),
 ('TV(2.0) Loss', 1146.093)], overall loss: -1080.17626953125
Iteration: 986, named\_losses: [('ActivationMax Loss', -2234.1587),
 ('L-6.0 Norm Loss', 1.0681355),
 ('TV(2.0) Loss', 1147.212)], overall loss: -1085.8785400390625
Iteration: 987, named\_losses: [('ActivationMax Loss', -2225.2688),
 ('L-6.0 Norm Loss', 1.0673156),
 ('TV(2.0) Loss', 1144.9888)], overall loss: -1079.212646484375
Iteration: 988, named\_losses: [('ActivationMax Loss', -2234.9988),
 ('L-6.0 Norm Loss', 1.0668299),
 ('TV(2.0) Loss', 1149.2988)], overall loss: -1084.633056640625
Iteration: 989, named\_losses: [('ActivationMax Loss', -2229.9019),
 ('L-6.0 Norm Loss', 1.0726674),
 ('TV(2.0) Loss', 1146.8511)], overall loss: -1081.97802734375
Iteration: 990, named\_losses: [('ActivationMax Loss', -2234.623),
 ('L-6.0 Norm Loss', 1.072234),
 ('TV(2.0) Loss', 1150.5186)], overall loss: -1083.0322265625
Iteration: 991, named\_losses: [('ActivationMax Loss', -2227.611),
 ('L-6.0 Norm Loss', 1.0711522),
 ('TV(2.0) Loss', 1147.983)], overall loss: -1078.5570068359375
Iteration: 992, named\_losses: [('ActivationMax Loss', -2234.628),
 ('L-6.0 Norm Loss', 1.0696721),
 ('TV(2.0) Loss', 1149.337)], overall loss: -1084.2213134765625
Iteration: 993, named\_losses: [('ActivationMax Loss', -2229.0596),
 ('L-6.0 Norm Loss', 1.0703431),
 ('TV(2.0) Loss', 1146.5754)], overall loss: -1081.413818359375
Iteration: 994, named\_losses: [('ActivationMax Loss', -2230.63),
 ('L-6.0 Norm Loss', 1.067137),
 ('TV(2.0) Loss', 1146.6567)], overall loss: -1082.906005859375
Iteration: 995, named\_losses: [('ActivationMax Loss', -2226.7993),
 ('L-6.0 Norm Loss', 1.0664921),
 ('TV(2.0) Loss', 1143.5121)], overall loss: -1082.2208251953125
Iteration: 996, named\_losses: [('ActivationMax Loss', -2236.5483),
 ('L-6.0 Norm Loss', 1.0682409),
 ('TV(2.0) Loss', 1148.652)], overall loss: -1086.8280029296875
Iteration: 997, named\_losses: [('ActivationMax Loss', -2233.3481),
 ('L-6.0 Norm Loss', 1.0672288),
 ('TV(2.0) Loss', 1145.9495)], overall loss: -1086.33154296875
Iteration: 998, named\_losses: [('ActivationMax Loss', -2234.095),
 ('L-6.0 Norm Loss', 1.0674864),
 ('TV(2.0) Loss', 1151.7848)], overall loss: -1081.2427978515625
Iteration: 999, named\_losses: [('ActivationMax Loss', -2226.0688),
 ('L-6.0 Norm Loss', 1.0668033),
 ('TV(2.0) Loss', 1146.2385)], overall loss: -1078.763427734375
Iteration: 1000, named\_losses: [('ActivationMax Loss', -2228.343),
 ('L-6.0 Norm Loss', 1.0644373),
 ('TV(2.0) Loss', 1150.7878)], overall loss: -1076.49072265625
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.image.AxesImage at 0x7f188a274e48>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Q7_files/Q7_50_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{visualizing-input-that-maximizes-the-output-of-each-class}{%
\subsection*{7.4.5 Visualizing input that maximizes the output of each
class}\label{visualizing-input-that-maximizes-the-output-of-each-class}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{k}{for} \PY{n}{class\PYZus{}idx} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Lets turn off verbose output this time to avoid clutter and just see the output.}
    \PY{n}{img} \PY{o}{=} \PY{n}{visualize\PYZus{}activation}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{layer\PYZus{}idx}\PY{p}{,} \PY{n}{filter\PYZus{}indices}\PY{o}{=}\PY{n}{class\PYZus{}idx}\PY{p}{,} \PY{n}{input\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.}\PY{p}{,} \PY{l+m+mf}{1.}\PY{p}{)}\PY{p}{,}
        \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{tv\PYZus{}weight}\PY{o}{=}\PY{l+m+mf}{8.}\PY{p}{,} \PY{n}{lp\PYZus{}norm\PYZus{}weight}\PY{o}{=}\PY{l+m+mf}{10.}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}plt.figure()}
    \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{n}{class\PYZus{}idx}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Networks perception of }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{class\PYZus{}idx}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{img}\PY{p}{[}\PY{o}{.}\PY{o}{.}\PY{o}{.}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Q7_files/Q7_52_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{end-of-notebook}{%
\section*{End of Notebook}\label{end-of-notebook}}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
