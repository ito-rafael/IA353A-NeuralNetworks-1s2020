\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Transferência negativa}{2}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Camadas compartilhadas}{2}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}MALSAR}{2}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Considerando a camada $q$ de uma rede neural MLP, temos:}{3}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Onde:}{3}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Calculando a variância de ambos lados da equação anterior, temos:}{3}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Devemos agora fazer algumas considerações:}{3}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Continuando o desenvolvimento da equação anterior:}{3}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A partir de i), temos:}{3}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Se duas variáveis são independentes entre si, temos a igualdade:\\ \href  {https://en.wikipedia.org/wiki/Variance\#Product_of_independent_variables}{https://en.wikipedia.org/wiki/Variance\#Product\_of\_independent\_variables}:}{3}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Vamos agora abrir o produto das matrizes $W$ e $x$ em uma soma dos produtos de seus termos $w_i$ e $x_i$. Usando também a consideração dada por ii) e sabendo que $b$ é uma constante (e portanto sua variância é zero), temos que:}{3}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A partir de iii) e iv), temos que $\mathbb  {E}(w_i) = 0$ e $\mathbb  {E}(x_i) = 0$, sendo ambos $W$ e $x$ variáveis i.i.d. Assim:}{4}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Queremos provar que $b = \sqrt  {\frac  {3}{n^{[q-1]}}}$ para que a variância da entrada da camada $q$ seja igual a variância da camada $q-1$.}{4}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sabendo que a variância de uma variável aleatória que segue uma distribuição uniforme entre $a$ e $b$, isto é, $X \sim \mathbb  {U}[a,b]$, é dada por: $Var(X) = \frac  {(b-a)^2}{12}$ (\href  {https://proofwiki.org/wiki/Variance_of_Continuous_Uniform_Distribution}{prova}). Temos:}{4}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Como queremos $Var(x^{[q]}) = Var(x^{[q-1]})$, temos:}{4}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Principais seções do padrão de documentação}{5}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{As principais seções do padrão de documentação de datasets são mostradas na seção 3 do artigo, entitulada ``Questions and Workflow''. Os itens desta seção, assim como uma breve descrição são apresentados a seguir:}{5}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dois exemplos de datasheet para dataset são mostrados no apêndice do artigo. Um para o dataset ``Labeled Faces in the Wild'' e outro para o dataset ``Pang and Lee’s polarity''.}{5}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Artigos com propósitos similares}{5}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1}Artigo 1}{5}{subsubsection.9.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.2}Artigo 2}{5}{subsubsection.9.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}EfficientNet}{6}{subsection.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{O artigo \href  {https://arxiv.org/pdf/1905.11946.pdf}{(Tan \& Le, 2019)} propõe uma forma de escalar modelos baseados em redes convolucionais (ConvNets), como por exemplo MobileNets e ResNet, levando em conta três parâmetros avaliados conjuntamente: profundidade, largura e resolução. Adicionalmente, os autores usam um método de NAS (\emph  {neural architecture search}) para encontrar um modelo base (baseline), para em seguida usar o método de escalamento proposto e obter uma família de modelos denominada \emph  {EfficientNets}, cujos resultados são impressionantes, sendo mais eficientes em termos de custo computacional (modelos menores e mais rápidos) e performance, com resultados melhores atingindo o estado da arte (SOTA) para base de dados como ImageNet, CIFAR-100 e outras.}{6}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Supondo que se queira um modelo maior que use $2^N$ mais recursos, propõe-se aumentar a rede multiplicando os três parâmetros pelas seguintes constantes: profundidade multiplicada por $\alpha ^N$, largura multiplicada por $\beta ^N$ e tamanho da imagem (resolução) por $\gamma ^N$, onde $\alpha $, $\beta $ e $\gamma $ são determinados através de um grid search no modelo original.}{6}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Na seção 3 do artigo, os autores mostram que o escalamento dos três parâmetros aumentam a performance dos modelos, mas cada um seguindo sua própria curva de saturação (figura 3 do artigo). Assim, eles chegam na primeira observação. A segunda observação foi obtida através dos experimentos cujos resultados são mostrados na figura 4 do artigo. Aqui é concluído que para uma melhor acurácia e eficiência, é necessário um balanceamento entre os fatores de escala de largura, profundidade e resolução.}{6}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{O método de escalamento proposto é mostrado a seguir:}{6}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{sujeito a:}{6}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{O modelo base, denominado EfficientNet, foi obtido através de uma busca (NAS) multi-objetiva, procurando otimizar tanto a acurácia quanto FLOPS (operações de ponto flutuante por segundo). O função objetivo é dada por: $ACC(m) \times [FLOPS(m)/T]^w$, onde $ACC(m)$ e $FLOPS(m)$ é a acurária e FLOPS do modelo $m$, $T$ é o FLOPS alvo e $w$ controla o \emph  {tradeoff} entre acurária e FLOPS. Tomando $w = -0.07$, chega-se no modelo EfficientNet-B0.}{6}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A partir do modelo EfficientNet-B0, toma-se $\phi =1$ (dobro de recursos) e busca-se as constantes através de um \emph  {grid search}. Encontra-se $\alpha =1.2$, $\beta =1.1$ e $\gamma =1.15$, satisfazendo a restrição $\alpha \cdot \beta ^2 \cdot \gamma ^2 \approx 2$. Em seguida, essas constantes são usadas para escalar diferentes modelos alterando o valor de $\phi $, obtendo-se os modelos EfficientNet-B1 até B7. Além destes modelos, MobileNets e ResNets também foram escaladas seguindo essa regra.}{6}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A partir da seção 5 do artigo, são descritos os experimentos e resultados, discussões e conclusão. Conforme descrito no início da resposta desta questão, os resultados obtidos são impressionantes, ganhando tanto em performance (custo computacional) quanto em desempenho (acurária), atingindo um novo SOTA na área de visão computacional e ConvNets.}{6}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}FixEfficientNet}{7}{subsection.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{O segundo artigo \href  {https://arxiv.org/pdf/2003.08237.pdf}{(Touvron et al., 2020)} na verdade é uma nota que complementa o artigo ``\href  {https://arxiv.org/pdf/1906.06423.pdf}{Fixing the train-test resolution discrepancy}'', que introduz o método denominado FixRes.}{7}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{O artigo original mostra que as técnicas de \emph  {data-augmentation} usadas até então induziam uma discrepância entre o tamanhos dos objetos visto pelo classificador durante o treinamento e durante a fase de teste. Assim, os autores propõe uma estratégia que otimiza a performance do classificador, empregando diferentes resoluções em teste e treinamento. Isso é feito através de um \emph  {fine-tuning} da rede na resolução de teste.}{7}{section*.29}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparação entre diversas redes com e sem a aplicação do método FixRes\relax }}{7}{figure.caption.31}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:q10}{{2}{7}{Comparação entre diversas redes com e sem a aplicação do método FixRes\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {paragraph}{A nota que complementa o aritgo original toma as diversas redes renomadas, dentre elas as EfficientNets do item anterior, e aplica o método FixRes para obter uma família de redes denominadas FixEfficientNet. Os resultados podem ser vistos na figura \ref  {fig:q10}.}{7}{figure.caption.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Como podemos ver, em um curto período de tempo temos novos modelos SOTA. Alguns podem vir de arquiteturas novas (ex: EfficientNet em ConvNets e Transformers em NLP), outros de melhorias feitas na metodologia a partir de modelos já existentes (ex: escalamento apresentado no artigo da EfficientNet e método FixRes).}{7}{section*.32}\protected@file@percent }
