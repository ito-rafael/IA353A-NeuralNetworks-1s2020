\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {paragraph}{The Jupyter notebook related to this section with the results presented here can be opened in Google Colab environment with following link:\\}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Training results and two study cases}{2}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}Maze 1}{2}{subsubsection.8.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Maze 1\relax }}{2}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:maze1}{{2}{2}{Maze 1\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Training results summary for maze 1\relax }}{2}{table.caption.5}\protected@file@percent }
\newlabel{tab:maze1-results}{{1}{2}{Training results summary for maze 1\relax }{table.caption.5}{}}
\newlabel{fig:maze1-case1}{{3a}{2}{Maze 1 - Study case 1\relax }{figure.caption.6}{}}
\newlabel{sub@fig:maze1-case1}{{a}{2}{Maze 1 - Study case 1\relax }{figure.caption.6}{}}
\newlabel{fig:maze1-case2}{{3b}{2}{Maze 1 - Study case 2\relax }{figure.caption.6}{}}
\newlabel{sub@fig:maze1-case2}{{b}{2}{Maze 1 - Study case 2\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Maze 2}{3}{subsubsection.8.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Maze 2\relax }}{3}{figure.caption.7}\protected@file@percent }
\newlabel{fig:maze2}{{4}{3}{Maze 2\relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Training results summary for maze 2\relax }}{3}{table.caption.8}\protected@file@percent }
\newlabel{tab:maze2-results}{{2}{3}{Training results summary for maze 2\relax }{table.caption.8}{}}
\newlabel{fig:maze2-case1}{{5a}{3}{Maze 2 - Study case 1\relax }{figure.caption.9}{}}
\newlabel{sub@fig:maze2-case1}{{a}{3}{Maze 2 - Study case 1\relax }{figure.caption.9}{}}
\newlabel{fig:maze2-case2}{{5b}{3}{Maze 2 - Study case 2\relax }{figure.caption.9}{}}
\newlabel{sub@fig:maze2-case2}{{b}{3}{Maze 2 - Study case 2\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Q-value for three different states}{4}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Maze 1 - Study case 1}{4}{subsubsection.8.2.1}\protected@file@percent }
\newlabel{fig:maze1-case1-states}{{6a}{4}{Maze 1: Study case 1 \vspace {12mm}\relax }{figure.caption.10}{}}
\newlabel{sub@fig:maze1-case1-states}{{a}{4}{Maze 1: Study case 1 \vspace {12mm}\relax }{figure.caption.10}{}}
\newlabel{fig:maze1-case1-state1}{{6b}{4}{State 1 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.6266 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.4188 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -0.0743 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: 0.0651 \relax }{figure.caption.10}{}}
\newlabel{sub@fig:maze1-case1-state1}{{b}{4}{State 1 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.6266 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.4188 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -0.0743 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: 0.0651 \relax }{figure.caption.10}{}}
\newlabel{fig:maze1-case1-state2}{{6c}{4}{State 2 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.4398 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.4012 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -0.2719 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.1840 \relax }{figure.caption.10}{}}
\newlabel{sub@fig:maze1-case1-state2}{{c}{4}{State 2 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.4398 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.4012 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -0.2719 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.1840 \relax }{figure.caption.10}{}}
\newlabel{fig:maze1-case1-state3}{{6d}{4}{State 3 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.0811 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.0670 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: 0.4976 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: 0.0538 \relax }{figure.caption.10}{}}
\newlabel{sub@fig:maze1-case1-state3}{{d}{4}{State 3 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.0811 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.0670 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: 0.4976 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: 0.0538 \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Maze 1 - Study case 2}{4}{subsubsection.8.2.2}\protected@file@percent }
\newlabel{fig:maze1-case2-states}{{7a}{4}{Maze 1: Study case 2 \vspace {12mm}\relax }{figure.caption.11}{}}
\newlabel{sub@fig:maze1-case2-states}{{a}{4}{Maze 1: Study case 2 \vspace {12mm}\relax }{figure.caption.11}{}}
\newlabel{fig:maze1-case2-state1}{{7b}{4}{State 1 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.3038 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.4816 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -0.8406 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.4413 \relax }{figure.caption.11}{}}
\newlabel{sub@fig:maze1-case2-state1}{{b}{4}{State 1 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.3038 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.4816 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -0.8406 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.4413 \relax }{figure.caption.11}{}}
\newlabel{fig:maze1-case2-state2}{{7c}{4}{State 2 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: 0.3047 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: 0.6444 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: 0.9281 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: 0.0919 \relax }{figure.caption.11}{}}
\newlabel{sub@fig:maze1-case2-state2}{{c}{4}{State 2 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: 0.3047 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: 0.6444 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: 0.9281 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: 0.0919 \relax }{figure.caption.11}{}}
\newlabel{fig:maze1-case2-state3}{{7d}{4}{State 3 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: 0.3186 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: 0.6454 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -1.9548 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: 1.0120 \relax }{figure.caption.11}{}}
\newlabel{sub@fig:maze1-case2-state3}{{d}{4}{State 3 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: 0.3186 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: 0.6454 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -1.9548 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: 1.0120 \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3}Maze 2 - Study case 1}{4}{subsubsection.8.2.3}\protected@file@percent }
\newlabel{fig:maze2-case1-states}{{8a}{4}{Maze 1: Study case 1 \vspace {12mm}\relax }{figure.caption.12}{}}
\newlabel{sub@fig:maze2-case1-states}{{a}{4}{Maze 1: Study case 1 \vspace {12mm}\relax }{figure.caption.12}{}}
\newlabel{fig:maze2-case1-state1}{{8b}{4}{State 1 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.5747 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.4207 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -0.4067 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.3697 \relax }{figure.caption.12}{}}
\newlabel{sub@fig:maze2-case1-state1}{{b}{4}{State 1 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.5747 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.4207 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -0.4067 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.3697 \relax }{figure.caption.12}{}}
\newlabel{fig:maze2-case1-state2}{{8c}{4}{State 2 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.2018 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.6692 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -0.5693 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.6134 \relax }{figure.caption.12}{}}
\newlabel{sub@fig:maze2-case1-state2}{{c}{4}{State 2 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.2018 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.6692 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -0.5693 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.6134 \relax }{figure.caption.12}{}}
\newlabel{fig:maze2-case1-state3}{{8d}{4}{State 3 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: 0.1001 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.1958 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: 0.4426 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.0384 \relax }{figure.caption.12}{}}
\newlabel{sub@fig:maze2-case1-state3}{{d}{4}{State 3 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: 0.1001 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.1958 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: 0.4426 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.0384 \relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.4}Maze 2 - Study case 2}{4}{subsubsection.8.2.4}\protected@file@percent }
\newlabel{fig:maze2-case2-states}{{9a}{4}{Maze 2: Study case 2 \vspace {12mm}\relax }{figure.caption.13}{}}
\newlabel{sub@fig:maze2-case2-states}{{a}{4}{Maze 2: Study case 2 \vspace {12mm}\relax }{figure.caption.13}{}}
\newlabel{fig:maze2-case2-state1}{{9b}{4}{State 1 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.5848 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.6763 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -0.4808 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.2210 \relax }{figure.caption.13}{}}
\newlabel{sub@fig:maze2-case2-state1}{{b}{4}{State 1 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.5848 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.6763 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: -0.4808 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.2210 \relax }{figure.caption.13}{}}
\newlabel{fig:maze2-case2-state2}{{9c}{4}{State 2 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.0098 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.2776 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: 0.2342 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: 0.0314 \relax }{figure.caption.13}{}}
\newlabel{sub@fig:maze2-case2-state2}{{c}{4}{State 2 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.0098 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: -0.2776 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: 0.2342 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: 0.0314 \relax }{figure.caption.13}{}}
\newlabel{fig:maze2-case2-state3}{{9d}{4}{State 3 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.2766 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: 0.0523 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: 0.9958 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.1527 \relax }{figure.caption.13}{}}
\newlabel{sub@fig:maze2-case2-state3}{{d}{4}{State 3 \\ \scriptsize \hspace *{5mm} $\boldsymbol {\cdot }$ left: -0.2766 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ up: 0.0523 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ right: 0.9958 \\ \hspace *{5mm} $\boldsymbol {\cdot }$ down: -0.1527 \relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}RMS during the training}{5}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explique como é definida a função de erro quadrático médio usada no treinamento.}{5}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inicialmente, devemos relembrar a equação de Bellman. Queremos achar a função Q-valor ótima, $Q^*$, que satisfaça a seguinte equação:}{5}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A política ótima $\pi ^*$ é dada se tomarmos a melhor ação de acordo com os valores da função $Q^*$.}{5}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Para resolver a equação anterior, temos um algoritmo iterativo dado por:}{5}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quando $i$ tende a infinito, temos que $Q_i$ tende a $Q^*$.}{5}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Entretanto, essa abordagem não é escalável. Assim o que se faz é usar uma rede neural para aprender e estimar a função Q-valor, sintetizando portanto, um mapeamento entre os estados e os q-valores.}{5}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Faremos então:}{5}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Onde $\theta $ representa os parâmetros do modelo (pesos da rede neural).}{5}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Assim, tomando $y_i$ como a saída da rede neural e $L_i(\theta _i)$ a função de perda usada durante seu treinamento, podemos retomar a equação de Bellman aproximando o lado direito com o lado esquerdo:}{5}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{E é justamente assim que é definida a função de erro quadrático médio usado no treinamento. Procura-se um mapeamento entre os estados, usado como entrada da rede neural (neste caso é o panorama do labirinto), e a a função Q-valor ótima. Ao minizarmos o erro quadrático médio entre a saída da rede e função Q-valor parametrizada por theta, estamos na verdade forçando que o lado esquerdo da equação de Bellman seja igual ao lado direito. Conforme o agente se movimenta e explora o ambiente, atua-se no vetor de pesos através de técnicas de gradiente, e com isso chegamos em um mapeamento onde a saída da rede neural representa a função Q-valor ótima aproximada.}{5}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Para finalizar, usando como gancho para a resposta do próximo item, temos que a entrada da rede (inputs) o estado do ambiente e a saída da rede (targets) calculada como a recompensa dada pela ação tomada anteriormente mais o fator de desconto (gamma) multiplicado pelo máximo de $Q(s',a')$ (que representa o máximo da saída do próximo estado, ou apenas a recompensa em caso de game over).}{5}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Experience replay}{6}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explique como é trabalhada a técnica de experience replay.}{6}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Para se trabalhar com a técnica de experience replay, é proposta uma classe denominada Experience. Nesta classe temos o ``construtor" da classe \_\_init\_\_ (magic method, ou ainda ``dunder``) e três métodos: remember, predict e get\_data.}{6}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A principal ideia aqui é usar uma memória para armazenar episódios. Um episódio é composto de uma lista com cinco elementos:}{6}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Assim, para cada movimento do rato, temos um episódio que podemos inserir em uma memória. Conforme essa memória vai enchendo, defini-se um limite para deletar episódios mais antigos armazenados. Neste caso em específico, adotou-se uma memória de tamanho padrão 1000, mas definindo-a como 8 vezes o tamanho do labirinto na chamada do treinamento da rede neural.}{6}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{O método remember é chamado após cada movimento do agente, sendo sua função armazenar as informações daquele episódio na memória. O método get\_data é usado para pegar da memória a entrada (inputs) e saída (targets) que será usado no treinamento da rede neural, sendo que o número de amostras retornado por get\_data é definido como o mínimo entre o tamanho da memória e 50 (data\_size). E é esse processo que caracteriza a técnica experience replay, armazenando experiências recentes do agente (com a premissa de quanto mais ele se movimenta no ambiente, melhor vai ficando a predição da rede neural e portanto seus próximos movimentos) e usando dessas experiências para o treinamento, dado que a solução do problema trata-se de um processo iterativo.}{6}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inicialmente, a saída da rede neural produzirá resultados aleatórios, mas conforme o treinamento for avançando e os parâmetros ajustados adequadamente, sua saída vai convergindo para a solução da equação de Bellman.}{6}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Question 9: GAN}{7}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Jupyter notebook related to this section with the results presented here can be opened in Google Colab environment with following link:\\}{7}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}MNIST}{7}{subsection.9.1}\protected@file@percent }
\newlabel{fig:mnist-epoch0}{{10a}{7}{First image\relax }{figure.caption.32}{}}
\newlabel{sub@fig:mnist-epoch0}{{a}{7}{First image\relax }{figure.caption.32}{}}
\newlabel{fig:mnist-epoch1000}{{10b}{7}{After 1,000 epochs\relax }{figure.caption.32}{}}
\newlabel{sub@fig:mnist-epoch1000}{{b}{7}{After 1,000 epochs\relax }{figure.caption.32}{}}
\newlabel{fig:mnist-epoch10000}{{10c}{7}{After 10,000 epochs\relax }{figure.caption.32}{}}
\newlabel{sub@fig:mnist-epoch10000}{{c}{7}{After 10,000 epochs\relax }{figure.caption.32}{}}
\newlabel{fig:mnist-epoch20000}{{10d}{7}{After 20,000 epochs\relax }{figure.caption.32}{}}
\newlabel{sub@fig:mnist-epoch20000}{{d}{7}{After 20,000 epochs\relax }{figure.caption.32}{}}
\newlabel{fig:mnist-epoch30000}{{10e}{7}{After 30,000 epochs\relax }{figure.caption.32}{}}
\newlabel{sub@fig:mnist-epoch30000}{{e}{7}{After 30,000 epochs\relax }{figure.caption.32}{}}
\newlabel{fig:mnist-epoch50000}{{10f}{7}{After 50,000 epochs\relax }{figure.caption.32}{}}
\newlabel{sub@fig:mnist-epoch50000}{{f}{7}{After 50,000 epochs\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces GAN trained for MNIST dataset\relax }}{7}{figure.caption.32}\protected@file@percent }
\newlabel{fig:gan-mnist}{{10}{7}{GAN trained for MNIST dataset\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Fashion MNIST}{7}{subsection.9.2}\protected@file@percent }
\newlabel{fig:fashion_mnist-epoch0}{{11a}{7}{First image\relax }{figure.caption.33}{}}
\newlabel{sub@fig:fashion_mnist-epoch0}{{a}{7}{First image\relax }{figure.caption.33}{}}
\newlabel{fig:fashion_mnist-epoch1000}{{11b}{7}{After 1,000 epochs\relax }{figure.caption.33}{}}
\newlabel{sub@fig:fashion_mnist-epoch1000}{{b}{7}{After 1,000 epochs\relax }{figure.caption.33}{}}
\newlabel{fig:fashion_mnist-epoch10000}{{11c}{7}{After 10,000 epochs\relax }{figure.caption.33}{}}
\newlabel{sub@fig:fashion_mnist-epoch10000}{{c}{7}{After 10,000 epochs\relax }{figure.caption.33}{}}
\newlabel{fig:fashion_mnist-epoch20000}{{11d}{7}{After 20,000 epochs\relax }{figure.caption.33}{}}
\newlabel{sub@fig:fashion_mnist-epoch20000}{{d}{7}{After 20,000 epochs\relax }{figure.caption.33}{}}
\newlabel{fig:fashion_mnist-epoch30000}{{11e}{7}{After 30,000 epochs\relax }{figure.caption.33}{}}
\newlabel{sub@fig:fashion_mnist-epoch30000}{{e}{7}{After 30,000 epochs\relax }{figure.caption.33}{}}
\newlabel{fig:fashion_mnist-epoch50000}{{11f}{7}{After 50,000 epochs\relax }{figure.caption.33}{}}
\newlabel{sub@fig:fashion_mnist-epoch50000}{{f}{7}{After 50,000 epochs\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces GAN trained for fashion MNIST dataset\relax }}{7}{figure.caption.33}\protected@file@percent }
\newlabel{fig:gan-fashion_mnist}{{11}{7}{GAN trained for fashion MNIST dataset\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Question 10: NLP}{8}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Jupyter notebook related to this section with the results presented here can be opened in Google Colab environment with following link:\\}{8}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}word2vec}{8}{subsection.10.1}\protected@file@percent }
\newlabel{fig:cbow}{{12a}{8}{CBOW\relax }{figure.caption.36}{}}
\newlabel{sub@fig:cbow}{{a}{8}{CBOW\relax }{figure.caption.36}{}}
\newlabel{fig:skip-gram}{{12b}{8}{skip-gram\relax }{figure.caption.36}{}}
\newlabel{sub@fig:skip-gram}{{b}{8}{skip-gram\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Architectures used in word2vec\relax }}{8}{figure.caption.36}\protected@file@percent }
\newlabel{fig:word2vec}{{12}{8}{Architectures used in word2vec\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {paragraph}{O word2vec, proposto por \href  {https://github.com/ito-rafael/machine-learning/blob/master/papers/NLP/2013\%20-\%20\%5Bword2vec\%5D\%20Efficient\%20Estimation\%20of\%20Word\%20Representations\%20in\%20Vector\%20Space\%20\%5BGoogle\%20Inc\%5D.pdf}{(Mikolov et al., 2013)} é uma ferramenta que fornece uma implementação eficiente das arquiteturas CBOW (continuous bag-of-words), Figura \ref  {fig:cbow}, e skip-gram, Figura \ref  {fig:skip-gram}, para computar representação de vetores de palavras. Um resumo de uma página deste artigo feito por mim pode ser encontrado no seguinte \href  {https://github.com/ito-rafael/machine-learning/blob/master/one-page\%20papers\%20summary/2013\%20-\%20\%5Bword2vec\%5D\%20Efficient\%20Estimation\%20of\%20Word\%20Representations\%20in\%20Vector\%20Space\%20\%5BGoogle\%20Inc\%5D.pdf}{link}. O código do wordvec está disponível em \href  {https://code.google.com/archive/p/word2vec/}{https://code.google.com/archive/p/word2vec/}.}{8}{figure.caption.36}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Resumidamente, o modelo CBOW tenta prever a palavra atual, baseada no contexto de algumas palavras anteriores e algumas palavras posteriores. Já o modelo Skip-gram tenta maximizar a classificação de uma palavra baseado em outra palavra na mesma frase.}{8}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Além dessas duas arquiteturas propostas, o artigo também apresenta medidas de similaridades sintáticas e semânticas entre palavras (feitas algebricamente), a criação do dataset ``Semantic-Syntactic Word Relationship test set" e uma forma de treinamento paralelo implementada em um framework chamado “DistBelief".}{8}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Em termos práticos, o que se consegue com o word2vec é uma representação de palavras em um espaço de dimensão reduzida (um vetor no ${\rm  I\tmspace  -\thinmuskip {.1667em}R^{300}}$ por exemplo). Com isso, elimina-se a esparsidade da técnica anterior denominada bag-of-words, one tinha-se uma representação one-hot das palavras, isto é, a entrada da rede neural tinha o tamanho do vocabulário usado.}{8}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esses vetores são denominados de word embeddings e são usados na grande maioria de arquiteturas SOTA (state-of-the-art). Por exemplo, na arquitetura BERT, usa-se três tipos de embeddings: embedding posicional, embedding de segmento e embedding de palavras.}{8}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Por fim, é interessante ressaltar as relações entre embeddings obtidas através de simples operações algébricas. Por exemplo, se considerarmos a relação entre embeddings de palavras de país e capital, temos a seguinte relação: Paris - France + Italy = Rome. Isto é, operando com os embeddings das palavras Paris, France e Italy, temos como resultado um vetor, sendo que a palavra mais próxima desse vetor resultante é o embedding da palavra Rome. Um outro exemplo clássico é a operação entres os vetores das palavra King - Man + Woman. O embedding mais próximo deste resultado é o da palavra Queen.}{9}{section*.41}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Embeddings vizinhos ao embedding da palavra ``frog"\relax }}{9}{figure.caption.43}\protected@file@percent }
\newlabel{fig:glove}{{13}{9}{Embeddings vizinhos ao embedding da palavra ``frog"\relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {paragraph}{Um outro projeto semelhante é o GloVe, proposto em \href  {https://github.com/ito-rafael/machine-learning/blob/master/papers/NLP/2014\%20-\%20\%5BGloVe\%5D\%20GloVe:\%20Global\%20Vectors\%20for\%20Word\%20Representation\%20\%5BStanford\%20University\%5D\%20(Pennington\%20et\%20al.\%2C\%202014).pdf}{(Pennington et al., 2014)}, que também propõe vetores para representação de palavras. Um exemplo interessante pode ser encontrado na \href  {https://nlp.stanford.edu/projects/glove/}{página inicial do projeto} e também mostrado na Figura \ref  {fig:glove}, onde é mostrado que as palavras cujos embedding mais próximos do embedding da palavra frog são: frogs, toad, litoria, leptodactylidae, rana, lizard e eleutherodactylus.}{9}{figure.caption.43}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}t-SNE}{9}{subsection.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A técnica t-distributed Stochastic Neighbor Embedding, ou apenas t-SNE, proposta em \href  {https://github.com/ito-rafael/machine-learning/blob/master/papers/visualization/2008\%20-\%20\%5Bt-SNE\%5D\%20Visualizing\%20Data\%20using\%20t-SNE\%20\%5BUniversity\%20of\%20Toronto\%5D\%20(Maaten\%20\%26\%20Hinton\%2C\%202008).pdf}{(Maaten \& Hinton, 2008)} teve como autor Laurens Maaten sob orientação de um dos maiores nomes na área de machine learning, Geoffrey Hinton, tendo como revisor do artigo Yoshua Bengio, também um dos maiores nomes da atualidade na área. Esta é uma técnica de aprendizado não-supervisionado e não-linear usada como redução de dimensionalidade (geralmente usada para visualizar dados em alta dimensão). Pontos similares nessa dimensão elevada são convertidos em probabilidades conjuntas e procura-se minimizar a divergência de Kullback-Leibler (KL) entre os dados de alta dimensão e as probabilidades conjunta dos embeddings de dimensão reduzida. Geralmente trabalha-se com dimensão 2 ou 3, por questões de interpretação visual.}{9}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Para calcular as probabilidades conjuntas, primeiro centra-se uma Gaussiana (tendo como hiperparâmetro uma variável denominada perplexidade) em cada ponto na dimensão original e elevada, mede-se a densidade de todos os pontos sob a distribuicão Gaussiana e porfim renormaliza-se os pontos. Isso nos dá um conjunto de probabilidades para todos os pontos, que chamaremos de $P_{ij}$. Em seguida, a mesma coisa é feita, porém usando uma distribuição Student t-distribution. Isso nos dá um segundo conjunto de probabilidades $Q_{ij}$ no espaço de dimensão reduzida. Finalmente, o último passo, conforme já citado anteriormente, consiste em minimizar a divergência de KL através de uma técnica de gradiente descendente.}{9}{section*.45}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Exemplo da técnica t-SNE aplicada na base MNIST\relax }}{10}{figure.caption.47}\protected@file@percent }
\newlabel{fig:tsne-mnist}{{14}{10}{Exemplo da técnica t-SNE aplicada na base MNIST\relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {paragraph}{Quando os dados estão em uma dimensão muito elevada, recomenda-se inicialmente reduzir a dimensão usando outra técnica (por exemplo PCA ou TruncatedSVD para uma dimensão 50), para em seguida aplicar t-SNE. Essa e outras dicas podem ser vista na própria página do autor do paper, Laurens, encontrada em \href  {https://lvdmaaten.github.io/tsne/}{https://lvdmaaten.github.io/tsne/}. Um outro detalhe importante, é que função custo usada na implementação do t-SNE não é convexa, isso faz com que diferente resultados sejam obtidos com diferente inicializações.}{10}{figure.caption.47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Results discussion}{10}{subsection.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Após o treinamento dado por 10 épocas, brincadeiras com palavras e o modelo treinado são feitas. Para isso utiliza-se a propriedade model.wv e diversos métodos.}{10}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inicialmente procura-se a palavra mais similar a palavra ``dirty''. Embora nem todas palavras sejam sinônimas, podemos ver que são palavras relacionadas, todas indicando muito provavelmente uma avaliação negativa no dataset. Isto é, quando a palavra ``dirty'' aparece em uma avaliação, é bem possível que uma outra palavra semelhante, como por exemplo alguma dessas da lista, também apareça, indicando algo negativo.}{10}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Em seguida, procura-se as 6 (dada pelo parâmetro ``topn'') palavras mais similares à palavra ``polite'', chegando em: courteous, friendly, cordial, professional, attentive e curteous. Além das topn palavras mais similares também temos o cálculo da métrica usada, que é a similaridade de cosenos. Podemos ver que de fato temos palavras próximas.}{10}{section*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Agora procura-se as 6 palavras mais similares à palavra ``france''. Aqui, temos como retorno as seguintes palavras: canada, germany, spain, hawaii, thailand e mexico. Podemos ver que todas palavras similares também corresponde a países, embora o valor da similaridade de cosenos tenha sido inferior ao caso anterior da palavra ``polite''. Podemos talvez ir até além, tentando fazer referências entre os países, como por exemplo o fato de o Canadá apresentar o francês como um dos idiomas oficiais. Alemanha e Espanha são países que fazem fronteiras com a França. Por fim, Havaí, Tailândia e México provavelmente aparecem na lista por serem países bastante representativos na base dados, dado que trata-se de avaliações de hotéis ao redor do globo (a Terra não é plana) e tais países são considerados turísticos.}{11}{section*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{O próximo exemplo é o de palavras similares à palavra ``shocked''. Novamente temos um padrão nas resposta obtidas, todas na classe de adjetivo e com a mesma terminação, indicando sensação de surpresa.}{11}{section*.52}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Neste próximo exemplo usa-se três palavras com contribuição positiva e uma palavra com contribuição negativa para a medida de similaridade. As palavras com contribuição positivas são bed, sheet e pillow, e a com contribuição negativa é couch. Devido ao fato de as três palavras com contribuição positiva serem itens de cama (cama, lençol e travesseiro), temos como saída também itens de cama: cobertor, edredon, colchão, fronha e colcha, ao mesmo tempo que não temos nenhum item relacionado a palavra sofá (ex: cushion e sofa).}{11}{section*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{O próximo item mede a similaridade entre as palavras ``dirty'' e ``smelly''. Embora sujo não seja exatamente um sinônimo de malcheiroso, temos uma medida de similaridade de cosenos relativamente alta de 0.75. Isso mostra que para o dataset em questão, essas palavras são próximas, no sentido de que é bastante plausível que ambas apareçam próximas em uma avaliação de hotel, dado que ambas são adjetivos negativos usados em uma avaliação.}{11}{section*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quatro palavras são usadas no próximo exemplo: bed, pillow, duvet e shower. Aqui, procura-se o item que não pertence ao conjunto. Claramente podemos ver que é a palavra chuveiro. Embora quase todo hotel tenha um chuveiro, todas outras palavras são itens de cama, o que indica que os embeddings das palavras cama, travesseiro e edredon estejam bem próximas, enquanto que o embedding da palavra chuveiro esteja mais distante das demais.}{11}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Os próximos dois itens apresentam as palavras em uma forma visual através de um gráfico 2D. Para isso trabalha-se com redução de dimensionalidade de 300 (dimensão dos embeddings) para 50 usando PCA, e em seguida reduz-se para dimensão 2 utilizando a técnica t-SNE. A função tsnescatterplot recebe como entrada três argumentos: modelo, uma palavra (query) e uma lista de palavras. A palavra query é mostrada em vermelho. As palavras mostradas em azul são as 10 palavras mais similares calculadas a partir da query. Por fim, as palavras da lista recebida como parâmetro da função são mostradas em verde.}{11}{section*.56}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces T-SNE 1\relax }}{12}{figure.caption.58}\protected@file@percent }
\newlabel{fig:tsne1}{{15}{12}{T-SNE 1\relax }{figure.caption.58}{}}
\@writefile{toc}{\contentsline {paragraph}{O primeiro gráfico é o da Figura \ref  {fig:tsne1}. Aqui a query é a palavra ``bed'' e a lista de palavras contém palavras não relacionadas com a query: futebol, pássaro, cachorro, leite, etc. Podemos ver que as palavras similares (mostradas em azul) à palavra cama, encontram-se relativamente próximas da palavra ``bed''. Já as palavras em verde, da lista aparentemente aleatória, encontram-se mais distantes da palavra query. Algumas exceções são visíveis, como por exemplo as palavras dog e grass, que podem ter ido parar aí devido a inicialização da técnica t-SNE, mas no geral, temos esse padrão citado anteriormente.}{12}{figure.caption.58}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces T-SNE 2\relax }}{13}{figure.caption.60}\protected@file@percent }
\newlabel{fig:tsne2}{{16}{13}{T-SNE 2\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {paragraph}{Por fim, o último gráfico mostrado na Figura \ref  {fig:tsne2} apresenta como query a palavra ``matress'' e recebe uma lista de 10 palavras, compreendendo as palavras entre a 10ª e 20ª mais similares a query. Dessa forma, o que temos é uma dispersão maior das palavras ao redor da query, mas ainda mantendo o padrão de as palavras azuis no geral estarem mais próximas. Uma observação importante é que query apresenta um erro de digitação (apresenta apenas um ``t'', sendo escrita como ``matress'' ao invés de ``mattress''), e curiosamente, tanto o plural correto (mattresses) quanto o plural escrito de forma equivocada (matresses) aparecem entre as palavras mais similares, indicando que essas palavras encontram-se no vocabulário do dataset e ainda que são usadas em contextos semelhantes.}{13}{figure.caption.60}\protected@file@percent }
