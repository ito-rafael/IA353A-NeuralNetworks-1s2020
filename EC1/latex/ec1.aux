\relax 
\providecommand\hyper@newdestlabel[2]{}
\bbl@beforestart
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}SGD}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Underdetermined}{2}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{../Q1/sgd\textunderscore under.m}{2}{lstlisting.-1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sgd-under-0_1}{{2a}{2}{Progression of W using step of 0.1\relax }{figure.caption.3}{}}
\newlabel{sub@fig:sgd-under-0_1}{{a}{2}{Progression of W using step of 0.1\relax }{figure.caption.3}{}}
\newlabel{fig:sgd-under-0_001}{{2b}{2}{Progression of W using step of 0.001\relax }{figure.caption.3}{}}
\newlabel{sub@fig:sgd-under-0_001}{{b}{2}{Progression of W using step of 0.001\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Progression of W for underdetermined system using SGD\relax }}{2}{figure.caption.3}\protected@file@percent }
\newlabel{fig:sdg-under}{{2}{2}{Progression of W for underdetermined system using SGD\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Overdetermined}{3}{subsubsection.1.1.2}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{../Q1/sgd\textunderscore over.m}{3}{lstlisting.-2}\protected@file@percent }
\newlabel{fig:sgd-over-0_1}{{3a}{3}{Progression of W using step of 0.1\relax }{figure.caption.4}{}}
\newlabel{sub@fig:sgd-over-0_1}{{a}{3}{Progression of W using step of 0.1\relax }{figure.caption.4}{}}
\newlabel{fig:sgd-over-0_001}{{3b}{3}{Progression of W using step of 0.001\relax }{figure.caption.4}{}}
\newlabel{sub@fig:sgd-over-0_001}{{b}{3}{Progression of W using step of 0.001\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Progression of W for overdetermined system using SGD\relax }}{3}{figure.caption.4}\protected@file@percent }
\newlabel{fig:sdg-over}{{3}{3}{Progression of W for overdetermined system using SGD\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Adam}{4}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Underdetermined}{4}{subsubsection.1.2.1}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{../Q1/adam\textunderscore under.m}{4}{lstlisting.-3}\protected@file@percent }
\newlabel{fig:adam-under-0_1}{{4a}{4}{Progression of W using step of 0.1\relax }{figure.caption.5}{}}
\newlabel{sub@fig:adam-under-0_1}{{a}{4}{Progression of W using step of 0.1\relax }{figure.caption.5}{}}
\newlabel{fig:adam-under-0_001}{{4b}{4}{Progression of W using step of 0.001\relax }{figure.caption.5}{}}
\newlabel{sub@fig:adam-under-0_001}{{b}{4}{Progression of W using step of 0.001\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Progression of W for underdetermined system using Adam\relax }}{4}{figure.caption.5}\protected@file@percent }
\newlabel{fig:adam-under}{{4}{4}{Progression of W for underdetermined system using Adam\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Overdetermined}{5}{subsubsection.1.2.2}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{../Q1/adam\textunderscore over.m}{5}{lstlisting.-4}\protected@file@percent }
\newlabel{fig:adam-over-0_1}{{5a}{5}{Progression of W using step of 0.1\relax }{figure.caption.6}{}}
\newlabel{sub@fig:adam-over-0_1}{{a}{5}{Progression of W using step of 0.1\relax }{figure.caption.6}{}}
\newlabel{fig:adam-over-0_001}{{5b}{5}{Progression of W using step of 0.001\relax }{figure.caption.6}{}}
\newlabel{sub@fig:adam-over-0_001}{{b}{5}{Progression of W using step of 0.001\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Progression of W for overdetermined system using Adam\relax }}{5}{figure.caption.6}\protected@file@percent }
\newlabel{fig:adam-over}{{5}{5}{Progression of W for overdetermined system using Adam\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Comparison}{6}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We can see from the plots that for SGD both underdetermined and overdetermined systems converged to the right solution using step equal to 0.1. When using step equal to 0.001, the optimizer did not converged, but we can see that it is on the way, not reaching it possibly because of the step value.}{6}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The same thing happened when using Adam optimizer. For steps equal to 0.1, both underdetermined and overdetermined converged to the right solution, what didn't happen when using step of 0.001.}{6}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparing both plots, in general we can see that SGD has a smoother path, walking directly to the solution. Adam, on the other hand, due to the adaptative moment estimation, shows a curlier path through the solution, sometimes even surrounding it.}{6}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Since we are searching for $x$ that minimizes the previous expression, we will calculate the derivative with relation to $x$ and set it equal to zero:}{7}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Property used: $\|x\|_{Q}^{2} = x^{T}Qx$}{7}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Property used: $(M + N)^T = M^T + N^T$}{7}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Property used: $(MN)^T = N^T M^T$}{7}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Properties used:}{7}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Using the previous properties, we have:}{7}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{If the regularization coefficient was used, we would have:}{8}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{This, would lead to the solution:}{8}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{So, even if we choose an $\alpha $ different than 1, it is always possible to embed its value in the matrix $Q$, since $\alpha $ is only multiplying the $Q$ matrix.}{8}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{If $M$ is symmetric, then $M^T$ must be equal to $M$:}{9}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Now we need to prove that $M$ must be positive semi-definite, i.e., $z^TMz \geq 0, \forall z \in R^n$:}{9}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Since N has full rank, we have $min(m,n) = n$ linearly independent columns.}{9}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We want to prove that $N$ may not be unique in $M=N^TN$. For this, lets consider the orthogonal matrix $Q$ and the new matrix that is the result by its multiplication with $N$, i.e., $QN$:}{9}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Since $Q$ is orthogonal, $Q^TQ$ is equal to the identity matrix $I$. Hence:}{9}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{So if $M$ can be decomposed in $N^TN$, it can also be decomposed by $(QN)^T(QN)$, with $Q$ being an orthogonal matrix.}{9}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{In order to find $N$ we can use Cholesky decomposition. Here we will show an example for n = 3, but we will generalize to other dimensions:}{9}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Taylor series expansion of a function $f(x,y)$ in a neighborhood around $(x_0,y_0)$ is as follows:}{10}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ignoring terms from second order and higher, and considering the linearized function given $f_L(x,y)=2x+py-8$, we have:}{10}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Taking $f(x,y) = x\sqrt  y$, we have:}{10}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Now, if we compare the terms that only depend on $x$, that only depend on $y$ and the independent terms (that only depend on $x_0$ and $y_0$), we have:}{10}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Substituting in (i) in (iii):}{10}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Substituting in (ii):}{10}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{So the point from where the function was linearized is $(x_0,y_0) = (8,4)$ and the coefficient $p = 2$, then $f_L(x,y) = 2x + 2y -8$.}{10}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Na inferência dedutiva a conclusão sempre está contida nas premissas. Ao derivar uma afirmação $y$ partindo de $x$, dizemos que $y$ é uma consequência lógica de $x$, ou seja, a inferência de $y$ é baseada no que foi assumido em $x$ e será sempre verdadeira. Também podemos incluir casos particulares que são inferidos a partir de premissas universais e gerais.}{11}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dos três tipos de inferência esta é a única cuja conclusão lógica é sempre verdadeira.}{11}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{É a única das inferências que não acrescenta conhecimento novo.}{11}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Se um pesquisador estiver treinando uma rede neural e por algum motivo as informações referentes ao estado atual do treinamento seja perdido (seja por falta de energia, Colab retomando a máquina virtual, ou qualquer outro) sem que haja um backup dos pesos e parâmetros, será necessário reiniciar o treinamento do zero. Logo, o pesquisador opta por salvar checkpoints do status do treinamento (pesos e outros parâmetros), para caso isso ocorra, o treinamento não seja inteiramente perdido e o pesquisador possa continuar do último estado salvo. Essa ação preventiva é tomada baseada em uma inferência dedutiva.\\ 1. Se cai a energia e não tenho backup $\DOTSB \tmspace  +\thickmuskip {.2777em}\DOTSB \Relbar \joinrel \Rightarrow \tmspace  +\thickmuskip {.2777em}$ perco o treinamento\\ 2. Quando perco o treinamento (sem backup) $\DOTSB \tmspace  +\thickmuskip {.2777em}\DOTSB \Relbar \joinrel \Rightarrow \tmspace  +\thickmuskip {.2777em}$ recomeço do zero\\ conclusão: se cai energia sem backup, perco tempo recomeçando do zero.\\ precaução: vou salvar checkpoints do treinamento.}{11}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Diferente da dedução aqui não temos mais a garantia de veracidade a cerca de inferência. Em indução temos a intuição de probabilidade, agregando experiência e conhecimento. A partir de um certo conjunto de elementos conhecidos ou observáveis, e portanto de casos particulares, chega-se em uma conclusão geral. Assim, uma característica marcante deste tipo de inferência é a capacidade de generalização.}{11}{section*.52}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Entre as vantagens deste modo de inferência podemos listar a capacidade de generalização mesmo sem haver uma certeza lógica e uso de experiências anteriores a cerca do que se está inferindo.}{11}{section*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esta inferência não é 100\% verdade como a dedutiva, portanto pode levar a generalizações errôneas. Por trabalhar com experiências passadas, tem potencial de acrescentar informação menor do que a abdução, pois boa parte dela foi usada como experiência para a própria inferência.}{11}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ao rodar o loop de treinamento de uma rede neural, o pesquisador se depara com uma exceção Python por conta de um erro em operações envolvendo matrizes que apresenta um log de erro não direto e difícil de interpretar, não sendo muito útil portanto. Em experiências anteriores, na maior parte das vezes o erro era causado por descasamento nos shapes de tensores/arrays ou dimensões erradas. Assim, o pesquisador decide usar um debugger (ex: pdb) colocando o breakpoint imediatamente na linha antes do erro acontecer e checar os shapes e dimensões dos tensores envolvidos, pois desconfia que o erro envolva uma dessas duas coisas.}{12}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Na inferência abdutiva trabalha-se com hipóteses para explicação do que foi observado. Trata-se de inferir $x$ como explicação para $y$. A conclusão não é dado pela lógica, mas sim pela capacidade em justificar e argumentar a escolha da melhor hipótese dado um contexto. Mesmo que este tipo de inferência não tenha a certeza da inferência dedutiva, cria-se conclusões que nos fazem considerar a hipótese como plausível e/ou verdadeira.}{12}{section*.61}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dos três tipos de inferência esta é a que mais pode acrescentar informação, caso a inferência seja condizente com a realidade e possa se sustentar com argumentos. Quando usada adequadamente, pode ser bastante útil informações a priori (priors) em estatística Bayesiana.}{12}{section*.63}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Entretanto, é mais fraca do ponto de vista lógico, no sentido de não haver certeza alguma no que se está concluindo, havendo apenas formação de hipóteses que podem sustentar a análise.}{12}{section*.65}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{O pesquisador está pela primeira vez trabalhando big data que requer uso intenso de GPU, horas de processamento, milhões de amostras, milhões de parâmetros ajustáveis, etc. Ele está treinando uma arquitetura do T5 da Google para traduzir sentenças do inglês para português. Ao iniciar o treinamento percebe que o tempo de processamento de uma época está na ordem de 10 horas. Como o pesquisador não dispõe de tempo para treinar por algumas épocas, decide que precisa diminuir o tempo de treinamento, mas tentando perder o mínimo de qualidade possível. Assim, o pesquisador decide tentar usar uma precisão de 16-bits ao invés da tradicional 32-bits. Essa foi uma escolha dentre inúmeras outras (abaixar batch size, aumentar learning rate, dimunuir tamanho das sequências de entrada/saída, usar um modelo menos parâmetros, alterar o scheduling factor, etc). Dentre todas as ações a se tomar, o pesquisador toma como primeiro palpite a tentativa de usar outro valor de precisão, com o pensamento de que este pode oferecer o melhor tradeoff entre custo computacional, mantendo qualidade na tradução de seu modelo.}{12}{section*.67}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{No treinamento de uma rede neural, um dos principais conceitos está no aprendizado através de dados. O que procura-se no treinamento, é a distrubuição original e desconhecida que gerou os dados, para que se possa generalizar para além dos dados que se dispõe. Assim, se pudermos modelar uma distribuição, baseado nos dados de treinamento, podemos tirar conclusões a respeito de qualquer dado possível que esteja submetido àquela distribuição (se a generalização estiver boa).\\ Uma das definições de aprendizado de máquina, dada por Tom Mitchell (1997) é: "diz-se que um programa de computador aprendeu de uma experiência E com respeito a uma certa tarefa T e com certa performance P, se sua performance em T medida por P aumenta com a experiência E". Nessa afirmação, pode-ser ver claramente os príncipios de inferência indutiva, havendo a questão da experiência em E e capacidade de generalização em T medida por P.\\ Por fim, temos exatamente a situação em que parte-se de casos particulares (dados de treinamento) a fim de se inferir o caso geral (distruição que gerou os dados / capacidade de generalização).}{13}{section*.69}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{O primeiro paper selecionado é de 2002 com 2173 citações. Esse paper foi publicado no JAMA (The Journal of the American Medical Association) que é um jornal da área médica com 48 publicações por ano pela AMA (American Medical Association).}{14}{section*.72}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Neste artigo foram avaliadas três intervenções de treinamento cognitivo em pessoas entre 65 e 94 anos, totalizando 2832 voluntários. Os participantes foram aleatoriamente divididos em quatro grupos:}{14}{section*.73}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{As intervenções melhoraram as habilidades cognitivas ao qual o voluntário foi submetido por até dois anos. 87\% dos participantes do grupo de velocidade de processamento, 74\% do grupo de raciocínio e 26\% dos participantes do treinamento de memória apresentaram melhoras cognitivas confiáveis imediatamente após o período de intervenção. Por fim, os autores concluem que os resultados corroboram com a efetividade e durabilidade de intervenções de treinamento cognitivos em idosos para o desenvolvimento de habilidades cognitivas específicas.}{14}{section*.74}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{O segundo paper selecionado é de 2009 e conta com 315 citações. Esse paper foi publicado no jornal acadêmico Alzheimer's \& Dementia, que conta com publicações mensais da associação sem fins lucrativos Journal of the Alzheimer's Association.}{15}{section*.76}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Este artigo se enquadra na categoria ``review article", onde os autores revisam a literatura e avaliam os efeitos imediatos e de longo prazo de intervenções cognitivas na saúde de idosos. Por ser publicado em uma revista sobre Alzheimer, este acaba sendo o foco principal do quesito ``saúde".}{15}{section*.77}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A metodologia adotada neste artigo foi a busca de diferentes termos relacionados a treinamento cognitivo (ex: brain training, cognitive intervention) em diferentes fontes, sendo que somente estudos publicados como RTCs (randomized controlled trials) foram incluídos nas análises.}{15}{section*.78}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Os tipos de treinamento cognitivos estudados são os mesmos do artigo citado anteriormente: memória, raciocínio e velocidade de processamento. Adicionalmente, foi incluído também um treinamento multimodal, que combina mais de uma categoria ou envolve dois ou mais sistemas sensoriais.}{15}{section*.79}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Os resultados mostram que a literatura neste tema é limitada, que não há acompanhamento por tempo suficiente após os treinamentos e que há falta de consenso em quais seriam os treinamentos cognitivos mais eficientes.}{15}{section*.80}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Por fim, os autores reconhecem que os treinamentos melhoram habilidades específicas dos participantes, mas que isso não necessariamente generaliza bem para outras atividades. Como exemplo, eles citam que os participantes que receberam treinamento para memória, conseguiram sim melhorias neste quesito, mas obtiveram performance baixa em raciocínio. O mesmo vale no sentido contrário: participantes que receberam treinamento para raciocínio, mostraram melhoras neste quesito, mas sem efeitos em outras tarefas, como memória. Assim, os autores concluem que os treinamentos cognitivos ajudam no desenvolvimento da habilidade da tarefa na qual a pessoa foi submetida, mas que em se tratando de prevenção ao Alzheimer, que não foram encontradas evidências de que programas de intervenções cognitivas estruturadas atrasam ou diminuem a progressão do Alzheimer em idosos.}{15}{section*.81}\protected@file@percent }
