\relax 
\providecommand\hyper@newdestlabel[2]{}
\bbl@beforestart
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}SGD}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Underdetermined}{2}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{../Q1/sgd\textunderscore under.m}{2}{lstlisting.-1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sgd-under-0_1}{{2a}{2}{Progression of W using step of 0.1\relax }{figure.caption.3}{}}
\newlabel{sub@fig:sgd-under-0_1}{{a}{2}{Progression of W using step of 0.1\relax }{figure.caption.3}{}}
\newlabel{fig:sgd-under-0_001}{{2b}{2}{Progression of W using step of 0.001\relax }{figure.caption.3}{}}
\newlabel{sub@fig:sgd-under-0_001}{{b}{2}{Progression of W using step of 0.001\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Progression of W for underdetermined system using SGD\relax }}{2}{figure.caption.3}\protected@file@percent }
\newlabel{fig:sdg-under}{{2}{2}{Progression of W for underdetermined system using SGD\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Overdetermined}{3}{subsubsection.1.1.2}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{../Q1/sgd\textunderscore over.m}{3}{lstlisting.-2}\protected@file@percent }
\newlabel{fig:sgd-over-0_1}{{3a}{3}{Progression of W using step of 0.1\relax }{figure.caption.4}{}}
\newlabel{sub@fig:sgd-over-0_1}{{a}{3}{Progression of W using step of 0.1\relax }{figure.caption.4}{}}
\newlabel{fig:sgd-over-0_001}{{3b}{3}{Progression of W using step of 0.001\relax }{figure.caption.4}{}}
\newlabel{sub@fig:sgd-over-0_001}{{b}{3}{Progression of W using step of 0.001\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Progression of W for overdetermined system using SGD\relax }}{3}{figure.caption.4}\protected@file@percent }
\newlabel{fig:sdg-over}{{3}{3}{Progression of W for overdetermined system using SGD\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Adam}{4}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Underdetermined}{4}{subsubsection.1.2.1}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{../Q1/adam\textunderscore under.m}{4}{lstlisting.-3}\protected@file@percent }
\newlabel{fig:adam-under-0_1}{{4a}{4}{Progression of W using step of 0.1\relax }{figure.caption.5}{}}
\newlabel{sub@fig:adam-under-0_1}{{a}{4}{Progression of W using step of 0.1\relax }{figure.caption.5}{}}
\newlabel{fig:adam-under-0_001}{{4b}{4}{Progression of W using step of 0.001\relax }{figure.caption.5}{}}
\newlabel{sub@fig:adam-under-0_001}{{b}{4}{Progression of W using step of 0.001\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Progression of W for underdetermined system using Adam\relax }}{4}{figure.caption.5}\protected@file@percent }
\newlabel{fig:adam-under}{{4}{4}{Progression of W for underdetermined system using Adam\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Overdetermined}{5}{subsubsection.1.2.2}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{../Q1/adam\textunderscore over.m}{5}{lstlisting.-4}\protected@file@percent }
\newlabel{fig:adam-over-0_1}{{5a}{5}{Progression of W using step of 0.1\relax }{figure.caption.6}{}}
\newlabel{sub@fig:adam-over-0_1}{{a}{5}{Progression of W using step of 0.1\relax }{figure.caption.6}{}}
\newlabel{fig:adam-over-0_001}{{5b}{5}{Progression of W using step of 0.001\relax }{figure.caption.6}{}}
\newlabel{sub@fig:adam-over-0_001}{{b}{5}{Progression of W using step of 0.001\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Progression of W for overdetermined system using Adam\relax }}{5}{figure.caption.6}\protected@file@percent }
\newlabel{fig:adam-over}{{5}{5}{Progression of W for overdetermined system using Adam\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Comparison}{6}{subsection.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Values of regularization coefficient found in coarse and fine searches\relax }}{6}{table.caption.7}\protected@file@percent }
\newlabel{tab:alpha_results}{{1}{6}{Values of regularization coefficient found in coarse and fine searches\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Since we are searching for $x$ that minimizes the previous expression, we will calculate the derivative with relation to $x$ and set it equal to zero:}{7}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Property used: $\|x\|_{Q}^{2} = x^{T}Qx$}{7}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Property used: $(M + N)^T = M^T + N^T$}{7}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Property used: $(MN)^T = N^T M^T$}{7}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Properties used:}{7}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Using the previous properties, we have:}{7}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{If $M$ is symmetric, then $M^T$ must be equal to $M$:}{8}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Now we need to prove that $M$ must be positive semi-definite, i.e., $z^TMz \geq 0, \forall z \in ^n$}{8}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{...}{8}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We want to prove that $N$ may not be unique in $M=N^TN$. For this, lets consider the orthogonal matrix $Q$ and the new matrix that is the result by its multiplication with $N$, i.e., $QN$:}{8}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Since $Q$ is orthogonal, $Q^TQ$ is equal to the identity matrix $I$. Hence:}{8}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{So if $M$ can be decomposed in $N^TN$, it can also be decomposed by $(QN)^T(QN)$, with $Q$ being an orthogonal matrix.}{8}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{...}{8}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Taylor series expansion of a function $f(x,y)$ in a neighborhood around $(x_0,y_0)$ is as follows:}{9}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ignoring terms from second order and higher, and considering the linearized function given $f_L(x,y)=2x+py-8$, we have:}{9}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Taking $f(x,y) = x\sqrt  y$, we have:}{9}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Now, if we compare the terms that only depend on $x$, that only depend on $y$ and the independent terms (that only depend on $x_0$ and $y_0$), we have:}{9}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Substituting in (i) in (iii):}{9}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Substituting in (ii):}{9}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{So the point from where the function was linearized is $(x_0,y_0) = (8,4)$ and the coefficient $p = 2$, then $f_L(x,y) = 2x + 2y -8$.}{9}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Na inferência dedutiva a conclusão sempre está contida nas premissas. Ao derivar uma afirmação $y$ partindo de $x$, dizemos que $y$ é uma consequência lógica de $x$, ou seja, a inferência de $y$ é baseada no que foi assumido em $x$ e será sempre verdadeira. Também podemos incluir casos particulares que são inferidos a partir de premissas universais e gerais.}{10}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dos três tipos de inferência esta é a única cuja conclusão lógica é sempre verdadeira.}{10}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{É a única das inferências que não acrescenta conhecimento novo.}{10}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Se um pesquisador estiver treinando uma rede neural e por algum motivo as informações referentes ao estado atual do treinamento seja perdido (seja por falta de energia, Colab retomando a máquina virtual, ou qualquer outro) sem que haja um backup dos pesos e parâmetros, será necessário reiniciar o treinamento do zero. Logo, o pesquisador opta por salvar checkpoints do status do treinamento (pesos e outros parâmetros), para caso isso ocorra, o treinamento não seja inteiramente perdido e o pesquisador possa continuar do último estado salvo. Essa ação preventiva é tomada baseada em uma inferência dedutiva.\\ 1. Se cai a energia e não tenho backup $\DOTSB \tmspace  +\thickmuskip {.2777em}\DOTSB \Relbar \joinrel \Rightarrow \tmspace  +\thickmuskip {.2777em}$ perco o treinamento\\ 2. Quando perco o treinamento (sem backup) $\DOTSB \tmspace  +\thickmuskip {.2777em}\DOTSB \Relbar \joinrel \Rightarrow \tmspace  +\thickmuskip {.2777em}$ recomeço do zero\\ conclusão: se cai energia sem backup, perco tempo recomeçando do zero.\\ precaução: vou salvar checkpoints do treinamento.}{10}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Diferente da dedução aqui não temos mais a garantia de veracidade a cerca de inferência. Em indução temos a intuição de probabilidade, agregando experiência e conhecimento. A partir de um certo conjunto de elementos conhecidos ou observáveis, e portanto de casos particulares, chega-se em uma conclusão geral. Assim, uma característica marcante deste tipo de inferência é a capacidade de generalização.}{10}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Entre as vantagens deste modo de inferência podemos listar a capacidade de generalização mesmo sem haver uma certeza lógica e uso de experiências anteriores a cerca do que se está inferindo.}{10}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esta inferência não é 100\% verdade como a dedutiva, portanto pode levar a generalizações errôneas. Por trabalhar com experiências passadas, tem potencial de acrescentar informação menor do que a abdução, pois boa parte dela foi usada como experiência para a própria inferência.}{10}{section*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ao rodar o loop de treinamento de uma rede neural, o pesquisador se depara com uma exceção Python por conta de um erro em operações envolvendo matrizes que apresenta um log de erro não direto e difícil de interpretar, não sendo muito útil portanto. Em experiências anteriores, na maior parte das vezes o erro era causado por descasamento nos shapes de tensores/arrays ou dimensões erradas. Assim, o pesquisador decide usar um debugger (ex: pdb) colocando o breakpoint imediatamente na linha antes do erro acontecer e checar os shapes e dimensões dos tensores envolvidos, pois desconfia que o erro envolva uma dessas duas coisas.}{11}{section*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Na inferência abdutiva trabalha-se com hipóteses para explicação do que foi observado. Trata-se de inferir $x$ como explicação para $y$. A conclusão não é dado pela lógica, mas sim pela capacidade em justificar e argumentar a escolha da melhor hipótese dado um contexto. Mesmo que este tipo de inferência não tenha a certeza da inferência dedutiva, cria-se conclusões que nos fazem considerar a hipótese como plausível e/ou verdadeira.}{11}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dos três tipos de inferência esta é a que mais pode acrescentar informação, caso a inferência seja condizente com a realidade e possa se sustentar com argumentos. Quando usada adequadamente, pode ser bastante útil informações a priori (priors) em estatística Bayesiana.}{11}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Entretanto, é mais fraca do ponto de vista lógico, no sentido de não haver certeza alguma no que se está concluindo, havendo apenas formação de hipóteses que podem sustentar a análise.}{11}{section*.60}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{O pesquisador está pela primeira vez trabalhando big data que requer uso intenso de GPU, horas de processamento, milhões de amostras, milhões de parâmetros ajustáveis, etc. Ele está treinando uma arquitetura do T5 da Google para traduzir sentenças do inglês para português. Ao iniciar o treinamento percebe que o tempo de processamento de uma época está na ordem de 10 horas. Como o pesquisador não dispõe de tempo para treinar por algumas épocas, decide que precisa diminuir o tempo de treinamento, mas tentando perder o mínimo de qualidade possível. Assim, o pesquisador decide tentar usar uma precisão de 16-bits ao invés da tradicional 32-bits. Essa foi uma escolha dentre inúmeras outras (abaixar batch size, aumentar learning rate, dimunuir tamanho das sequências de entrada/saída, usar um modelo menos parâmetros, alterar o scheduling factor, etc). Dentre todas as ações a se tomar, o pesquisador toma como primeiro palpite a tentativa de usar outro valor de precisão, com o pensamento de que este pode oferecer o melhor tradeoff entre custo computacional, mantendo qualidade na tradução de seu modelo.}{11}{section*.62}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{No treinamento de uma rede neural, um dos principais conceitos está no aprendizado através de dados. O que procura-se no treinamento, é a distrubuição original e desconhecida que gerou os dados, para que se possa generalizar para além dos dados que se dispõe. Assim, se pudermos modelar uma distribuição, baseado nos dados de treinamento, podemos tirar conclusões a respeito de qualquer dado possível que esteja submetido àquela distribuição (se a generalização estiver boa).\\ Uma das definições de aprendizado de máquina, dada por Tom Mitchell (1997) é: "diz-se que um programa de computador aprendeu de uma experiência E com respeito a uma certa tarefa T e com certa performance P, se sua performance em T medida por P aumenta com a experiência E". Nessa afirmação, pode-ser ver claramente os príncipios de inferência indutiva, havendo a questão da experiência em E e capacidade de generalização em T medida por P.\\ Por fim, temos exatamente a situação em que parte-se de casos particulares (dados de treinamento) a fim de se inferir o caso geral (distruição que gerou os dados / capacidade de generalização).}{12}{section*.64}\protected@file@percent }
